{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"StarryNight","text":"<p>A toolkit for processing, analyzing, and managing optical pooled screening (OPS) image data.</p>"},{"location":"#what-you-can-do","title":"What You Can Do","text":"<ul> <li>Process microscopy images with advanced algorithms</li> <li>Build reproducible analysis pipelines</li> <li>Run jobs locally or scale to cloud environments</li> <li>Visualize results through an intuitive interface</li> </ul> <p>Installation Guide \u2192 | Complete Workflow \u2192 | Architecture \u2192</p>"},{"location":"CLAUDE/","title":"CLAUDE.md","text":"<p>This file provides guidance to Claude Code (claude.ai/code) when working with documentation and code in this repository.</p>"},{"location":"CLAUDE/#starrynight-documentation-guidelines","title":"StarryNight Documentation Guidelines","text":""},{"location":"CLAUDE/#structure-principles","title":"Structure Principles","text":"<ul> <li>Follow progressive disclosure (overview \u2192 concepts \u2192 details)</li> <li>Use clear section headers and consistent header levels</li> <li>Keep directory structure shallow (2-3 levels max)</li> <li>Ensure each section has a clear entry point document</li> </ul>"},{"location":"CLAUDE/#content-standards","title":"Content Standards","text":"<ul> <li>Use consistent Markdown formatting with appropriate language tags</li> <li>Write concisely with numbered steps for complex procedures</li> <li>Include concrete examples with real commands and outputs</li> <li>Format code blocks, file paths, and commands appropriately</li> <li>Link to existing docs rather than duplicating content</li> </ul>"},{"location":"CLAUDE/#common-commands","title":"Common Commands","text":"<ul> <li>Generate pipeline graphs: <code>python /path/to/cp_graph.py input.json output.dot --rank-nodes --remove-unused-data</code></li> <li>Render graphs: <code>dot -Gdpi=50 -Tpng input.dot -o output.png</code> or <code>dot -Tsvg input.dot -o output.svg</code></li> </ul>"},{"location":"CLAUDE/#notes-on-repository-restructuring","title":"Notes on Repository Restructuring","text":"<p>This repository is undergoing restructuring to improve organization: - Notebooks will be moved to <code>/examples/notebooks/</code></p>"},{"location":"CLAUDE/#interaction-guidelines","title":"Interaction Guidelines","text":"<ul> <li>Sometimes it might be easiest to ask the human to do some tasks, typically when they involve moving large chunks of text or running some commands that are better run by hand. In these cases, ask the human to do it</li> </ul>"},{"location":"architecture/00_architecture_overview/","title":"StarryNight Architecture Overview","text":""},{"location":"architecture/00_architecture_overview/#navigating-the-architecture-documentation","title":"Navigating the Architecture Documentation","text":"<p>This document provides a high-level structural map of the StarryNight system and serves as your main guide to the architecture documentation. Below is a recommended reading path based on your role and interests; the layer terminology is explained and defined next:</p> If you are a... Start here Then explore New developer This overview + Practical Integration Layer-specific docs based on your focus Algorithm developer Algorithm Layer CLI Layer \u2192 Module Layer Module developer Module Layer Pipeline Layer Pipeline developer Pipeline Layer Execution Layer Configuration developer Configuration Layer Relevant layer docs Biologist/Non-technical user Architecture for Biologists Practical Integration <p>Pipeline Terminology</p> <p>Throughout this documentation, \"pipeline\" refers to StarryNight pipelines - complete scientific workflows composed of multiple processing modules. This is distinct from:</p> <ul> <li>CellProfiler pipelines (<code>.cppipe</code> files) - single-tool image analysis workflows</li> <li>Snakemake pipelines - the underlying execution engine's rule-based workflows</li> </ul> <p>When we discuss CellProfiler or Snakemake pipelines specifically, we use those full terms. Otherwise, \"pipeline\" means StarryNight's higher-level workflow composition system.</p>"},{"location":"architecture/00_architecture_overview/#architecture-at-a-glance","title":"Architecture at a Glance","text":"<p>The architecture consists of conceptual layers and their implementation:</p> <pre><code>flowchart TB\n    %% Top container for all conceptual layers\n    subgraph \"Conceptual Architecture\"\n        A[Algorithm Layer&lt;br&gt;Algorithm Sets with&lt;br&gt;Data/Pipeline/Execution Functions] --&gt;|exposed as| B[CLI Layer&lt;br&gt;Command Groups&lt;br&gt;&amp; Commands]\n        B -.-&gt;|invoked by| C[Module Layer&lt;br&gt;Bilayer Specs + &lt;br&gt;Pipecraft Compute Graphs]\n        C --&gt;|composed into| D[Pipeline Layer&lt;br&gt;Sequential/Parallel&lt;br&gt;Workflows]\n        D --&gt;|translated to| E[Execution Layer&lt;br&gt;Snakemake Rules &amp; &lt;br&gt;Container Execution]\n    end\n\n    %% Bottom container for implementation directories\n    subgraph \"Code Implementation\"\n        I1[\"/starrynight/algorithms/\"]\n        I2[\"/starrynight/cli/\"]\n        I3[\"/starrynight/modules/\"]\n        I4[\"/starrynight/pipelines/\"]\n        I5[\"/pipecraft/\"]\n        I6[\"/pipecraft/backend/\"]\n    end\n\n    %% Connect layers to implementation\n    A -.-&gt; I1\n    B -.-&gt; I2\n    C -.-&gt; I3\n    D -.-&gt; I4 &amp; I5\n    E -.-&gt; I6\n\n    %% Styling\n    classDef code fill:#f9f9f9,stroke:#333,stroke-width:1px\n    class A,B,C,D,E code\n    classDef impl fill:#fffdf0,stroke:#333,stroke-width:1px\n    class I1,I2,I3,I4,I5,I6 impl</code></pre> <p>Note: The Configuration Layer (in <code>/starrynight/experiments/</code>) provides parameters across all layers but is not shown in this diagram for clarity.</p>"},{"location":"architecture/00_architecture_overview/#core-design-principles","title":"Core Design Principles","text":"<ul> <li>Separation of concerns: Each layer has distinct responsibilities</li> <li>Progressive composition: Build complex pipelines from simple components</li> <li>Containerized execution: Run code in reproducible environments</li> <li>Configuration inference: The Configuration Layer provides adaptive parameters across all other layers through smart defaults and data-driven inference</li> </ul>"},{"location":"architecture/00_architecture_overview/#layer-overview","title":"Layer Overview","text":"<p>StarryNight consists of six interconnected layers:</p> Layer Primary Responsibility Key Components Algorithm Core computational operations Python functions for LoadData generation, CellProfiler pipeline creation, image processing CLI Command-line interface Click-based command groups (<code>illum</code>, <code>analysis</code>, <code>segcheck</code>, <code>cp</code>), parameter parsing Module Standardized processing steps Bilayers specifications, Pipecraft compute graphs, container configurations Pipeline Complete workflow orchestration Module composition, sequential/parallel execution paths Execution Backend runtime infrastructure Snakemake DAG generation, Docker/AWS Batch backends, resource management Configuration Parameter inference and management Experiment definitions, adaptive parameter inference"},{"location":"architecture/00_architecture_overview/#data-and-control-flow","title":"Data and Control Flow","text":"<p>The StarryNight architecture transforms data through two primary phases: pipeline composition and runtime execution. While shown as linear flows for clarity, the architecture supports iterative scientific workflows where researchers can adjust parameters and re-run specific steps based on results.</p>"},{"location":"architecture/00_architecture_overview/#pipeline-composition-phase","title":"Pipeline Composition Phase","text":"<p>This phase focuses on how configurations become executable workflows:</p> <pre><code>sequenceDiagram\n    participant Config as Configuration Layer\n    participant Module as Module Layer\n    participant Pipeline as Pipeline Layer\n    participant Execution as Execution Layer\n\n    Config-&gt;&gt;Module: Supply module parameters\n    Module-&gt;&gt;Module: Generate compute graphs\n    Module-&gt;&gt;Pipeline: Pass module specifications\n    Pipeline-&gt;&gt;Pipeline: Compose workflow\n    Pipeline-&gt;&gt;Execution: Submit workflow\n    Execution-&gt;&gt;Execution: Translate to Snakemake rules</code></pre>"},{"location":"architecture/00_architecture_overview/#runtime-execution-phase","title":"Runtime Execution Phase","text":"<p>This phase shows how workflows execute and process data:</p> <pre><code>sequenceDiagram\n    participant Execution as Execution Layer\n    participant Runtime as Container Runtime\n    participant CLI as CLI Layer\n    participant Algorithm as Algorithm Layer\n    participant Storage as File System\n\n    Execution-&gt;&gt;Runtime: Schedule container execution\n    Runtime-&gt;&gt;CLI: Invoke CLI commands\n    CLI-&gt;&gt;Algorithm: Call algorithm functions\n    Algorithm-&gt;&gt;Storage: Write intermediate results\n    Runtime-&gt;&gt;Runtime: Process data through pipeline\n    Runtime-&gt;&gt;Storage: Write final outputs</code></pre>"},{"location":"architecture/00_architecture_overview/#key-architecture-steps","title":"Key Architecture Steps","text":"<p>The complete flow can be summarized in these key steps:</p> <ol> <li>Configuration Layer defines parameters for all layers</li> <li>Module Layer generates compute graphs from configuration</li> <li>Pipeline Layer composes modules into workflows</li> <li>Execution Layer translates workflows to backend-specific formats</li> <li>Container runtime executes the workflows</li> <li>CLI Layer processes commands and invokes algorithms</li> <li>Algorithm Layer performs core processing functions</li> <li>Results are stored in configured locations</li> </ol>"},{"location":"architecture/00_architecture_overview/#implementation-organization","title":"Implementation Organization","text":"<p>StarryNight is organized as a monorepo with four main packages:</p>"},{"location":"architecture/00_architecture_overview/#starrynight-core-starrynightsrcstarrynight","title":"StarryNight Core (<code>/starrynight/src/starrynight/</code>)","text":"<p>The foundation of the platform providing specialized algorithms for microscopy image analysis:</p> <ul> <li>CLI Tools: Command-line interfaces for each algorithm</li> <li>Algorithms: Image processing algorithms for microscopy data</li> <li>Modules System: Standardized module structure for algorithm implementation</li> <li>Parsers: File path parsing and metadata extraction</li> <li>Utilities: Common functions for file handling, data transformation, etc.</li> </ul>"},{"location":"architecture/00_architecture_overview/#pipecraft-pipecraftsrcpipecraft","title":"PipeCraft (<code>/pipecraft/src/pipecraft/</code>)","text":"<p>The pipeline compiler and execution framework:</p> <ul> <li>Pipeline Definition: Python API for defining computational workflows</li> <li>Node System: Individual processing steps as configurable nodes</li> <li>Backend Abstraction: Support for local, Docker, and AWS Batch execution</li> <li>Template System: Pre-defined templates for common backends</li> </ul>"},{"location":"architecture/00_architecture_overview/#conductor-job-orchestration-and-api","title":"Conductor (Job orchestration and API)","text":"<p>Manages the execution environment:</p> <ul> <li>REST API: API for job management and monitoring</li> <li>Database: Storage for project configurations and job results</li> <li>Job Management: Scheduling, execution, and monitoring of jobs</li> <li>WebSockets: Real-time updates on job status</li> </ul>"},{"location":"architecture/00_architecture_overview/#canvas-web-ui-for-pipeline-configuration","title":"Canvas (Web UI for pipeline configuration)","text":"<p>The web-based user interface handles:</p> <ul> <li>Job Configuration: Interface for setting up and parameterizing workflows</li> <li>Run Monitoring: Real-time status updates and logs</li> <li>Results Visualization: Interactive exploration of outputs</li> <li>File Management: Interface for managing input and output files</li> </ul>"},{"location":"architecture/00_architecture_overview/#extension-points","title":"Extension Points","text":"<p>StarryNight provides these key extension points:</p> <ul> <li>New algorithms: <code>/starrynight/algorithms/</code></li> <li>New CLI commands: <code>/starrynight/cli/</code></li> <li>New modules: <code>/starrynight/modules/</code></li> <li>New experiment types: <code>/starrynight/experiments/</code></li> <li>New backends: <code>/pipecraft/backend/</code></li> </ul> <p>For examples of how to extend these components, see the Practical Integration document.</p>"},{"location":"architecture/00_architecture_overview/#key-terms","title":"Key Terms","text":"<ul> <li>Algorithm Set: Group of related processing functions</li> <li>Module: Standardized component with inputs/outputs</li> <li>Pipeline: Composed workflow of modules</li> <li>Execution Backend: System that runs pipelines</li> <li>Experiment: Configuration for a specific workflow</li> <li>Bilayers: External schema system used by the Module Layer to standardize input/output specifications and enable UI generation</li> <li>Compute Graph: Definition of operations and connections</li> </ul> <p>For practical examples showing how these concepts work together, see Practical Integration</p>"},{"location":"architecture/01_algorithm_layer/","title":"StarryNight Algorithm Layer","text":""},{"location":"architecture/01_algorithm_layer/#overview","title":"Overview","text":"<p>The algorithm layer forms the foundation of the StarryNight framework. Algorithms are pure Python functions that implement specific image processing operations and pipeline generation capabilities without dependencies on higher-level StarryNight components.</p> <p>This document explains the structure and organization of the algorithm layer.</p> <p>CellProfiler Context</p> <p>While many examples in this document reference CellProfiler-specific functions, the algorithm layer design extends beyond CellProfiler. The architecture supports various algorithm types including indexing, inventory management, and other non-CellProfiler operations as detailed in the \"Beyond CellProfiler\" section.</p>"},{"location":"architecture/01_algorithm_layer/#purpose","title":"Purpose","text":"<p>Algorithms in StarryNight serve several essential purposes:</p> <ol> <li>Image Processing Logic - Implementing the core computational steps</li> <li>Pipeline Generation - Creating CellProfiler pipeline files programmatically</li> <li>LoadData Creation - Generating CSV files that tell CellProfiler how to load images</li> <li>Pipeline Execution - Running processing steps on prepared data</li> </ol> <p>By focusing solely on the core computational logic without higher-level concerns, the algorithm layer maintains simplicity and testability.</p>"},{"location":"architecture/01_algorithm_layer/#design-principles","title":"Design Principles","text":"<p>Algorithms in StarryNight follow these key design principles:</p> <ul> <li>Independence: No dependencies on other StarryNight components</li> <li>Clear Boundaries: Well-defined inputs, outputs, and side effects</li> <li>Focused Responsibility: Each algorithm performs a specific task</li> <li>Portability: Can be used in any context without modification</li> <li>Explicit State: Avoid global state or implicit dependencies</li> </ul> <p>The most important characteristic of the algorithm layer is its complete independence from the rest of the system.</p>"},{"location":"architecture/01_algorithm_layer/#algorithm-sets-structure","title":"Algorithm Sets Structure","text":"<p>Algorithms are organized into \"algorithm sets\" -- groups of related functions that collectively handle a specific pipeline stage.</p>"},{"location":"architecture/01_algorithm_layer/#common-pattern","title":"Common Pattern","text":"<p>Most algorithm sets follow a consistent pattern with three key function types:</p> <p>Common patterns</p> 1: LoadData Generation2: Pipeline Generation3: Execution <p>Functions that create CSV files defining which images to process</p> <ul> <li>Typically named <code>gen_&lt;algorithm&gt;_load_data_by_batch_plate()</code></li> <li>These functions identify relevant images from indexes or metadata</li> <li>They organize images by batch, plate, well, and site</li> <li>They output CSV files with paths and metadata CellProfiler needs to load images</li> <li>LoadData files contain metadata, filenames, paths, and frame information</li> </ul> <p>Functions that create processing pipeline definitions</p> <ul> <li>Typically named <code>gen_&lt;algorithm&gt;_cppipe_by_batch_plate()</code></li> <li>These functions programmatically create processing pipelines</li> <li>They configure pipeline modules with appropriate parameters</li> <li>They often infer parameters from sample LoadData files</li> <li>They output pipeline files in formats like .cppipe or .json</li> </ul> <p>Functions that run the pipeline on the loaded data</p> <ul> <li>Often using <code>run_cp_parallel()</code> or similar functions</li> <li>These functions handle running the generated pipelines against the data</li> <li>They manage parallel execution for performance</li> <li>They handle resource allocation and cleanup</li> <li>They organize outputs according to the experimental structure</li> </ul> <p>This pattern provides a clear separation of concerns even within the algorithm layer itself.</p>"},{"location":"architecture/01_algorithm_layer/#common-across-algorithm-sets","title":"Common Across Algorithm Sets","text":"<p>While each algorithm set handles different processing stages, they share common characteristics:</p> <ol> <li>Input/Output Pattern - Each algorithm expects specific inputs and produces well-defined outputs</li> <li>Parameter Handling - Consistent parameter passing and validation</li> <li>Path Handling - Using standard approaches for file paths</li> <li>Error Handling - Consistent approaches to error conditions</li> </ol>"},{"location":"architecture/01_algorithm_layer/#implementation-patterns","title":"Implementation Patterns","text":"<p>Algorithms employ several recurring implementation patterns:</p> <p>Implementation patterns</p> Sample Data InferencePath HandlingProcessing Organization <p>This pattern primarily applies to pipeline generation algorithms. When creating pipeline definitions, these algorithms:</p> <ol> <li>Read a sample LoadData file</li> <li>Extract channel names, cycle counts, and other metadata</li> <li>Use this information to configure the pipeline appropriately</li> </ol> <p>This approach allows pipeline generation algorithms to adapt to different experimental contexts without requiring all parameters to be specified explicitly.</p> <p>This pattern applies to all algorithm types. All algorithms use the <code>cloudpathlib</code> library's <code>AnyPath</code> class, which provides a consistent interface for:</p> <ul> <li>Local file paths</li> <li>Cloud storage paths (S3, etc.)</li> <li>Relative and absolute paths</li> </ul> <p>This abstraction enables algorithms to work with data regardless of its location, which is essential for all algorithm types from LoadData generation to execution.</p> <p>This pattern applies primarily to LoadData generation and execution algorithms. These algorithms organize processing by experimental hierarchy:</p> <ul> <li>Plate - A multi-well plate containing experimental samples</li> <li>Well - A compartment within a plate with one experimental condition</li> <li>Site - A microscope field of view capturing a region of a well</li> <li>Batch - A group of plates</li> </ul> <p>Algorithms typically group by batch-plate, batch-plate-well, or batch-plate-well-site depending on processing needs.</p>"},{"location":"architecture/01_algorithm_layer/#beyond-cellprofiler","title":"Beyond CellProfiler","text":"<p>While many algorithm sets focus on CellProfiler integration, others serve different purposes:</p> <ul> <li>Indexing - Creating indexes of available data</li> <li>Inventory - Managing metadata about available data</li> <li>Quality Control - Analyzing results for quality issues</li> <li>Feature Selection - Identifying informative features</li> <li>Data Visualization - Creating visualizations of results</li> </ul> <p>These non-CellProfiler algorithm sets use the same architectural principles but may not follow the three-part pattern of LoadData generation, pipeline generation, and execution.</p>"},{"location":"architecture/01_algorithm_layer/#algorithm-complexity-and-decision-points","title":"Algorithm Complexity and Decision Points","text":"<p>Pipeline generation algorithms can incorporate conditional logic based on user requirements. For example, these algorithms might add or remove specific modules based on flags that indicate:</p> <ul> <li>Whether to handle blurry images</li> <li>Whether to remove debris</li> <li>Which quality control steps to include</li> </ul> <p>This flexibility allows pipeline generation algorithms to adapt to different experimental needs while maintaining their functional structure.</p>"},{"location":"architecture/01_algorithm_layer/#code-examples","title":"Code Examples","text":"<p>The following examples are simplified pseudocode based on actual StarryNight implementations but edited for clarity. They demonstrate the typical patterns found in algorithm sets.</p> <p>Code Examples</p> Example 1: LoadData GenerationExample 2: Pipeline GenerationExample 3: Pipeline Execution <pre><code>def gen_algorithm_load_data_by_batch_plate(\n    index_path: Path | CloudPath,\n    out_path: Path | CloudPath,\n    path_mask: str | None,\n    for_special_type: bool = False,\n) -&gt; None:\n    \"\"\"Generate LoadData CSV files for a specific algorithm.\n\n    Reads image metadata from an index, organizes by batch/plate structure,\n    and writes LoadData CSV files for CellProfiler.\n    \"\"\"\n    # Read from index file (typically parquet format)\n    df = pl.read_parquet(index_path.resolve().__str__())\n\n    # Filter for relevant images based on criteria\n    if not for_special_type:\n        images_df = df.filter(pl.col(\"type_flag\").ne(True), pl.col(\"is_image\").eq(True))\n    else:\n        images_df = df.filter(pl.col(\"type_flag\").eq(True), pl.col(\"is_image\").eq(True))\n\n    # Organize images hierarchically (by batch, plate, etc.)\n    images_hierarchy_dict = gen_image_hierarchy(images_df)\n\n    # Handle path prefix for correct file resolution\n    default_path_prefix = images_df.select(\"prefix\").unique().to_series().to_list()[0]\n    if path_mask is None:\n        path_mask = default_path_prefix\n\n    # Generate LoadData files for each batch/plate combination\n    for batch in images_hierarchy_dict.keys():\n        for plate in images_hierarchy_dict[batch].keys():\n            if not for_special_type:\n                # Standard case\n                write_loaddata_csv_by_batch_plate(\n                    images_df, out_path, path_mask, batch, plate\n                )\n            else:\n                # Special case with cycle information\n                plate_cycles_list = get_cycles_by_batch_plate(images_df, batch, plate)\n                for cycle in plate_cycles_list:\n                    write_loaddata_csv_by_batch_plate_cycle(\n                        images_df, out_path, path_mask, batch, plate, cycle\n                    )\n</code></pre> <pre><code>def gen_algorithm_cppipe_by_batch_plate(\n    load_data_path: Path | CloudPath,\n    out_dir: Path | CloudPath,\n    workspace_path: Path | CloudPath,\n    special_option: bool = False,\n) -&gt; None:\n    \"\"\"Create a CellProfiler pipeline programmatically.\n\n    Reads a sample LoadData file to determine parameters,\n    then constructs a pipeline with appropriate modules.\n    \"\"\"\n    # Ensure output directory exists\n    out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Find appropriate LoadData files to use as samples\n    if not special_option:\n        type_suffix = \"standard\"\n        files_by_hierarchy = get_files_by([\"batch\"], load_data_path, \"*.csv\")\n    else:\n        type_suffix = \"special\"\n        files_by_hierarchy = get_files_by([\"batch\", \"plate\"], load_data_path, \"*.csv\")\n\n    # Get sample file for inferring parameters\n    _, files = flatten_dict(files_by_hierarchy)[0]\n\n    # Create pipeline with CellProfiler API\n    with CellProfilerContext(out_dir=workspace_path) as cpipe:\n        # Generate pipeline with appropriate modules based on options\n        cpipe = generate_specific_pipeline(cpipe, files[0], special_option)\n\n        # Save pipeline in multiple formats\n        filename = f\"algorithm_{type_suffix}.cppipe\"\n        with out_dir.joinpath(filename).open(\"w\") as f:\n            cpipe.dump(f)\n        filename = f\"algorithm_{type_suffix}.json\"\n        with out_dir.joinpath(filename).open(\"w\") as f:\n            json.dump(cpipe.json(), f)\n</code></pre> <pre><code>def run_cp_parallel(\n    uow_list: list[tuple[Path, Path]],\n    out_dir: Path,\n    plugin_dir: Path | None = None,\n    jobs: int = 20,\n) -&gt; None:\n    \"\"\"Run CellProfiler pipelines in parallel.\n\n    Takes a list of pipeline and LoadData file pairs (units of work),\n    then executes them with appropriate parallelism.\n\n    Parameters\n    ----------\n    uow_list : list of tuple of Path\n        List of tuples containing the paths to the pipeline and load data files.\n    out_dir : Path\n        Output directory path.\n    plugin_dir : Path\n        Path to cellprofiler plugin directory.\n    jobs : int, optional\n        Number of parallel jobs to use (default is 20).\n    \"\"\"\n    # Initialize execution environment (e.g., start Java VM for CellProfiler)\n    cellprofiler_core.utilities.java.start_java()\n\n    # Execute all units of work in parallel\n    # The 'parallel' function is a utility that runs the specified function\n    # on each item in the list with the given parameters\n    parallel(uow_list, run_cp, [out_dir, plugin_dir], jobs)\n</code></pre>"},{"location":"architecture/01_algorithm_layer/#algorithm-development","title":"Algorithm Development","text":"<p>For CellProfiler-related algorithm sets, developers typically need to:</p> <ol> <li>Create the appropriate functions following established patterns</li> <li>Implement LoadData generation</li> <li>Implement pipeline generation</li> <li>Implement execution logic</li> </ol> <p>For other algorithm types (indexing, inventory, quality control, etc.), the specific functions will vary based on purpose, but the underlying principles remain the same.</p> <p>Regardless of algorithm type, implementations should follow the established patterns and independence principles of the algorithm layer.</p>"},{"location":"architecture/01_algorithm_layer/#conclusion","title":"Conclusion","text":"<p>The algorithm layer provides the foundational capabilities of StarryNight through pure Python functions that implement core image processing logic. This organization enables flexible and extensible scientific image processing.</p> <p>Next: CLI Layer</p>"},{"location":"architecture/02_cli_layer/","title":"StarryNight CLI Layer","text":""},{"location":"architecture/02_cli_layer/#overview","title":"Overview","text":"<p>The CLI (Command Line Interface) layer in StarryNight provides command-line access to the underlying algorithms. It wraps pure algorithm functions with user-friendly interfaces, handles parameter parsing, and manages execution. Building upon the algorithm layer, the CLI layer serves as the primary direct interface for users to interact with StarryNight's capabilities through terminal commands.</p>"},{"location":"architecture/02_cli_layer/#purpose","title":"Purpose","text":"<p>The CLI layer serves several key purposes:</p> <ol> <li>User Access - Provides direct command-line access to StarryNight algorithms</li> <li>Parameter Handling - Converts command-line arguments to appropriate parameter types</li> <li>Path Management - Handles different path formats (local, cloud storage, relative, absolute)</li> <li>Command Organization - Structures commands into logical groups</li> <li>Documentation - Provides help text and usage examples</li> </ol> <p>The CLI layer directly imports algorithm functions and exposes them through command-line interfaces. For example, <code>analysis.py</code> in the CLI directory imports functions like <code>gen_analysis_load_data</code> and <code>gen_analysis_pipeline</code> from the algorithms layer and makes them accessible as CLI commands.</p>"},{"location":"architecture/02_cli_layer/#structure","title":"Structure","text":"<p>The CLI layer follows a consistent organization pattern:</p>"},{"location":"architecture/02_cli_layer/#command-groups","title":"Command Groups","text":"<p>Commands are organized into groups based on algorithm sets. For example:</p> <ul> <li><code>analysis</code> group for analysis algorithms</li> <li><code>illum</code> group for illumination algorithms</li> <li><code>segcheck</code> group for segmentation check algorithms</li> </ul>"},{"location":"architecture/02_cli_layer/#command-implementation","title":"Command Implementation","text":"<p>Each command typically:</p> <ol> <li>Imports algorithm functions</li> <li>Defines command parameters using Click decorators</li> <li>Implements a function that calls the underlying algorithm</li> <li>Manages path conversions and validations</li> </ol>"},{"location":"architecture/02_cli_layer/#click-library-integration","title":"Click Library Integration","text":"<p>StarryNight uses the Click library for CLI implementation. Click provides a decorator-based approach to define commands, options, and arguments with clear help text. This allows for a clean separation between interface definition and implementation logic.</p>"},{"location":"architecture/02_cli_layer/#command-group-pattern","title":"Command Group Pattern","text":"<p>The pattern for creating new CLI command groups is consistent:</p> <pre><code>@click.group()\ndef analysis():\n    \"\"\"Commands for analysis pipelines.\"\"\"\n    pass\n\n@analysis.command()\n@click.option(\"--input-path\", required=True, help=\"Path to input data\")\n@click.option(\"--output-path\", required=True, help=\"Path for output\")\n# Additional options...\ndef generate_load_data(input_path, output_path, **options):\n    \"\"\"Generate load data file for analysis pipeline.\"\"\"\n    # Convert paths to AnyPath\n    input_path = AnyPath(input_path)\n    output_path = AnyPath(output_path)\n\n    # Call the algorithm function\n    return algorithms.analysis.gen_analysis_load_data(\n        input_path=input_path,\n        output_path=output_path,\n        **options\n    )\n</code></pre>"},{"location":"architecture/02_cli_layer/#path-handling","title":"Path Handling","text":"<p>The CLI converts user-provided path strings into standardized path objects that work with both local and cloud storage. Users may provide paths in various formats (local, relative, cloud) which the CLI normalizes using the cloudpathlib library:</p> <pre><code>from cloudpathlib import AnyPath\n\ndef cli_function(input_path, output_path):\n    # Convert string paths to path objects\n    input_path = AnyPath(input_path)\n    output_path = AnyPath(output_path)\n\n    # Call algorithm with consistent path interface\n    algorithm_function(input_path, output_path)\n</code></pre> <p>This approach provides a consistent interface regardless of where data is stored, allowing algorithms to work with both local files and cloud storage seamlessly.</p>"},{"location":"architecture/02_cli_layer/#flag-handling","title":"Flag Handling","text":"<p>The CLI layer also handles flags that control algorithm behavior. For example, some commands support different assay types (SBS or CP) through flags rather than separate commands. This approach simplifies the user experience by providing a consistent interface while allowing the CLI to invoke different underlying algorithm functions based on the flags provided.</p>"},{"location":"architecture/02_cli_layer/#integration-with-mainpy","title":"Integration with main.py","text":"<p>The CLI layer integrates all command groups into a single entry point in <code>main.py</code>. Each algorithm set defines its own command group, which is then imported and registered with the main CLI application. This approach creates a modular structure where new algorithm sets can be easily added without modifying existing code.</p> <p>This creates a consistent, hierarchical command structure:</p> <pre><code>starrynight\n\u251c\u2500\u2500 analysis\n\u2502   \u251c\u2500\u2500 loaddata\n\u2502   \u2514\u2500\u2500 cppipe\n\u251c\u2500\u2500 illum\n\u2502   \u251c\u2500\u2500 calc\n\u2502   \u2502   \u251c\u2500\u2500 loaddata\n\u2502   \u2502   \u2514\u2500\u2500 cppipe\n\u2502   \u2514\u2500\u2500 apply\n\u2502       \u251c\u2500\u2500 loaddata\n\u2502       \u2514\u2500\u2500 cppipe\n\u251c\u2500\u2500 segcheck\n\u2514\u2500\u2500 [other commands...]\n</code></pre>"},{"location":"architecture/02_cli_layer/#creating-new-cli-commands","title":"Creating New CLI Commands","text":"<p>To create a new CLI command, developers follow this pattern:</p> <ol> <li>Create a new file in the CLI directory (or use an existing one)</li> <li>Import the relevant algorithm functions</li> <li>Create a click command group</li> <li>Define commands using Click decorators</li> <li>Implement the command functions to call algorithms</li> <li>Add the command group to main.py</li> </ol> <p>This pattern helps maintain a clean separation between different algorithm sets (groups of related functions that collectively handle a specific pipeline stage) while providing a unified command structure to users. Each algorithm set can evolve independently without affecting others, which simplifies maintenance and development.</p>"},{"location":"architecture/02_cli_layer/#cli-limitations","title":"CLI Limitations","text":"<p>While the CLI is powerful for direct algorithm access, it has limitations compared to higher-level StarryNight components:</p> <ol> <li>No containerization - Runs in the current environment without isolation</li> <li>Manual parameter specification - All parameters must be specified directly</li> <li>No workflow composition - Each command runs independently</li> <li>No parallelism management - Multi-step processes must be coordinated manually</li> </ol> <p>These limitations are addressed by the module and pipeline layers.</p>"},{"location":"architecture/02_cli_layer/#example-analysispy","title":"Example: analysis.py","text":"<p>The <code>cli/analysis.py</code> file demonstrates the CLI pattern (simplified excerpt):</p> <pre><code>import click\nfrom cloudpathlib import AnyPath\n\nfrom starrynight.algorithms.analysis import (\n    gen_analysis_cppipe_by_batch_plate,\n    gen_analysis_load_data_by_batch_plate,\n)\n\n@click.command(name=\"loaddata\")\n@click.option(\"-i\", \"--index\", required=True)\n@click.option(\"-o\", \"--out\", required=True)\n@click.option(\"-c\", \"--corr_images\", required=True)\n@click.option(\"-p\", \"--comp_images\", required=True)\n@click.option(\"-m\", \"--path_mask\", default=None)\ndef gen_analysis_load_data(\n    index: str,\n    out: str,\n    corr_images: str,\n    comp_images: str,\n    path_mask: str | None,\n) -&gt; None:\n    \"\"\"Generate analysis loaddata file.\"\"\"\n    gen_analysis_load_data_by_batch_plate(\n        AnyPath(index),\n        AnyPath(out),\n        path_mask,\n        AnyPath(corr_images),\n        AnyPath(comp_images),\n    )\n\n@click.group()\ndef analysis() -&gt; None:\n    \"\"\"Analysis commands.\"\"\"\n    pass\n\nanalysis.add_command(gen_analysis_load_data)\n# The actual file includes additional commands like 'cppipe'\n</code></pre>"},{"location":"architecture/02_cli_layer/#cli-usage-examples","title":"CLI Usage Examples","text":"<p>Example 1: Generate load data for analysis (local storage) <pre><code>starrynight analysis loaddata \\\n  -i /path/to/index \\\n  -o /path/to/output \\\n  -c /path/to/corrected_images \\\n  -p /path/to/compensated_images\n</code></pre></p> <p>Example 2: Generate a CellProfiler pipeline <pre><code>starrynight analysis cppipe \\\n  -l /path/to/loaddata \\\n  -o /path/to/output \\\n  -w /path/to/workspace \\\n  -b /path/to/barcode.csv \\\n  -n DAPI \\\n  -e CellMask \\\n  -m MitoTracker\n</code></pre></p> <p>Example 3: Working with cloud storage data <pre><code>starrynight illum calc loaddata \\\n  -i s3://bucket-name/path/to/index \\\n  -o s3://bucket-name/path/to/output\n</code></pre></p>"},{"location":"architecture/02_cli_layer/#conclusion","title":"Conclusion","text":"<p>The CLI layer sits directly above the algorithm layer in the StarryNight architecture, providing a straightforward interface to the underlying algorithms while remaining below the more sophisticated module layer. By following consistent patterns with the Click library and using cloudpathlib for path handling, the CLI offers a user-friendly experience while maintaining flexibility across different storage environments.</p> <p>While the CLI is only one way to access StarryNight functionality (alongside notebooks and the UI), it serves as an important bridge between pure algorithm functions and higher-level abstractions. It provides direct access for testing and scripting while establishing patterns that inform the module layer discussed next.</p> <p>Next: Module Layer</p>"},{"location":"architecture/03_module_layer/","title":"StarryNight Module Layer","text":""},{"location":"architecture/03_module_layer/#overview","title":"Overview","text":"<p>The module layer provides a standardized abstraction that bridges the CLI layer (command-line interfaces) and the pipeline layer (complex workflows). Its architectural significance lies in separating what should be done (specifications) from how it should be structured (compute graphs) and how it should be executed (backends).</p> <p>Modules define both specifications and compute graphs without performing computation. This separation enables backend-agnostic execution (local, cloud) while maintaining a consistent interface for pipeline composition. The module layer integrates with Bilayers for specifications and Pipecraft for compute graphs, creating a powerful abstraction that enables containerized, reproducible execution. Importantly, modules don't directly call algorithm functions but instead invoke CLI commands that in turn execute the underlying algorithms.</p>"},{"location":"architecture/03_module_layer/#purpose-and-advantages","title":"Purpose and Advantages","text":"<p>The module layer offers significant advantages over directly using CLI commands:</p>"},{"location":"architecture/03_module_layer/#practical-benefits","title":"Practical Benefits","text":"<ul> <li>Standardization - Consistent interfaces across different algorithm types</li> <li>Composability - Modules can be connected into larger workflows</li> <li>Containerization - Built-in container specification for reproducibility</li> <li>Documentation - Structured approach to capturing metadata and citations</li> </ul>"},{"location":"architecture/03_module_layer/#architectural-significance","title":"Architectural Significance","text":"<ul> <li>Backend Independence - Run the same module on different execution systems</li> <li>Inspection - Examine inputs, outputs, and operations before execution</li> <li>Automated UI Generation - Specifications support interface generation</li> <li>Complex Workflow Creation - Enables automatic generation of sophisticated execution plans</li> </ul>"},{"location":"architecture/03_module_layer/#the-dual-nature-of-modules","title":"The Dual Nature of Modules","text":"<p>The module layer's defining characteristic is its dual focus on specifications and compute graphs:</p> <ol> <li> <p>Specification (Spec) - Defines what a module does:</p> <ul> <li>Input ports with types, descriptions, and validation rules</li> <li>Output ports with types and descriptions</li> <li>Documentation and metadata</li> <li>Parameter constraints and defaults</li> </ul> </li> <li> <p>Compute Graph - Defines how operations should be structured:</p> <ul> <li>Container configurations</li> <li>Command construction</li> <li>Input/output relationships</li> <li>Execution sequence</li> </ul> </li> </ol> <p>Crucially, modules define computation but don't perform it. This separation enables inspection and modification before execution, and allows the same module to run on different platforms without changing its definition.</p>"},{"location":"architecture/03_module_layer/#module-sets-organization","title":"Module Sets Organization","text":"<p>Just as the CLI layer is organized into command groups that map to algorithm sets, the module layer is organized into corresponding module sets. For each CLI command group, there is typically a matching module set that provides the same functionality with the added abstraction of the module layer.</p> <p>Each module set typically follows a consistent pattern with three types of modules:</p> <ol> <li>Load Data Modules - Generate data loading configurations<ul> <li>Define which images to process</li> <li>Create CSV files for CellProfiler to locate images</li> <li>Organize data by batch, plate, well, and site</li> </ul> </li> <li>Pipeline Generation Modules - Create processing pipeline definitions<ul> <li>Generate CellProfiler pipeline files</li> <li>Configure pipeline parameters based on experiment settings</li> <li>Define processing operations</li> </ul> </li> <li>Execution Modules - Execute the pipeline on prepared data<ul> <li>Run pipelines with appropriate parallelism</li> <li>Manage resource allocation</li> <li>Organize outputs according to experimental structure</li> </ul> </li> </ol> <p>This pattern mirrors the organization of algorithms and CLI commands, but adds the standardized abstraction and container orchestration capabilities of the module layer.</p> <p>Common module sets include:</p> <ol> <li>CP Modules - For Cell Painting workflows:<ul> <li><code>cp_illum_calc</code> - Illumination calculation modules</li> <li><code>cp_illum_apply</code> - Illumination correction modules</li> <li><code>cp_segcheck</code> - Segmentation check modules</li> </ul> </li> <li>SBS Modules - For Sequencing By Synthesis workflows:<ul> <li><code>sbs_illum_calc</code> - SBS illumination calculation modules</li> <li><code>sbs_illum_apply</code> - SBS illumination correction modules</li> <li><code>sbs_align</code> - SBS alignment modules</li> <li><code>sbs_preprocess</code> - SBS preprocessing modules</li> </ul> </li> <li>Common Modules - For general operations:<ul> <li><code>gen_index</code> - Index generation modules</li> <li><code>gen_inv</code> - Inventory management modules</li> <li><code>analysis</code> - Analysis modules</li> </ul> </li> </ol>"},{"location":"architecture/03_module_layer/#module-implementation","title":"Module Implementation","text":"<p>A module is implemented as a Python class that inherits from <code>StarryNightModule</code>. Each module implementation follows a consistent structure with several key components:</p> <ol> <li>Unique Identifier - A string property that uniquely identifies the module</li> <li>Spec Definition - A property method that defines the module's specification using Bilayers</li> <li><code>from_config</code> Method - A class method for configuration from experiment and data configurations</li> <li>Compute Graph Generation - A method that creates the Pipecraft pipeline defining the computation structure</li> </ol> <p>This structure ensures that all modules provide the same capabilities, making them consistent and interchangeable at the architectural level.</p>"},{"location":"architecture/03_module_layer/#example-segmentation-check-module","title":"Example: Segmentation Check Module","text":"<p>Below is an example of a module that generates a CellProfiler pipeline for segmentation check. This example illustrates all the key components of a module implementation:</p> <pre><code>class CPSegcheckGenCPPipeModule(StarrynightModule):\n    \"\"\"CP segmentation check generate cppipe module.\"\"\"\n\n    @property\n    def uid(self) -&gt; str:\n        \"\"\"Return module unique id.\"\"\"\n        return \"cp_segcheck_gen_cppipe\"\n\n    def _spec(self) -&gt; SpecContainer:\n        \"\"\"Return module default spec.\"\"\"\n        return SpecContainer(\n            inputs={\n                \"loaddata_path\": TypeInput(\n                    name=\"Cellprofiler LoadData csvs\",\n                    type=TypeEnum.dir,\n                    description=\"Path to the LoadData csv.\",\n                    optional=False,\n                    value=self.data_config.workspace_path.joinpath(\n                        CP_SEGCHECK_CP_LOADDATA_OUT_PATH_SUFFIX\n                    )\n                    .resolve()\n                    .__str__(),\n                ),\n                \"workspace_path\": TypeInput(\n                    name=\"Workspace\",\n                    type=TypeEnum.dir,\n                    description=\"Workspace path.\",\n                    optional=True,\n                    value=self.data_config.workspace_path.joinpath(\n                        CP_SEGCHECK_OUT_PATH_SUFFIX\n                    )\n                    .resolve()\n                    .__str__(),\n                ),\n                \"nuclei_channel\": TypeInput(\n                    name=\"Nuclei channel\",\n                    type=TypeEnum.textbox,\n                    description=\"Channel to use for nuclei segmentation.\",\n                    optional=False,\n                    value=self.experiment.cp_config.nuclei_channel,\n                ),\n                \"cell_channel\": TypeInput(\n                    name=\"Cell channel\",\n                    type=TypeEnum.textbox,\n                    description=\"Channel to use for cell segmentation.\",\n                    optional=False,\n                    value=self.experiment.cp_config.cell_channel,\n                ),\n                \"use_legacy\": TypeInput(\n                    name=\"Use legacy pipeline\",\n                    type=TypeEnum.boolean,\n                    description=\"Flag for using legacy pipeline.\",\n                    optional=True,\n                    value=False,\n                ),\n            },\n            outputs={\n                \"cppipe_path\": TypeOutput(\n                    name=\"Cellprofiler pipeline\",\n                    type=TypeEnum.file,\n                    description=\"Generated pre segcheck cppipe files\",\n                    optional=False,\n                    value=self.data_config.workspace_path.joinpath(\n                        CP_SEGCHECK_CP_CPPIPE_OUT_PATH_SUFFIX\n                    )\n                    .resolve()\n                    .__str__(),\n                ),\n                \"notebook_path\": TypeOutput(\n                    name=\"QC notebook\",\n                    type=TypeEnum.notebook,\n                    description=\"Notebook for inspecting cellprofiler pipeline files\",\n                    optional=False,\n                ),\n            },\n            parameters=[],\n            display_only=[],\n            results=[],\n            exec_function=ExecFunction(\n                name=\"\",\n                script=\"\",\n                module=\"\",\n                cli_command=\"\",\n            ),\n            docker_image=None,\n            algorithm_folder_name=None,\n            citations=TypeCitations(\n                algorithm=[\n                    TypeAlgorithmFromCitation(\n                        name=\"Starrynight CP segmentation check generate cppipe module\",\n                        description=\"This module generates cppipe files for cp segmentation check module.\",\n                    )\n                ]\n            ),\n        )\n\n    def _create_uow(self) -&gt; list[UnitOfWork]:\n        \"\"\"Create units of work for Generate Index step.\n\n        Returns\n        -------\n        list[UnitOfWork]\n            List of unit of work.\n\n        \"\"\"\n        return []\n\n    def _create_pipe(self) -&gt; Pipeline:\n        \"\"\"Create pipeline for generating cpipe.\n\n        Returns\n        -------\n        Pipeline\n            Pipeline instance.\n\n        \"\"\"\n        spec = self.spec\n        cmd = [\n            \"starrynight\",\n            \"segcheck\",\n            \"cppipe\",\n            \"-l\",\n            spec.inputs[\"loaddata_path\"].value,\n            \"-o\",\n            spec.outputs[\"cppipe_path\"].value,\n            \"-w\",\n            spec.inputs[\"workspace_path\"].value,\n            \"-n\",\n            spec.inputs[\"nuclei_channel\"].value,\n            \"-c\",\n            spec.inputs[\"cell_channel\"].value,\n        ]\n\n        if spec.inputs[\"use_legacy\"].value is True:\n            cmd += [\n                \"--use_legacy\",\n            ]\n        gen_load_data_pipe = Seq(\n            [\n                Container(\n                    name=self.uid,\n                    input_paths={\n                        \"loaddata_path\": [spec.inputs[\"loaddata_path\"].value]\n                    },\n                    output_paths={\n                        \"cppipe_path\": [spec.outputs[\"cppipe_path\"].value]\n                    },\n                    config=ContainerConfig(\n                        image=\"ghrc.io/leoank/starrynight:dev\",\n                        cmd=cmd,\n                        env={},\n                    ),\n                ),\n            ]\n        )\n        return gen_load_data_pipe\n</code></pre> <p>This example illustrates several important aspects of module implementation:</p> <ol> <li>Module Identity - The <code>uid()</code> method provides a unique identifier</li> <li>Module Specification - The <code>_spec()</code> method defines inputs, outputs, and metadata and also set default values.</li> <li>Compute Graph Creation - The module uses the <code>_create_pipe</code> function to generate a Pipecraft pipeline</li> <li>CLI Command Construction - The module constructs a CLI command that will be executed in a container</li> <li>Container Specification - The module defines the container image and execution environment</li> </ol>"},{"location":"architecture/03_module_layer/#module-configuration","title":"Module Configuration","text":"<p>A powerful feature of the module layer is automatic configuration. This enables modules to be configured with minimal manual input by inferring paths and parameters from standard configurations:</p> <ol> <li>Data Configuration - Provides workspace paths, image paths, and scratch directories</li> <li>Experiment Configuration - Provides experiment-specific parameters like channel names, algorithms, and thresholds</li> <li>Default Path Inference - Creates conventional file paths based on workspace structure</li> </ol> <p>This automatic configuration approach has several advantages:</p> <ul> <li>Consistency - Ensures consistent file organization across projects</li> <li>Reduced Configuration - Minimizes parameters that users need to specify</li> <li>Standardization - Enforces standard naming and path conventions</li> <li>Modularity - Allows modules to be swapped without changing their configuration</li> </ul>"},{"location":"architecture/03_module_layer/#compute-graph-generation-with-pipecraft","title":"Compute Graph Generation with Pipecraft","text":"<p>The <code>_create_pipe</code> method is where modules generate their compute graphs using the Pipecraft library. This method defines the structure of the computation without actually executing it:</p> <ol> <li>CLI Command Construction - The module builds a command-line invocation of the underlying algorithm</li> <li>Container Configuration - Specifies the container image, environment, and resource requirements</li> <li>Input/Output Mapping - Defines how inputs and outputs are mapped to the container filesystem</li> <li>Execution Structure - Specifies the execution sequence (sequential, parallel, etc.)</li> </ol> <p>The compute graph provides a complete definition of how the operation should be executed, but remains independent of any specific execution technology. This separation allows the same module to be executed on different backends (local, cluster, cloud) without modification.</p>"},{"location":"architecture/03_module_layer/#specifications-with-bilayers","title":"Specifications with Bilayers","text":"<p>For the \"specification\" part of a module's dual nature, StarryNight uses Bilayers - an external schema system that standardizes how inputs, outputs, and metadata are defined.</p>"},{"location":"architecture/03_module_layer/#what-is-bilayers","title":"What is Bilayers?","text":"<p>Bilayers is an open-source specification and framework designed to make bioimage analysis algorithms accessible through auto-generated user interfaces. It bridges the gap between algorithm developers and biologists by providing:</p> <ul> <li>A standardized YAML-based configuration format for describing algorithm interfaces</li> <li>Automatic generation of web interfaces (Gradio) and Jupyter notebooks from these configurations</li> <li>A consistent way to package algorithms in Docker containers with their interfaces</li> <li>A schema validation system based on LinkML to ensure configurations are correct</li> </ul> <p>The Bilayers project enables algorithm developers to write a single configuration file that describes their tool's inputs, outputs, and parameters, and automatically get user-friendly interfaces without writing UI code.</p>"},{"location":"architecture/03_module_layer/#how-starrynight-integrates-bilayers","title":"How StarryNight Integrates Bilayers","text":"<p>StarryNight leverages the Bilayers specification system to standardize its module interfaces. The integration works through several mechanisms:</p> <ol> <li> <p>Schema Download and Synchronization: StarryNight maintains a local copy of the Bilayers validation schema, which is automatically downloaded from the Bilayers repository:    <pre><code># From starrynight/modules/common.py\nVALIDATE_SCHEMA_URL = \"https://raw.githubusercontent.com/bilayer-containers/bilayers/master/tests/test_config/validate_schema.yaml\"\n</code></pre></p> </li> <li> <p>Pydantic Model Generation: The Bilayers LinkML schema is converted into Pydantic models that StarryNight uses for runtime validation:    <pre><code>def update_module_schema() -&gt; None:\n    \"\"\"Download and update the module schema from bilayers.\"\"\"\n    schema_yaml = Path(__file__).parent.joinpath(\"validate_schema.yaml\")\n    schema_path = Path(__file__).parent.joinpath(\"schema.py\")\n    # Download schema and generate Pydantic models\n</code></pre></p> </li> <li> <p>SpecContainer Integration: Each StarryNight module defines its specification using the <code>SpecContainer</code> class, which is derived from the Bilayers schema. This ensures compatibility with the broader Bilayers ecosystem.</p> </li> </ol>"},{"location":"architecture/03_module_layer/#the-bilayers-specification-structure","title":"The Bilayers Specification Structure","text":"<p>The module layer uses Bilayers to create standardized definitions of:</p> <ul> <li>Input Specifications:<ul> <li>Type definitions (image, file, directory, array, measurement)</li> <li>Validation rules and constraints</li> <li>Default values and optional flags</li> <li>Descriptions for documentation</li> <li>CLI tag mappings for command generation</li> </ul> </li> <li>Output Specifications:<ul> <li>Output types and formats</li> <li>File naming patterns</li> <li>Directory structures</li> <li>Relationships to inputs</li> </ul> </li> <li>Parameter Definitions:<ul> <li>UI element types (checkbox, integer, float, dropdown, textbox)</li> <li>Value constraints and defaults</li> <li>Grouping for beginner/advanced modes</li> <li>Help text and documentation</li> </ul> </li> <li>Algorithm Metadata:<ul> <li>Citations and references</li> <li>Docker image specifications</li> <li>License information</li> <li>Algorithm descriptions</li> </ul> </li> </ul>"},{"location":"architecture/03_module_layer/#example-how-a-module-uses-bilayers","title":"Example: How a Module Uses Bilayers","text":"<p>When a StarryNight module implements its <code>_spec()</code> method, it returns a <code>SpecContainer</code> that follows the Bilayers schema:</p> <pre><code>def _spec(self) -&gt; SpecContainer:\n    return SpecContainer(\n        inputs={\n            \"input_image\": TypeInput(\n                name=\"input_image\",\n                type=TypeEnum.image,\n                description=\"Input microscopy image\",\n                cli_tag=\"-i\",\n                optional=False,\n            )\n        },\n        outputs={\n            \"processed_image\": TypeOutput(\n                name=\"processed_image\",\n                type=TypeEnum.image,\n                description=\"Processed output image\",\n            )\n        },\n        parameters=[\n            # Bilayers-compliant parameter definitions\n        ],\n        citations=TypeCitations(\n            algorithm=[\n                TypeAlgorithmFromCitation(\n                    name=\"Algorithm Name\",\n                    description=\"Algorithm description\",\n                )\n            ]\n        ),\n    )\n</code></pre>"},{"location":"architecture/03_module_layer/#benefits-of-using-bilayers","title":"Benefits of Using Bilayers","text":"<ol> <li>Standardization: All modules follow the same specification format, making them predictable and easy to understand.</li> <li>Interoperability: Because StarryNight uses the Bilayers specification, there's potential for:<ul> <li>Importing Bilayers-compatible tools from other projects</li> <li>Exporting StarryNight modules for use in other Bilayers-compatible systems</li> <li>Leveraging the broader Bilayers ecosystem of tools and interfaces</li> </ul> </li> <li>Automatic UI Generation: While StarryNight doesn't currently generate Gradio or Jupyter interfaces from these specs, the Bilayers-compliant specifications make this possible in the future.</li> <li>Validation: The LinkML-based schema provides robust validation of module specifications, catching configuration errors early.</li> <li>Documentation: The structured format ensures that all modules have consistent documentation for their inputs, outputs, and parameters.</li> </ol> <p>This standardized approach ensures consistent interface definitions across modules and enables potential future features like automatic UI generation from specifications.</p>"},{"location":"architecture/03_module_layer/#compute-graphs-with-pipecraft","title":"Compute Graphs with Pipecraft","text":"<p>For the \"compute graph\" part of a module's dual nature, StarryNight uses Pipecraft - a library that defines operations and their execution flow.</p> <p>Modules use Pipecraft to:</p> <ul> <li>Define operations that should be executed (typically in containers)</li> <li>Specify input and output paths</li> <li>Structure execution (sequential or parallel)</li> <li>Configure container environments</li> </ul> <p>A simple example of how a module creates a compute graph:</p> <pre><code># Create a pipeline with sequential execution\npipeline = Seq(\n    Container(\n        name=\"external_tool\",\n        input_paths={\n            \"some_input\": [...],\n        },\n        output_paths={\n            \"some_output\": [...]\n        },\n        config=ContainerConfig(\n            image=\"externaltoolimage\",\n            cmd=[\"externaltool\"],\n            env={},\n        ),\n    )\n)\n</code></pre> <p>More detailed coverage of Pipecraft's capabilities and architecture is provided in the Pipeline Layer section.</p>"},{"location":"architecture/03_module_layer/#module-usage","title":"Module Usage","text":"<p>Modules are typically used in two different contexts: direct usage for individual operations and pipeline composition for complete workflows.</p>"},{"location":"architecture/03_module_layer/#direct-module-usage","title":"Direct Module Usage","text":"<p>For individual module operations, modules can be created and executed directly:</p> <pre><code># Import necessary components\nfrom starrynight.config import DataConfig\nfrom starrynight.experiments.pcp_generic import PCPGenericExperiment\nfrom starrynight.modules.cp_segcheck import CPSegcheckGenCPPipeModule\nfrom pipecraft.backend.snakemake import SnakeMakeBackend, SnakeMakeConfig\n\n# Create configurations\ndata_config = DataConfig(\n    workspace_path=\"/path/to/workspace\",\n    images_path=\"/path/to/images\",\n    scratch_path=\"/path/to/scratch\"\n)\n\nexperiment = PCPGenericExperiment.from_index(\n    index_path=data_config.workspace_path / \"index.yaml\",\n    init_config={\n        \"nuclear_channel\": \"DAPI\",\n        \"cell_channel\": \"CellMask\"\n    }\n)\n\n# Create module\nsegcheck_module = CPSegcheckGenCPPipeModule(data_config, experiment)\n\n# Configure backend\nbackend_config = SnakeMakeConfig(\n    use_fluent_bit=False, print_exec=True, background=False\n)\n\n# Create backend\nexec_backend = SnakeMakeBackend(\n    segcheck_module.pipe, backend_config, exec_runs / \"run001\", exec_mounts\n)\n\n# Execute module\nrun = exec_backend.run()\nrun.wait()\n</code></pre>"},{"location":"architecture/03_module_layer/#pipeline-composition","title":"Pipeline Composition","text":"<p>For complex workflows, modules are typically composed into pipelines:</p> <pre><code># Create modules\nmodules = []\nmodules.append(GenIndexModule.from_config(data_config))\nmodules.append(GenInvModule.from_config(data_config))\nmodules.append(CPIllumCalcGenLoadDataModule.from_config(data_config, experiment))\nmodules.append(CPIllumCalcGenCPipeModule.from_config(data_config, experiment))\nmodules.append(CPIllumCalcRunModule.from_config(data_config, experiment))\n\n# Create pipeline\npipeline = Seq(modules)\n\n# Execute pipeline\nexec_backend = SnakeMakeBackend(\n    pipeline, backend_config, exec_runs / \"run001\", exec_mounts\n)\nrun = exec_backend.run()\nrun.wait()\n</code></pre> <p>In pipeline composition, modules are created and configured individually, but their compute graphs are combined into a larger structure that defines the complete workflow. This approach is discussed in detail in Pipeline Layer.</p>"},{"location":"architecture/03_module_layer/#creating-new-modules","title":"Creating New Modules","text":"<p>Creating a new module set involves implementing classes for each stage of processing:</p> <ol> <li>Plan the Module Set:<ul> <li>Identify the algorithm set to wrap</li> <li>Determine inputs, outputs, and parameters</li> <li>Design the module structure (typically load data, pipeline generation, and execution)</li> </ul> </li> <li>Create Module Classes:<ul> <li>Implement subclasses of <code>StarryNightModule</code> for each stage</li> <li>Define unique identifiers and specifications</li> <li>Implement <code>from_config</code> methods</li> <li>Create pipeline generation methods</li> </ul> </li> <li>Define Specifications:<ul> <li>Use Bilayers to define inputs, outputs, and parameters</li> <li>Document parameters with clear descriptions</li> <li>Define validation rules</li> </ul> </li> <li>Implement Pipeline Creation:<ul> <li>Use Pipecraft to define compute graphs</li> <li>Specify container configurations</li> <li>Map inputs and outputs properly</li> </ul> </li> <li>Test the Modules:<ul> <li>Test individual modules</li> <li>Test automatic configuration</li> <li>Test pipeline integration</li> </ul> </li> </ol>"},{"location":"architecture/03_module_layer/#relationship-to-adjacent-layers","title":"Relationship to Adjacent Layers","text":"<p>The module layer connects directly with two adjacent architectural layers:</p> <ol> <li>CLI Layer (below) - Modules invoke CLI commands in containers rather than directly calling algorithm functions</li> <li>Pipeline Layer (above) - Modules provide compute graphs that are composed into complete pipelines</li> </ol> <p>The module layer translates between CLI commands and complex workflows by providing a standardized interface and containerization approach for executing CLI commands in a pipeline context.</p>"},{"location":"architecture/03_module_layer/#module-registry-mechanism","title":"Module Registry Mechanism","text":"<p>StarryNight uses registry mechanisms to organize and discover available modules throughout the system. The module registry serves as a central catalog of all available modules, making it easy to:</p> <ol> <li>Discover available modules by name or type</li> <li>Instantiate modules programmatically</li> <li>Integrate modules into pipelines</li> <li>Extend the system with new modules</li> </ol>"},{"location":"architecture/03_module_layer/#registry-implementation","title":"Registry Implementation","text":"<p>The registry is implemented in <code>starrynight/modules/registry.py</code> as a Python dictionary mapping unique module identifiers to module classes:</p> <pre><code>MODULE_REGISTRY: dict[str, StarrynightModule] = {\n    # Generate inventory and index for the project\n    GenInvModule.uid(): GenInvModule,\n    GenIndexModule.uid(): GenIndexModule,\n    # CP illum calc\n    CPCalcIllumGenLoadDataModule.uid(): CPCalcIllumGenLoadDataModule,\n    CPCalcIllumGenCPPipeModule.uid(): CPCalcIllumGenCPPipeModule,\n    CPCalcIllumInvokeCPModule.uid(): CPCalcIllumInvokeCPModule,\n    # Additional modules...\n}\n</code></pre> <p>Each module defines its unique identifier through a static <code>uid()</code> method, which returns a string that serves as the module's registry key.</p>"},{"location":"architecture/03_module_layer/#registering-new-modules","title":"Registering New Modules","text":"<p>To add a new module to the system:</p> <ol> <li>Implement the module class following the module pattern</li> <li>Define a unique ID through the <code>uid()</code> static method</li> <li>Add the module to the <code>MODULE_REGISTRY</code> dictionary</li> </ol> <p>The registry pattern makes it easy to extend StarryNight with new module types while maintaining a clean, discoverable architecture.</p>"},{"location":"architecture/03_module_layer/#registry-usage","title":"Registry Usage","text":"<p>The module registry is used in several contexts:</p> <ol> <li>Pipeline Composition - Finding modules to include in a pipeline</li> <li>Experiment Configuration - Determining which modules to use for an experiment type</li> <li>Module Discovery - Listing available modules for user interfaces</li> <li>Dynamic Loading - Loading modules at runtime based on configuration</li> </ol> <p>Similar registry mechanisms exist for experiments (<code>experiments/registry.py</code>) and pipelines (<code>pipelines/registry.py</code>), creating a unified approach to component discovery across the system.</p>"},{"location":"architecture/03_module_layer/#conclusion","title":"Conclusion","text":"<p>The module layer's dual focus on specifications and compute graphs enables complex workflows to be defined simply and executed consistently across different environments. The registry mechanism provides a flexible way to organize and discover modules, facilitating extension and integration. The next section, Pipeline Layer, builds upon these abstractions to compose complete workflows from individual modules.</p>"},{"location":"architecture/04_pipeline_layer/","title":"StarryNight Pipeline Layer","text":""},{"location":"architecture/04_pipeline_layer/#overview","title":"Overview","text":"<p>The pipeline layer in StarryNight combines two critical aspects: Pipecraft integration for defining compute graphs and pipeline composition for building complete workflows. This layer represents the highest level of abstraction in the StarryNight architecture, sitting above the module layer and below the execution layer.</p> <p>The pipeline layer enables the creation, composition, and preparation of complex image processing workflows by connecting modules into complete pipelines with well-defined execution patterns. It establishes a clear separation between pipeline definition (what should be done and how it's structured) and execution (how it's actually run), which is a fundamental architectural principle of StarryNight.</p>"},{"location":"architecture/04_pipeline_layer/#purpose","title":"Purpose","text":"<p>The pipeline layer serves several key purposes in the StarryNight architecture:</p> <ol> <li>Compute Graph Definition - Creating structured representations of computational tasks</li> <li>Container Configuration - Specifying container settings for isolated execution</li> <li>Pipeline Composition - Combining operations into cohesive workflows</li> <li>Backend Independence - Separating pipeline definition from execution</li> <li>Parallelism Specification - Defining which operations can run in parallel</li> <li>Workflow Definition - Creating end-to-end processing workflows</li> <li>Module Coordination - Connecting modules in the correct sequence</li> <li>Execution Preparation - Preparing the complete pipeline for execution</li> </ol> <p>These capabilities enable the creation of complex, reproducible workflows for scientific image processing while maintaining a clean separation between definition and execution.</p>"},{"location":"architecture/04_pipeline_layer/#pipecraft-integration","title":"Pipecraft Integration","text":""},{"location":"architecture/04_pipeline_layer/#pipecraft-as-a-foundation","title":"Pipecraft as a Foundation","text":"<p>Pipecraft is a library that enables the creation of composable pipeline graphs in StarryNight. It provides primitives for defining computational operations, containers, and their connections, allowing modules to generate executable compute graphs without being tied to specific execution backends.</p> <p>Pipecraft serves as the technical foundation for pipeline construction in StarryNight, providing:</p> <ol> <li>A standardized framework for defining computational operations</li> <li>Clear abstractions for organizing operations (sequential, parallel)</li> <li>Container definitions independent of execution technology</li> <li>Input/output relationship specifications for pipeline steps</li> </ol>"},{"location":"architecture/04_pipeline_layer/#pipecraft-as-a-separate-package","title":"Pipecraft as a Separate Package","text":"<p>Pipecraft is a separate package within the StarryNight monorepo that provides the foundational pipeline construction capabilities. While most modules and algorithms live in the StarryNight core package, the pipeline construction functionality is implemented in Pipecraft, which:</p> <ul> <li>Provides a generic pipeline definition framework</li> <li>Is developed independently but within the same repository</li> <li>Implements a backend-agnostic approach to pipeline execution</li> <li>Could potentially be used by other systems beyond StarryNight</li> </ul> <p>This separation allows for focused development of pipeline construction capabilities while maintaining integration with the broader StarryNight framework.</p>"},{"location":"architecture/04_pipeline_layer/#pipecrafts-dual-role","title":"Pipecraft's Dual Role","text":"<p>Pipecraft serves two essential functions in the StarryNight architecture:</p> <ol> <li>Pipeline Definition: Creating compute graphs that represent operations and their relationships</li> <li>Execution Backend Support: Providing backends to execute the defined compute graphs</li> </ol> <p>This dual capability - both defining compute graphs AND providing execution backends - makes Pipecraft the central technical component that enables the separation between definition and execution, which is fundamental to the entire system.</p> <p>In the StarryNight architecture, Pipecraft integration happens inside modules. The module layer uses Bilayers to create specifications, and then interfaces with Pipecraft to define how those specifications should be executed as compute graphs. This marks the transition from specification (what needs to be done) to implementation (how to do it).</p>"},{"location":"architecture/04_pipeline_layer/#core-pipecraft-concepts","title":"Core Pipecraft Concepts","text":""},{"location":"architecture/04_pipeline_layer/#pipeline","title":"Pipeline","text":"<p>A <code>Pipeline</code> is the root object that represents the complete compute graph:</p> <pre><code>pipeline = pc.Pipeline()\n</code></pre> <p>This object serves as the container for all operations and their relationships.</p>"},{"location":"architecture/04_pipeline_layer/#sequential-and-parallel-blocks","title":"Sequential and Parallel Blocks","text":"<p>Pipecraft provides context managers for defining execution order:</p> <pre><code># Sequential execution (operations run one after another)\nwith pipeline.sequential() as seq:\n    # Operations defined here run in sequence\n\n# Parallel execution (operations can run simultaneously)\nwith pipeline.parallel() as par:\n    # Operations defined here can run in parallel\n</code></pre> <p>These blocks can be nested to create complex execution patterns with multiple layers of sequential and parallel operations.</p>"},{"location":"architecture/04_pipeline_layer/#container-nodes-and-configuration","title":"Container Nodes and Configuration","text":"<p>Container nodes represent operations that run in containerized environments:</p> <pre><code>seq.container(\n    name=\"operation_name\",\n    inputs={\n        \"input_name\": \"input_path\"\n    },\n    outputs={\n        \"output_name\": \"output_path\"\n    },\n    container_config=pc.ContainerConfig(\n        image=\"container/image:tag\",\n        command=[\"command\", \"arg1\", \"arg2\"]\n    )\n)\n</code></pre> <p>ContainerConfig objects specify the container execution environment:</p> <pre><code>pc.ContainerConfig(\n    image=\"cellprofiler/starrynight:latest\",\n    command=command,\n    environment={\n        \"ENV_VAR\": \"value\"\n    }\n)\n</code></pre> <p>A critical aspect of Pipecraft integration is container specification. This standardizes how containers should be executed, with specific images, commands, and environment variables. This containerization provides isolation and reproducibility.</p> <p>The container specification is runtime-agnostic. It defines what should be run in a container but leaves the specifics of how to execute that container to the execution backend. This abstraction allows the same pipeline to run with different container technologies (Docker, Singularity/Apptainer) depending on the execution environment.</p>"},{"location":"architecture/04_pipeline_layer/#simple-vs-complex-pipelines","title":"Simple vs. Complex Pipelines","text":"<p>For single-module operations, pipelines are simple and may contain just one node. This might seem redundant, but since compute graphs are composable, even simple operations follow the same pattern to enable later integration into more complex pipelines.</p> <p>More complex pipelines can connect multiple operations:</p> <pre><code>with pipeline.sequential() as seq:\n    # First operation\n    node1 = seq.container(\n        name=\"operation1\",\n        # config...\n    )\n\n    # Second operation\n    node2 = seq.container(\n        name=\"operation2\",\n        # config...\n    )\n\n    # Connect nodes\n    pipeline.connect(node1.outputs[\"result\"], node2.inputs[\"source\"])\n</code></pre>"},{"location":"architecture/04_pipeline_layer/#pipeline-composition","title":"Pipeline Composition","text":"<p>Pipeline composition is the final layer of abstraction in StarryNight, allowing multiple modules to be combined into complete workflows.</p>"},{"location":"architecture/04_pipeline_layer/#pipeline-composition-function","title":"Pipeline Composition Function","text":"<p>Pipeline composition is typically implemented as a function that takes configurations and returns a composed pipeline:</p> <pre><code>def create_pcp_generic_pipeline(\n    data_config: DataConfig,\n    experiment: PCPGenericExperiment\n) -&gt; Tuple[List[StarryNightModule], pc.Pipeline]:\n    \"\"\"\n    Create a complete PCP generic pipeline.\n\n    Parameters\n    ----------\n    data_config : DataConfig\n        Data configuration with paths\n    experiment : PCPGenericExperiment\n        Experiment configuration\n\n    Returns\n    -------\n    Tuple[List[StarryNightModule], pc.Pipeline]\n        List of configured modules and the composed pipeline\n    \"\"\"\n    # Implementation...\n</code></pre>"},{"location":"architecture/04_pipeline_layer/#module-creation-and-configuration","title":"Module Creation and Configuration","text":"<p>The pipeline composition function first creates and configures all necessary modules:</p> <pre><code># Create modules\nmodules = []\n\n# Index and inventory\nindex_module = GenIndexModule(data_config)\nmodules.append(index_module)\n\ninventory_module = GenInvModule(data_config)\nmodules.append(inventory_module)\n\n# Cell Painting modules\ncp_illum_calc_load_data = CPIllumCalcGenLoadDataModule(data_config, experiment)\nmodules.append(cp_illum_calc_load_data)\n\ncp_illum_calc_pipeline = CPIllumCalcGenCPipeModule(data_config, experiment)\nmodules.append(cp_illum_calc_pipeline)\n\n# More module creation...\n</code></pre> <p>Each module is created and configured based on data and experiment configurations. This approach ensures that all modules have the necessary information to function correctly within the pipeline.</p>"},{"location":"architecture/04_pipeline_layer/#building-the-pipeline-structure","title":"Building the Pipeline Structure","text":"<p>After creating modules, the function constructs the pipeline structure using Pipecraft's sequential and parallel blocks:</p> <pre><code># Create main pipeline\npipeline = pc.Pipeline()\n\nwith pipeline.sequential() as main_seq:\n    # First run index and inventory\n    with main_seq.sequential() as setup_seq:\n        setup_seq.add_pipeline(index_module.pipeline)\n        setup_seq.add_pipeline(inventory_module.pipeline)\n\n    # Then run CP and SBS in parallel\n    with main_seq.parallel() as parallel_proc:\n        # CP pipeline branch\n        with parallel_proc.sequential() as cp_seq:\n            cp_seq.add_pipeline(cp_illum_calc_load_data.pipeline)\n            cp_seq.add_pipeline(cp_illum_calc_pipeline.pipeline)\n            # Add more CP modules...\n\n        # SBS pipeline branch\n        with parallel_proc.sequential() as sbs_seq:\n            sbs_seq.add_pipeline(sbs_illum_calc_load_data.pipeline)\n            sbs_seq.add_pipeline(sbs_illum_calc_pipeline.pipeline)\n            # Add more SBS modules...\n\n    # Finally run analysis\n    with main_seq.sequential() as analysis_seq:\n        analysis_seq.add_pipeline(analysis_load_data.pipeline)\n        analysis_seq.add_pipeline(analysis_pipeline.pipeline)\n</code></pre> <p>This structure defines both sequencing and parallelism in the pipeline. Note how modules are not directly added to the pipeline; instead, their pipeline properties are added using <code>add_pipeline()</code>. This ensures that each module's compute graph is properly integrated into the overall pipeline.</p> <p>The composition function returns both the configured modules and the composed pipeline:</p> <pre><code>return modules, pipeline\n</code></pre> <p>This allows users to access both the individual modules (for inspection or modification) and the complete pipeline (for execution).</p> <p>Returning both the modules and the pipeline enables advanced usage patterns where modules can be individually inspected or modified, and then the pipeline can be recreated with the updated modules. This is particularly valuable for interactive development in notebook environments.</p>"},{"location":"architecture/04_pipeline_layer/#parallel-and-sequential-blocks","title":"Parallel and Sequential Blocks","text":"<p>The pipeline composition can express complex execution patterns through nested sequential and parallel blocks:</p> <pre><code>with pipeline.sequential() as main_seq:\n    # First step runs sequentially\n    with main_seq.sequential() as first_step:\n        # Operations that must run one after another\n        first_step.add_pipeline(module1.pipeline)\n        first_step.add_pipeline(module2.pipeline)\n\n    # Second step has parallel branches\n    with main_seq.parallel() as parallel_branches:\n        # Branch A runs sequentially\n        with parallel_branches.sequential() as branch_a:\n            branch_a.add_pipeline(module3.pipeline)\n            branch_a.add_pipeline(module4.pipeline)\n\n        # Branch B runs sequentially\n        with parallel_branches.sequential() as branch_b:\n            branch_b.add_pipeline(module5.pipeline)\n            branch_b.add_pipeline(module6.pipeline)\n\n    # Final step runs after all parallel branches complete\n    with main_seq.sequential() as final_step:\n        final_step.add_pipeline(module7.pipeline)\n</code></pre> <p>This structure allows for expressing complex workflows with appropriate dependencies and execution patterns.</p>"},{"location":"architecture/04_pipeline_layer/#expressing-parallelism","title":"Expressing Parallelism","text":"<p>The pipeline layer handles parallelism at multiple levels, allowing for efficient execution of complex workflows.</p>"},{"location":"architecture/04_pipeline_layer/#between-steps-parallelism","title":"Between Steps Parallelism","text":"<p>The pipeline composition can express parallelism between different steps. For example, Cell Painting and Sequencing By Synthesis processing can run independently until they reach a point where they need to be combined for analysis.</p> <p>This is expressed using parallel blocks in the pipeline composition:</p> <pre><code>with pipeline.parallel() as par:\n    # These branches run in parallel\n    with par.sequential() as branch_a:\n        # Operations in branch A\n\n    with par.sequential() as branch_b:\n        # Operations in branch B\n</code></pre>"},{"location":"architecture/04_pipeline_layer/#within-steps-parallelism","title":"Within Steps Parallelism","text":"<p>There's also parallelism within specific steps. For instance, illumination correction can be applied to multiple images simultaneously.</p> <p>This can be expressed using parallel operations within a module:</p> <pre><code>def create_pipeline(self) -&gt; pc.Pipeline:\n    \"\"\"Create compute graph with parallel processing.\"\"\"\n    pipeline = pc.Pipeline()\n\n    # Get samples from spec\n    samples = self.spec.inputs[\"samples\"].value\n\n    with pipeline.sequential() as seq:\n        # First create output directory\n        setup = seq.container(/* ... */)\n\n        # Then process samples in parallel\n        with seq.parallel() as par:\n            for sample in samples:\n                # Each sample processed in parallel\n                par.container(/* ... */)\n\n    return pipeline\n</code></pre> <p>This approach maximizes efficiency by processing independent items concurrently.</p>"},{"location":"architecture/04_pipeline_layer/#unit-of-work-api","title":"Unit of Work API","text":"<p>An experimental API for expressing finer-grained parallelism within operations is being developed. This \"unit of work\" API aims to express more detailed parallelism within operations, allowing for better resource utilization in complex workflows. However, this API is still under development and not yet widely used in production workflows.</p>"},{"location":"architecture/04_pipeline_layer/#pipeline-execution","title":"Pipeline Execution","text":"<p>Once a pipeline is composed, it can be executed using a backend:</p> <pre><code># Create pipeline\nmodules, pipeline = create_pcp_generic_pipeline(data_config, pcp_experiment)\n\n# Configure backend\nbackend_config = pc.SnakemakeBackendConfig(\n    use_opentelemetry=False,\n    print_exec=True\n)\nexec_backend = pc.SnakemakeBackend(backend_config)\n\n# Execute pipeline\nexec_backend.run(\n    pipeline=pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"complete_pipeline\"\n)\n</code></pre> <p>This executes the entire workflow in a single operation. The Snakemake backend, which is the primary execution backend in StarryNight, translates the Pipecraft pipeline into a Snakemake workflow and executes it. This process is covered in detail in the Execution Layer section.</p> <p>All pipeline steps run in containers, ensuring reproducibility and isolation. The Snakemake backend handles the execution of composed pipelines across various infrastructures that support Snakemake.</p>"},{"location":"architecture/04_pipeline_layer/#complete-examples","title":"Complete Examples","text":""},{"location":"architecture/04_pipeline_layer/#example-complete-pipeline-composition","title":"Example: Complete Pipeline Composition","text":"<p>Here's a detailed example of pipeline composition:</p> <pre><code>def create_pcp_generic_pipeline(data_config, experiment):\n    \"\"\"Create complete PCP generic pipeline.\"\"\"\n    # Create all modules\n    modules = []\n\n    # Setup modules\n    index_module = GenIndexModule.from_config(data_config)\n    modules.append(index_module)\n\n    inventory_module = GenInvModule.from_config(data_config)\n    modules.append(inventory_module)\n\n    # Cell Painting modules\n    cp_modules = []\n\n    cp_illum_calc_load_data = CPIllumCalcGenLoadDataModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_calc_load_data)\n    modules.append(cp_illum_calc_load_data)\n\n    cp_illum_calc_pipeline = CPIllumCalcGenCPipeModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_calc_pipeline)\n    modules.append(cp_illum_calc_pipeline)\n\n    cp_illum_calc_run = CPIllumCalcRunModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_calc_run)\n    modules.append(cp_illum_calc_run)\n\n    cp_illum_apply_load_data = CPIllumApplyGenLoadDataModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_apply_load_data)\n    modules.append(cp_illum_apply_load_data)\n\n    cp_illum_apply_pipeline = CPIllumApplyGenCPipeModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_apply_pipeline)\n    modules.append(cp_illum_apply_pipeline)\n\n    cp_illum_apply_run = CPIllumApplyRunModule.from_config(data_config, experiment)\n    cp_modules.append(cp_illum_apply_run)\n    modules.append(cp_illum_apply_run)\n\n    cp_segcheck_load_data = CPSegcheckGenLoadDataModule.from_config(data_config, experiment)\n    cp_modules.append(cp_segcheck_load_data)\n    modules.append(cp_segcheck_load_data)\n\n    cp_segcheck_pipeline = CPSegcheckGenCPipeModule.from_config(data_config, experiment)\n    cp_modules.append(cp_segcheck_pipeline)\n    modules.append(cp_segcheck_pipeline)\n\n    cp_segcheck_run = CPSegcheckRunModule.from_config(data_config, experiment)\n    cp_modules.append(cp_segcheck_run)\n    modules.append(cp_segcheck_run)\n\n    # SBS modules\n    sbs_modules = []\n\n    sbs_illum_calc_load_data = SBSIllumCalcGenLoadDataModule.from_config(data_config, experiment)\n    sbs_modules.append(sbs_illum_calc_load_data)\n    modules.append(sbs_illum_calc_load_data)\n\n    # Additional SBS modules...\n\n    # Analysis modules\n    analysis_modules = []\n\n    analysis_load_data = AnalysisGenLoadDataModule.from_config(data_config, experiment)\n    analysis_modules.append(analysis_load_data)\n    modules.append(analysis_load_data)\n\n    analysis_pipeline = AnalysisGenCPipeModule.from_config(data_config, experiment)\n    analysis_modules.append(analysis_pipeline)\n    modules.append(analysis_pipeline)\n\n    analysis_run = AnalysisRunModule.from_config(data_config, experiment)\n    analysis_modules.append(analysis_run)\n    modules.append(analysis_run)\n\n    # Create main pipeline\n    pipeline = pc.Pipeline()\n\n    with pipeline.sequential() as main_seq:\n        # First run index and inventory\n        with main_seq.sequential() as setup_seq:\n            setup_seq.add_pipeline(index_module.pipeline)\n            setup_seq.add_pipeline(inventory_module.pipeline)\n\n        # Then run CP and SBS in parallel\n        with main_seq.parallel() as parallel_proc:\n            # CP pipeline branch\n            with parallel_proc.sequential() as cp_seq:\n                for module in cp_modules:\n                    cp_seq.add_pipeline(module.pipeline)\n\n            # SBS pipeline branch\n            with parallel_proc.sequential() as sbs_seq:\n                for module in sbs_modules:\n                    sbs_seq.add_pipeline(module.pipeline)\n\n        # Finally run analysis\n        with main_seq.sequential() as analysis_seq:\n            for module in analysis_modules:\n                analysis_seq.add_pipeline(module.pipeline)\n\n    # Return modules and pipeline\n    return modules, pipeline\n</code></pre>"},{"location":"architecture/04_pipeline_layer/#notebook-example-for-pipeline-execution","title":"Notebook Example for Pipeline Execution","text":"<p>Here's a complete notebook example for creating and executing a pipeline:</p> <pre><code># Import necessary components\nfrom starrynight.config import DataConfig\nfrom starrynight.experiments.pcp_generic import PCPGenericExperiment\nfrom starrynight.pipelines.pcp_generic import create_pcp_generic_pipeline\nimport pipecraft as pc\nfrom pathlib import Path\n\n# Set up paths\nworkspace_path = Path(\"/path/to/workspace\")\nimages_path = Path(\"/path/to/images\")\nscratch_path = Path(\"/path/to/scratch\")\n\n# Create data config\ndata_config = DataConfig(\n    workspace_path=workspace_path,\n    images_path=images_path,\n    scratch_path=scratch_path\n)\n\n# Configure experiment\npcp_init_config = {\n    \"nuclear_channel\": \"DAPI\",\n    \"cell_channel\": \"CellMask\",\n    \"mito_channel\": \"MitoTracker\",\n    \"barcode_csv_path\": str(workspace_path / \"barcodes.csv\"),\n    \"image_overlap_percentage\": 10\n}\n\n# Create experiment\npcp_experiment = PCPGenericExperiment.from_index(\n    index_path=data_config.workspace_path / \"index.yaml\",\n    init_config=pcp_init_config\n)\n\n# Configure backend\nbackend_config = pc.SnakemakeBackendConfig(\n    use_opentelemetry=False,\n    print_exec=True\n)\nexec_backend = pc.SnakemakeBackend(backend_config)\n\n# Create pipeline\nprint(\"Creating pipeline...\")\nmodules, pipeline = create_pcp_generic_pipeline(data_config, pcp_experiment)\n\n# Run the pipeline\nprint(\"Running pipeline...\")\nexec_backend.run(\n    pipeline=pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"complete_pipeline\"\n)\n\nprint(\"Pipeline complete!\")\n</code></pre>"},{"location":"architecture/04_pipeline_layer/#modifying-modules-after-composition","title":"Modifying Modules After Composition","text":"<p>Even after composition, individual modules can be modified:</p> <pre><code># Create pipeline\nmodules, pipeline = create_pcp_generic_pipeline(data_config, pcp_experiment)\n\n# Find specific module to modify\nsegcheck_module = next(m for m in modules if m.module_id == \"cp_segcheck_gen_cppipe\")\n\n# Modify parameter\nsegcheck_module.spec.inputs[\"nuclear_channel\"].value = \"Modified_DAPI\"\n\n# Recreate pipeline with modified modules\n_, updated_pipeline = create_pcp_generic_pipeline(data_config, pcp_experiment, modules=modules)\n\n# Execute modified pipeline\nexec_backend.run(\n    pipeline=updated_pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"modified_pipeline\"\n)\n</code></pre> <p>This capability to modify modules and then recreate the pipeline highlights the flexibility and power of the StarryNight architecture.</p>"},{"location":"architecture/04_pipeline_layer/#the-power-of-automatic-generation","title":"The Power of Automatic Generation","text":"<p>One of the most significant benefits of the pipeline construction approach is its ability to generate complex execution plans automatically. The pipeline layer can generate sophisticated Snakemake workflows with hundreds of rules from a high-level pipeline definition.</p> <p>This automatic generation of complex execution plans is a key value of the entire architecture. It transforms abstract pipeline definitions into concrete, executable workflows without requiring manual creation of complex execution plans.</p> <p>Using a composed pipeline offers significant advantages over CLI usage:</p> <ol> <li>Dependency Management - Automatic handling of module dependencies</li> <li>Parallelism - Automatic parallel execution where possible</li> <li>Single Command - Execute the entire workflow with one command</li> <li>Resource Optimization - Better resource utilization across steps</li> <li>Unified Logging - Consolidated logging and monitoring</li> <li>Reproducibility - Containerized execution ensures consistency</li> <li>Scalability - Works from laptops to high-performance computing environments</li> </ol> <p>The pipeline approach provides automation, structure, and reproducibility beyond what's possible with direct CLI usage.</p>"},{"location":"architecture/04_pipeline_layer/#relationship-to-adjacent-layers","title":"Relationship to Adjacent Layers","text":"<p>The pipeline layer builds directly on the module layer below it and connects to the execution layer above it:</p> <ol> <li> <p>Module Layer (Below) - The pipeline layer composes modules defined in the module layer, using their compute graphs as building blocks for larger workflows.</p> </li> <li> <p>Execution Layer (Above) - The pipeline layer creates pipeline definitions that are executed by the execution layer, which translates them into specific execution technologies (like Snakemake).</p> </li> </ol> <p>This positioning makes the pipeline layer a critical bridge between individual module abstractions and concrete execution plans.</p>"},{"location":"architecture/04_pipeline_layer/#conclusion","title":"Conclusion","text":"<p>Pipeline construction and composition represent the highest level of abstraction in the StarryNight architecture. By leveraging Pipecraft for compute graph definition and providing a structured approach to composing complete workflows, StarryNight enables complex image processing pipelines to be defined, configured, and executed with clarity and flexibility.</p> <p>The key architectural achievement is the separation between pipeline definition and execution, allowing the same pipeline to run on different backends while maintaining a consistent definition approach. This separation, combined with the powerful composition capabilities, enables the automatic generation of complex execution plans from high-level abstractions.</p> <p>The pipeline construction capabilities bridge the gap between individual module execution and complete workflow automation, providing a powerful yet comprehensible system for scientific image processing.</p> <p>Next: Execution Layer</p>"},{"location":"architecture/05_execution_layer/","title":"StarryNight Execution Layer","text":""},{"location":"architecture/05_execution_layer/#overview","title":"Overview","text":"<p>The execution layer in StarryNight defines how modules and pipelines are executed in computing environments. This layer consists of two key components: the execution model, which handles how modules and pipelines are configured and executed in different contexts, and the Snakemake backend, which translates Pipecraft pipelines into concrete, reproducible workflows. Together, these components form the final layer in StarryNight's architecture, turning abstract pipeline definitions into actual running processes.</p>"},{"location":"architecture/05_execution_layer/#purpose","title":"Purpose","text":"<p>The execution layer serves several critical purposes in the StarryNight architecture:</p> <ol> <li>Execution Preparation - Preparing configured pipelines for runtime</li> <li>Instantiation - Creating configured module instances</li> <li>Backend Selection - Choosing appropriate execution backends</li> <li>Execution Control - Initiating and monitoring pipeline execution</li> <li>Result Management - Handling outputs and logs</li> <li>Workflow Translation - Converting Pipecraft pipelines to executable format</li> <li>Dependency Management - Handling dependencies between pipeline steps</li> <li>Container Execution - Managing execution of containerized operations</li> <li>Parallel Processing - Controlling parallel execution of independent steps</li> </ol> <p>This system provides the connection between abstract pipeline definitions and concrete execution in computing environments.</p>"},{"location":"architecture/05_execution_layer/#execution-model","title":"Execution Model","text":"<p>The execution model in the StarryNight execution layer defines how modules and pipelines are run in different contexts, with a particular focus on the notebook workflow.</p>"},{"location":"architecture/05_execution_layer/#notebook-workflow","title":"Notebook Workflow","text":"<p>The typical notebook workflow includes these key steps:</p>"},{"location":"architecture/05_execution_layer/#1-import-components","title":"1. Import Components","text":"<pre><code># Import necessary modules\nfrom starrynight.modules.inventory import GenInvModule\nfrom starrynight.modules.index import GenIndexModule\nfrom starrynight.modules.cp_illum_calc import CPIllumCalcGenLoadDataModule, CPIllumCalcGenCPipeModule\nfrom starrynight.config import DataConfig\nfrom starrynight.experiments.pcp_generic import PCPGenericExperiment\nimport pipecraft as pc\n</code></pre>"},{"location":"architecture/05_execution_layer/#2-configure-data-paths","title":"2. Configure Data Paths","text":"<pre><code># Set up data paths\nworkspace_path = \"/path/to/workspace\"\nimages_path = \"/path/to/images\"\nscratch_path = \"/path/to/scratch\"\n\n# Create data config\ndata_config = DataConfig(\n    workspace_path=workspace_path,\n    images_path=images_path,\n    scratch_path=scratch_path\n)\n</code></pre>"},{"location":"architecture/05_execution_layer/#3-configure-backend","title":"3. Configure Backend","text":"<pre><code># Configure Snakemake backend\nbackend_config = pc.SnakemakeBackendConfig(\n    use_opentelemetry=False,  # Disable telemetry for notebook\n    print_exec=True           # Print execution details\n)\n\n# Create backend instance\nexec_backend = pc.SnakemakeBackend(backend_config)\n</code></pre>"},{"location":"architecture/05_execution_layer/#module-configuration-and-execution","title":"Module Configuration and Execution","text":"<p>The execution model handles how modules are configured and run:</p> <pre><code># Create and run index module\ngen_index_mod = GenIndexModule(data_config)\nexec_backend.run(\n    gen_index_mod.pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"index\"\n)\n\n# Configure experiment\npcp_init_config = {\n    \"nuclear_channel\": \"DAPI\",\n    \"cell_channel\": \"CellMask\",\n    \"mito_channel\": \"MitoTracker\",\n    \"barcode_csv_path\": \"/path/to/barcodes.csv\",\n    \"image_overlap_percentage\": 10\n}\n\n# Create configured experiment\npcp_experiment = PCPGenericExperiment.from_index(\n    index_path=data_config.workspace_path / \"index.yaml\",\n    init_config=pcp_init_config\n)\n\n# Create and run pipeline module\nillum_load_data_mod = CPIllumCalcGenLoadDataModule(\n    data_config=data_config,\n    experiment=pcp_experiment\n)\n\nexec_backend.run(\n    illum_load_data_mod.pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"illum_load_data\"\n)\n</code></pre>"},{"location":"architecture/05_execution_layer/#backend-selection","title":"Backend Selection","text":"<p>The execution system uses backend implementations in Pipecraft. Currently, StarryNight primarily uses the Snakemake backend, configured with options such as:</p> <ol> <li>Telemetry Settings - Whether to use OpenTelemetry for logging</li> <li>Output Settings - How to display execution information</li> <li>Resource Settings - CPU, memory, and other resource limits</li> </ol>"},{"location":"architecture/05_execution_layer/#execution-artifacts","title":"Execution Artifacts","text":"<p>When a pipeline is executed, several artifacts are generated:</p>"},{"location":"architecture/05_execution_layer/#compiled-workflow","title":"Compiled Workflow","text":"<p>The compiled workflow (e.g., Snakefile) contains the full definition of the operations to be performed. This file includes rules for each operation, input/output specifications, container configuration, and command-line instructions.</p>"},{"location":"architecture/05_execution_layer/#execution-logs","title":"Execution Logs","text":"<p>The execution logs capture the entire execution process, including command outputs, errors, and runtime information. These logs provide a complete record of execution for troubleshooting and auditing.</p>"},{"location":"architecture/05_execution_layer/#results","title":"Results","text":"<p>The results of the execution are stored in configured output locations, as defined in the module specifications.</p>"},{"location":"architecture/05_execution_layer/#module-state-management","title":"Module State Management","text":"<p>An important aspect of the notebook workflow is module state management. The notebook environment maintains module state during its execution, allowing for iterative development and inspection. This enables users to inspect module configurations, modify parameters, and re-run operations without restarting the entire workflow.</p>"},{"location":"architecture/05_execution_layer/#snakemake-backend","title":"Snakemake Backend","text":"<p>The Snakemake backend is StarryNight's primary execution engine, responsible for translating Pipecraft pipelines into Snakemake workflows and executing them.</p> <p>Snakemake is a workflow management system that:</p> <ul> <li>Uses a Python-based language to define rules</li> <li>Manages dependencies between rules using input/output relationships</li> <li>Supports parallel execution of independent tasks</li> <li>Provides container integration (Docker, Singularity/Apptainer)</li> <li>Handles resource management and scheduling</li> </ul>"},{"location":"architecture/05_execution_layer/#backend-implementation-in-pipecraft","title":"Backend Implementation in Pipecraft","text":"<p>The Snakemake backend is implemented in the Pipecraft package rather than in the StarryNight core package. This architectural decision:</p> <ul> <li>Keeps backend implementation details separate from the scientific image processing logic</li> <li>Allows for multiple backends to be developed without modifying the core package</li> <li>Maintains a clean separation between pipeline definition and execution</li> </ul> <p>When StarryNight modules and pipelines are executed, they use the backend implementations from Pipecraft through a well-defined API.</p>"},{"location":"architecture/05_execution_layer/#the-aha-moment-of-automatic-generation","title":"The \"Aha Moment\" of Automatic Generation","text":"<p>Critical Point: The Snakemake backend delivers one of the most impressive capabilities of the StarryNight system - the automatic generation of complex workflow files. This automatic generation of complex Snakefiles from high-level abstractions is a central architectural achievement that demonstrates the value of the entire system.</p> <p>For developers who have written Snakemake files manually, seeing a complex 500-line Snakefile generated automatically from high-level module definitions provides an immediate understanding of the system's value. It exemplifies how the StarryNight architecture transforms simple, user-friendly abstractions into complex, reproducible workflows.</p>"},{"location":"architecture/05_execution_layer/#generated-snakefile-structure","title":"Generated Snakefile Structure","text":"<p>When a Pipecraft pipeline is compiled to a Snakefile, it generates a structure like this:</p> <pre><code># Generated Snakefile\n\nrule all:\n    input:\n        \"path/to/final/output.csv\"\n\nrule operation_name:\n    input:\n        input_name=\"path/to/input/file.csv\",\n        workspace=\"path/to/workspace\"\n    output:\n        pipeline=\"path/to/output/pipeline.cppipe\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"starrynight segcheck generate-pipeline --output-path {output.pipeline} --load-data {input.input_name} --nuclear-channel DAPI --cell-channel CellMask\"\n</code></pre> <p>The compiled Snakefile defines what inputs each rule expects, what outputs it will create, which container to use, and the actual command to invoke inside that container.</p>"},{"location":"architecture/05_execution_layer/#rule-structure","title":"Rule Structure","text":"<p>Each rule in the Snakefile represents a computational step and includes:</p> <ol> <li>Rule Name - Identifier for the operation</li> <li>Inputs - Files or directories required for the operation</li> <li>Outputs - Files or directories produced by the operation</li> <li>Container - Container image to use for execution</li> <li>Shell Command - Command to execute inside the container</li> </ol>"},{"location":"architecture/05_execution_layer/#complex-workflow-example","title":"Complex Workflow Example","text":"<p>For a multi-step pipeline, the Snakefile would contain multiple interconnected rules:</p> <pre><code>rule all:\n    input:\n        \"results/analysis_complete.txt\"\n\nrule generate_load_data:\n    input:\n        images=\"path/to/images\"\n    output:\n        load_data=\"workspace/load_data/illum_calc_load_data.csv\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"starrynight illum generate-load-data --images-path {input.images} --output-path {output.load_data} --batch-id Batch1 --plate-id Plate1\"\n\nrule generate_pipeline:\n    input:\n        load_data=\"workspace/load_data/illum_calc_load_data.csv\"\n    output:\n        pipeline=\"workspace/pipelines/illum_calc_pipeline.cppipe\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"starrynight illum generate-pipeline --output-path {output.pipeline} --load-data {input.load_data}\"\n\nrule run_pipeline:\n    input:\n        load_data=\"workspace/load_data/illum_calc_load_data.csv\",\n        pipeline=\"workspace/pipelines/illum_calc_pipeline.cppipe\"\n    output:\n        results=\"workspace/results\",\n        complete=\"results/analysis_complete.txt\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"\"\"\n        starrynight illum run-pipeline --load-data {input.load_data} --pipeline {input.pipeline} --output-dir {output.results}\n        touch {output.complete}\n        \"\"\"\n</code></pre> <p>Snakemake automatically determines the execution order based on the input/output dependencies.</p>"},{"location":"architecture/05_execution_layer/#container-execution-model","title":"Container Execution Model","text":"<p>StarryNight uses containerization for reproducible algorithm execution. This is implemented through a structured approach in the PipeCraft package.</p>"},{"location":"architecture/05_execution_layer/#container-definition","title":"Container Definition","text":"<p>The <code>Container</code> class in <code>pipecraft/node.py</code> defines execution environments with: - <code>image</code>: Docker/Singularity image reference - <code>cmd</code>: Command to run within the container - <code>env</code>: Environment variables</p> <p>Modules use this pattern to define containerized operations:</p> <pre><code># From starrynight/modules/cp_illum_calc/calc_cp.py\nContainer(\n    name=\"cp_calc_illum_invoke_cp\",\n    input_paths={\n        \"cppipe_path\": [...],\n        \"load_data_path\": [...],\n    },\n    output_paths={\n        \"cp_illum_calc_dir\": [...]\n    },\n    config=ContainerConfig(\n        image=\"ghrc.io/leoank/starrynight:dev\",\n        cmd=[\"starrynight\", \"cp\", \"-p\", spec.inputs[0].path, ...],\n        env={},\n    ),\n)\n</code></pre>"},{"location":"architecture/05_execution_layer/#backend-integration","title":"Backend Integration","text":"<p>The <code>SnakeMakeBackend</code> in <code>pipecraft/backend/snakemake.py</code> translates container specifications to Snakemake rules: - Container images become Snakemake container directives - Input/output paths define rule dependencies - Commands define the shell execution</p> <p>This is implemented in the Mako template at <code>pipecraft/backend/templates/snakemake.mako</code>:</p> <pre><code>rule ${container.name.replace(\" \", \"_\").lower()}:\n  input:\n    # Input path definitions...\n  output:\n    # Output path definitions...\n  container: \"docker://${container.config.image}\"\n  shell:\n    \"${' '.join(container.config.cmd)}\"\n</code></pre>"},{"location":"architecture/05_execution_layer/#execution-flow","title":"Execution Flow","text":"<p>The execution process follows these steps: 1. Modules define containers with appropriate configurations 2. The pipeline connects containers in sequential or parallel arrangements 3. The backend compiles the pipeline to Snakemake rules 4. Snakemake handles container execution and dependency tracking 5. Results are stored at specified output paths</p>"},{"location":"architecture/05_execution_layer/#parallelism-in-execution","title":"Parallelism in Execution","text":"<p>The execution system handles two levels of parallelism:</p>"},{"location":"architecture/05_execution_layer/#rule-level-parallelism","title":"Rule-level Parallelism","text":"<p>Snakemake automatically handles rule-level parallelism based on the dependency graph:</p> <ul> <li>Independent rules can run in parallel</li> <li>Rules that depend on the outputs of other rules wait for those rules to complete</li> <li>The order of execution is determined by the input/output dependencies, not by the order in the file</li> </ul>"},{"location":"architecture/05_execution_layer/#task-level-parallelism","title":"Task-level Parallelism","text":"<p>For rules that process multiple similar items:</p> <ul> <li>Multiple instances of the same rule can run in parallel</li> <li>Each instance processes a different input/output combination</li> <li>This is particularly useful for operations like applying illumination correction to multiple images</li> </ul> <p>The level of parallelism can be controlled with Snakemake parameters:</p> <pre><code>snakemake --cores 4  # Run with 4 CPU cores\n</code></pre>"},{"location":"architecture/05_execution_layer/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture/05_execution_layer/#compiling-without-executing","title":"Compiling Without Executing","text":"<p>You can compile a pipeline without executing it:</p> <pre><code>exec_backend.compile(\n    pipeline=module.pipeline,\n    config=backend_config,\n    working_dir=working_dir\n)\n</code></pre> <p>This generates the Snakefile without running it, allowing for inspection and manual execution. Once generated, this Snakefile can be run directly using the Snakemake command-line tool, giving users flexibility in how they execute workflows.</p>"},{"location":"architecture/05_execution_layer/#logs-and-monitoring","title":"Logs and Monitoring","text":"<p>The Snakemake backend captures detailed logs of execution. These logs include command outputs, error messages, and execution status for each step in the pipeline. They are stored in the working directory and can be accessed for troubleshooting or monitoring.</p> <p>When execution fails, several troubleshooting approaches are available:</p> <ol> <li>Examine logs in the working directory</li> <li>Check container execution details</li> <li>Validate input configurations</li> <li>Inspect the compiled workflow file</li> </ol>"},{"location":"architecture/05_execution_layer/#execution-with-telemetry","title":"Execution with Telemetry","text":"<p>For production environments, telemetry can be enabled to send execution information to a monitoring system. This is typically disabled for notebook environments but can be enabled for centralized monitoring in production deployments.</p>"},{"location":"architecture/05_execution_layer/#complete-examples","title":"Complete Examples","text":""},{"location":"architecture/05_execution_layer/#example-notebook","title":"Example Notebook","text":"<p>Here's a complete notebook example integrating these concepts:</p> <pre><code># Import necessary components\nfrom starrynight.config import DataConfig\nfrom starrynight.experiments.pcp_generic import PCPGenericExperiment\nfrom starrynight.modules.inventory import GenInvModule\nfrom starrynight.modules.index import GenIndexModule\nfrom starrynight.pipelines.pcp_generic import create_pcp_generic_pipeline\nimport pipecraft as pc\nimport os\nfrom pathlib import Path\n\n# Set up paths\nworkspace_path = Path(\"/path/to/workspace\")\nimages_path = Path(\"/path/to/images\")\nscratch_path = Path(\"/path/to/scratch\")\n\n# Create data config\ndata_config = DataConfig(\n    workspace_path=workspace_path,\n    images_path=images_path,\n    scratch_path=scratch_path\n)\n\n# Configure backend\nbackend_config = pc.SnakemakeBackendConfig(\n    use_opentelemetry=False,\n    print_exec=True\n)\nexec_backend = pc.SnakemakeBackend(backend_config)\n\n# Run indexing and inventory\nprint(\"Running indexing...\")\ngen_index_mod = GenIndexModule(data_config)\nexec_backend.run(\n    gen_index_mod.pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"index\"\n)\n\nprint(\"Running inventory...\")\ngen_inv_mod = GenInvModule(data_config)\nexec_backend.run(\n    gen_inv_mod.pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"inventory\"\n)\n\n# Configure experiment\npcp_init_config = {\n    \"nuclear_channel\": \"DAPI\",\n    \"cell_channel\": \"CellMask\",\n    \"mito_channel\": \"MitoTracker\",\n    \"barcode_csv_path\": str(workspace_path / \"barcodes.csv\"),\n    \"image_overlap_percentage\": 10\n}\n\npcp_experiment = PCPGenericExperiment.from_index(\n    index_path=data_config.workspace_path / \"index.yaml\",\n    init_config=pcp_init_config\n)\n\n# Create complete pipeline\nprint(\"Creating pipeline...\")\nmodules, pipeline = create_pcp_generic_pipeline(data_config, pcp_experiment)\n\n# Run the pipeline\nprint(\"Running pipeline...\")\nexec_backend.run(\n    pipeline=pipeline,\n    config=backend_config,\n    working_dir=data_config.scratch_path / \"runs\" / \"complete_pipeline\"\n)\n\nprint(\"Pipeline complete!\")\n</code></pre>"},{"location":"architecture/05_execution_layer/#example-generated-snakefile","title":"Example: Generated Snakefile","text":"<p>Here's an excerpt from an actual generated Snakefile:</p> <pre><code># This file was generated by StarryNight\n\nfrom snakemake.io import directory\n\nrule all:\n    input:\n        \"workspace/results/analysis_complete.txt\"\n\nrule cp_illum_calc_load_data:\n    input:\n        images=\"path/to/images/Batch1/Plate1\"\n    output:\n        load_data=\"workspace/load_data/illum_calc_load_data.csv\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"starrynight illum generate-load-data --images-path {input.images} --output-path {output.load_data} --batch-id Batch1 --plate-id Plate1 --channel DAPI --channel CellMask --channel MitoTracker\"\n\nrule cp_illum_calc_pipeline:\n    input:\n        load_data=\"workspace/load_data/illum_calc_load_data.csv\"\n    output:\n        pipeline=\"workspace/pipelines/illum_calc_pipeline.cppipe\"\n    container:\n        \"cellprofiler/starrynight:latest\"\n    shell:\n        \"starrynight illum generate-pipeline --output-path {output.pipeline} --load-data {input.load_data}\"\n\n# Additional rules...\n</code></pre>"},{"location":"architecture/05_execution_layer/#future-backends","title":"Future Backends","text":"<p>The Snakemake backend demonstrates the power of the architecture by showing that compute graphs can be converted to executable Snakemake workflows. This separation of pipeline definition from execution enables the possibility of developing additional backends for different environments, such as:</p> <ol> <li>Cloud-based execution (AWS, GCP, Azure)</li> <li>HPC cluster execution</li> <li>Kubernetes-based execution</li> <li>Custom execution environments</li> </ol> <p>This extensibility is a direct result of the architecture's separation of concerns, where pipelines are defined independently of how they are executed.</p>"},{"location":"architecture/05_execution_layer/#comparison-with-other-approaches","title":"Comparison with Other Approaches","text":"<p>The notebook workflow provides several advantages over direct CLI usage:</p> <ol> <li>State Persistence - Module configurations are maintained in memory</li> <li>Parameter Inference - Automatic configuration from experiments</li> <li>Containerization - Automatic execution in containers</li> <li>Workflow Composition - Easy combination of multiple steps</li> </ol> <p>The execution through Snakemake also offers benefits compared to direct execution:</p> <ol> <li>Reproducibility - Ensures consistent execution across environments</li> <li>Scalability - Scales from laptops to HPC clusters</li> <li>Restart Capability - Can resume from failures without redoing completed work</li> <li>Resource Management - Can specify CPU, memory, and other resource requirements</li> <li>Integration - Works well with containers and existing tools</li> </ol>"},{"location":"architecture/05_execution_layer/#conclusion","title":"Conclusion","text":"<p>The execution system in StarryNight provides a powerful approach to running pipelines in a reproducible, containerized manner. By combining a flexible execution model with the Snakemake backend, it enables complex workflows to be executed consistently across different environments.</p> <p>The automatic generation of detailed, executable Snakefiles from high-level abstractions is one of the most impressive achievements of the StarryNight architecture. This capability demonstrates the power of the separation between definition and execution in the system design, allowing complex workflows to be defined at a high level and automatically translated into executable form.</p> <p>The execution system bridges the gap between abstract pipeline definitions and concrete execution, providing the final layer in StarryNight's architecture that turns conceptual workflows into running processes.</p> <p>Next: Configuration Layer</p>"},{"location":"architecture/06_configuration_layer/","title":"StarryNight Configuration Layer","text":""},{"location":"architecture/06_configuration_layer/#overview","title":"Overview","text":"<p>The configuration layer in StarryNight consists of two interconnected systems: experiment configuration, which manages experiment-specific parameters and infers settings from data, and module configuration, which connects these parameters with module specifications. Together, these systems enable automatic setup of complex pipelines with minimal manual input, creating a bridge between user-provided parameters and the detailed configuration needed for pipeline execution.</p>"},{"location":"architecture/06_configuration_layer/#cross-cutting-nature","title":"Cross-Cutting Nature","text":"<p>Unlike the other five layers which operate in a primarily sequential flow, the Configuration Layer functions as a dimensional plane that intersects with all layers. It provides contextual intelligence and adaptive parameters throughout the system:</p> <ul> <li>With Algorithm Layer: Provides parameters that influence algorithm behavior</li> <li>With CLI Layer: Supplies default values and validation rules for command parameters</li> <li>With Module Layer: Transforms high-level experiment parameters into detailed specifications</li> <li>With Pipeline Layer: Influences how modules are composed and connected</li> <li>With Execution Layer: Controls runtime behavior and resource allocation</li> </ul> <p>This cross-cutting nature allows the Configuration Layer to maintain state across layer boundaries and adapt the system to specific experimental contexts without changing the core architecture.</p>"},{"location":"architecture/06_configuration_layer/#purpose","title":"Purpose","text":"<p>The configuration layer in StarryNight serves several critical purposes:</p> <ol> <li>Parameter Management - Collecting and organizing essential parameters</li> <li>Parameter Inference - Determining settings automatically from data</li> <li>Standardization - Providing consistent configuration across modules</li> <li>Extensibility - Supporting different experiment types</li> <li>Module Configuration - Simplifying the setup of pipeline modules</li> <li>Automatic Setup - Configuring modules with minimal manual input</li> <li>Parameter Consistency - Ensuring consistent parameters across pipeline steps</li> <li>Path Management - Setting up standardized input and output paths</li> </ol>"},{"location":"architecture/06_configuration_layer/#experiment-configuration","title":"Experiment Configuration","text":"<p>Experiment configuration provides a systematic way to manage experiment-specific parameters and infer settings from data using a clean implementation of the Pydantic data validation framework.</p>"},{"location":"architecture/06_configuration_layer/#experiment-classes","title":"Experiment Classes","text":"<p>Experiments are implemented as Python classes that inherit from a base experiment class:</p> <pre><code>class PCPGenericExperiment(ExperimentBase):\n    \"\"\"\n    Experiment configuration for generic Plate Cell Painting.\n    \"\"\"\n    # Implementation...\n</code></pre>"},{"location":"architecture/06_configuration_layer/#from-index-method","title":"From Index Method","text":"<p>A critical method in experiment classes is <code>from_index</code>, which initializes the experiment from index data:</p> <p>This method takes two main parameters:</p> <ol> <li>Index Path - Path to the index file generated by indexing</li> <li>Initial Config - User-provided parameters that cannot be inferred</li> </ol> <pre><code>@staticmethod\ndef from_index(index_path: Path, init_config: dict) -&gt; Self:\n    \"\"\"Configure experiment with index.\"\"\"\n    init_config_parsed = PCPGenericInitConfig.model_validate(init_config)\n    if index_path.name.endswith(\".csv\"):\n        index_df = pl.scan_csv(index_path)\n    else:\n        index_df = pl.scan_parquet(index_path)\n\n    # Get dataset_id from index\n    dataset_id = (\n        index_df.select(pl.col(\"dataset_id\")).unique().collect().rows()[0][0]\n    )\n\n    # Extract images per well from data\n    cp_im_per_well = (\n        cp_images_df.group_by(\"batch_id\", \"plate_id\", \"well_id\")\n        .agg(pl.col(\"key\").count())\n        .collect()\n        .select(pl.col(\"key\"))\n        .unique()\n        .rows()[0][0]\n    )\n</code></pre>"},{"location":"architecture/06_configuration_layer/#initial-configuration","title":"Initial Configuration","text":"<p>The initial configuration includes parameters that cannot be inferred from data:</p> <pre><code>pcp_init_config = {\n    \"nuclear_channel\": \"DAPI\",\n    \"cell_channel\": \"CellMask\",\n    \"mito_channel\": \"MitoTracker\",\n    \"barcode_csv_path\": \"/path/to/barcodes.csv\",\n    \"image_overlap_percentage\": 10\n}\n</code></pre> <p>These parameters are experiment-specific and must be provided by the user.</p>"},{"location":"architecture/06_configuration_layer/#parameter-inference","title":"Parameter Inference","text":"<p>A key feature of experiment classes is their ability to infer parameters from data. The system uses Polars (a high-performance dataframe library) to perform complex data analysis tasks:</p> <pre><code># Extract number of cycles\nsbs_n_cycles = (\n    sbs_images_df.select(pl.col(\"cycle_id\").unique().count())\n    .collect()\n    .rows()[0][0]\n)\n\n# Extract channel list\nsbs_channel_lists = (\n    cp_images_df.select(pl.col(\"channel_dict\"))\n    .collect()\n    .to_dict()[\"channel_dict\"]\n)\n# unique list of list adapted from Stack Overflow\nsbs_channel_list = [list(x) for x in set(tuple(x) for x in sbs_channel_lists)]\n</code></pre> <p>Examples of inferred parameters:</p> <ol> <li>Images per Well - Calculated from inventory data</li> <li>Channel Count - Determined from image metadata</li> <li>Channel List - Extracted from available images</li> <li>Dataset Structure - Inferred from file organization</li> </ol> <p>This inference reduces the manual configuration burden on users.</p>"},{"location":"architecture/06_configuration_layer/#using-experiment-configurations","title":"Using Experiment Configurations","text":"<p>Once configured, the experiment object is passed to modules when creating them:</p> <pre><code># Create a module with experiment configuration\nillum_calc_module = CPIllumCalcLoadDataModule.from_config(\n    data_config=data_config,\n    experiment=pcp_experiment\n)\n</code></pre> <p>The module then uses the experiment configuration to set its parameters.</p>"},{"location":"architecture/06_configuration_layer/#different-experiment-types","title":"Different Experiment Types","text":"<p>The architecture supports different experiment types through a class-based inheritance system. Each experiment type can have its own class with specific parameter inference logic.</p>"},{"location":"architecture/06_configuration_layer/#creating-new-experiment-types","title":"Creating New Experiment Types","text":"<p>To create a new experiment type:</p> <ol> <li>Create a new file in the experiments folder</li> <li>Define a class that inherits from ExperimentBase</li> <li>Implement the from_index method</li> <li>Define parameter inference logic</li> <li>Register the experiment class in the registry</li> </ol>"},{"location":"architecture/06_configuration_layer/#experiment-registry","title":"Experiment Registry","text":"<p>Experiments are registered in a registry to make them discoverable:</p> <pre><code>from starrynight.experiments.registry import register_experiment\n\n@register_experiment(\"pcp_generic\")\nclass PCPGenericExperiment(ExperimentBase):\n    \"\"\"Experiment configuration for generic Plate Cell Painting.\"\"\"\n    # Implementation...\n</code></pre> <p>This allows experiments to be looked up by name.</p>"},{"location":"architecture/06_configuration_layer/#module-configuration","title":"Module Configuration","text":"<p>Module configuration connects experiment parameters with module specifications, allowing modules to be automatically configured based on experiment settings and data configurations.</p>"},{"location":"architecture/06_configuration_layer/#from_config-method","title":"from_config Method","text":"<p>The primary method for module configuration is <code>from_config</code>:</p> <pre><code>@staticmethod\ndef from_config(\n    data: DataConfig,\n    experiment: Experiment | None = None,\n    spec: Container | None = None,\n) -&gt; \"StarrynightModule\":\n    \"\"\"Create module from experiment and data config.\"\"\"\n    if spec is None:\n        spec = CPCalcIllumInvokeCPModule._spec()\n        spec.inputs[0].path = (\n            data.workspace_path.joinpath(\n                CP_ILLUM_CALC_CP_CPPIPE_OUT_PATH_SUFFIX,\n                CP_ILLUM_CALC_CP_CPPIPE_OUT_NAME,\n            )\n            .resolve()\n            .__str__()\n        )\n</code></pre> <p>This method creates a configured module based on the provided configurations.</p>"},{"location":"architecture/06_configuration_layer/#data-configuration","title":"Data Configuration","text":"<p>The <code>DataConfig</code> object provides essential path information:</p> <pre><code>class DataConfig(BaseModel):\n    \"\"\"Data configuration schema.\"\"\"\n\n    dataset_path: Path | CloudPath\n    storage_path: Path | CloudPath\n    workspace_path: Path | CloudPath\n</code></pre> <p>These paths are used to locate inputs and set up outputs.</p>"},{"location":"architecture/06_configuration_layer/#experiment-integration","title":"Experiment Integration","text":"<p>The experiment parameter provides experiment-specific information, allowing modules to adapt to specific experiment requirements.</p>"},{"location":"architecture/06_configuration_layer/#spec-parameter","title":"Spec Parameter","text":"<p>The <code>spec</code> parameter allows custom specifications:</p> <pre><code># Create module with custom spec\ncustom_spec = create_custom_spec()\nmodule = ModuleClass.from_config(data_config, experiment, spec=custom_spec)\n</code></pre> <p>If not provided, a default spec is created based on the data and experiment configurations.</p>"},{"location":"architecture/06_configuration_layer/#implementation-pattern","title":"Implementation Pattern","text":"<p>The typical implementation of <code>from_config</code> follows this pattern:</p> <ol> <li>Create default spec if none provided</li> <li>Configure inputs based on data_config</li> <li>Configure parameters based on experiment (if provided)</li> <li>Configure outputs based on data_config</li> <li>Create and return module with configured spec</li> </ol> <pre><code>@classmethod\ndef from_config(cls, data_config, experiment=None, spec=None):\n    # Create default spec if none provided\n    if spec is None:\n        spec = cls().spec\n\n        # Configure inputs from data_config\n        spec.inputs[\"workspace_path\"].value = data_config.workspace_path\n\n        # Configure based on experiment if provided\n        if experiment is not None:\n            spec.inputs[\"nuclear_channel\"].value = experiment.nuclear_channel\n            spec.inputs[\"cell_channel\"].value = experiment.cell_channel\n\n        # Configure outputs\n        output_path = data_config.workspace_path / \"results\" / \"output.csv\"\n        spec.outputs[\"results\"].value = output_path\n\n    # Create and return module\n    return cls(spec=spec)\n</code></pre>"},{"location":"architecture/06_configuration_layer/#configuration-flow-examples","title":"Configuration Flow Examples","text":""},{"location":"architecture/06_configuration_layer/#example-segmentation-check-module","title":"Example: Segmentation Check Module","text":"<p>The segmentation check module provides a good example of experiment integration, requiring experiment-specific channel information.</p>"},{"location":"architecture/06_configuration_layer/#example-illumination-calculation-module","title":"Example: Illumination Calculation Module","text":"<p>The illumination calculation module demonstrates that not all modules require experiment parameters. Some modules only use data configuration for paths, without experiment-specific parameters.</p>"},{"location":"architecture/06_configuration_layer/#detailed-configuration-flow","title":"Detailed Configuration Flow","text":"<p>Let's examine the detailed configuration flow for a module:</p>"},{"location":"architecture/06_configuration_layer/#1-create-module-instance","title":"1. Create Module Instance","text":"<pre><code># In a notebook or script\nsegcheck_module = CPSegcheckGenCPipeModule.from_config(\n    data_config=data_config,\n    experiment=pcp_experiment\n)\n</code></pre>"},{"location":"architecture/06_configuration_layer/#2-from_config-implementation","title":"2. from_config Implementation","text":"<pre><code>@classmethod\ndef from_config(cls, data_config, experiment=None, spec=None):\n    if spec is None:\n        spec = cls().spec\n\n        # Configure workspace path\n        spec.inputs[\"workspace_path\"].value = data_config.workspace_path\n\n        # Configure load data path\n        load_data_path = data_config.workspace_path / \"load_data\" / \"segcheck_load_data.csv\"\n        spec.inputs[\"load_data\"].value = load_data_path\n\n        # Configure channels from experiment\n        if experiment is not None:\n            spec.inputs[\"nuclear_channel\"].value = experiment.nuclear_channel\n            spec.inputs[\"cell_channel\"].value = experiment.cell_channel\n\n        # Configure output pipeline path\n        pipeline_path = data_config.workspace_path / \"pipelines\" / \"segcheck_pipeline.cppipe\"\n        spec.outputs[\"pipeline\"].value = pipeline_path\n\n        # Configure output notebook path\n        notebook_path = data_config.workspace_path / \"notebooks\" / \"segcheck_visualization.ipynb\"\n        spec.outputs[\"notebook\"].value = notebook_path\n\n    return cls(spec=spec)\n</code></pre>"},{"location":"architecture/06_configuration_layer/#3-create-pipeline","title":"3. Create Pipeline","text":"<p>The configured module then uses this information to create its pipeline:</p> <pre><code>def create_pipeline(self):\n    # Construct CLI command using spec values\n    command = [\n        \"starrynight\", \"segcheck\", \"generate-pipeline\",\n        \"--output-path\", str(self.spec.outputs[\"pipeline\"].value),\n        \"--load-data\", str(self.spec.inputs[\"load_data\"].value),\n        \"--nuclear-channel\", str(self.spec.inputs[\"nuclear_channel\"].value),\n        \"--cell-channel\", str(self.spec.inputs[\"cell_channel\"].value)\n    ]\n\n    # Create pipeline with container\n    pipeline = pc.Pipeline()\n    with pipeline.sequential() as seq:\n        seq.container(\n            name=\"segcheck_pipeline_gen\",\n            inputs={\n                \"load_data\": str(self.spec.inputs[\"load_data\"].value),\n                \"workspace\": str(self.spec.inputs[\"workspace_path\"].value)\n            },\n            outputs={\n                \"pipeline\": str(self.spec.outputs[\"pipeline\"].value)\n            },\n            container_config=pc.ContainerConfig(\n                image=\"cellprofiler/starrynight:latest\",\n                command=command\n            )\n        )\n\n    return pipeline\n</code></pre>"},{"location":"architecture/06_configuration_layer/#path-handling-patterns","title":"Path Handling Patterns","text":"<p>Modules follow consistent patterns for handling paths:</p> <ol> <li>Input Data - Typically under <code>data_config.dataset_path</code></li> <li>Intermediate Data - Under <code>data_config.workspace_path</code> with subdirectories:</li> <li><code>load_data/</code> - For load data files</li> <li><code>pipelines/</code> - For pipeline files</li> <li><code>results/</code> - For processing results</li> <li>Execution Data - Under <code>data_config.storage_path / \"runs\" / module_name</code></li> </ol>"},{"location":"architecture/06_configuration_layer/#common-module-sets-and-their-configuration","title":"Common Module Sets and Their Configuration","text":"<p>Different module sets have different configuration patterns:</p>"},{"location":"architecture/06_configuration_layer/#cp-modules-cell-painting","title":"CP Modules (Cell Painting)","text":"<p>CP modules typically require: - Nuclear channel - Cell channel - Other specific channels (e.g., Mito channel) - Paths to Cell Painting images</p>"},{"location":"architecture/06_configuration_layer/#sbs-modules-sequencing-by-synthesis","title":"SBS Modules (Sequencing By Synthesis)","text":"<p>SBS modules typically require: - Barcode information - Image overlap percentage - Paths to SBS images</p>"},{"location":"architecture/06_configuration_layer/#common-modules-index-inventory","title":"Common Modules (Index, Inventory)","text":"<p>These modules typically only require: - Data configuration for paths - No experiment-specific parameters</p>"},{"location":"architecture/06_configuration_layer/#advanced-configuration-topics","title":"Advanced Configuration Topics","text":""},{"location":"architecture/06_configuration_layer/#updating-module-configuration","title":"Updating Module Configuration","text":"<p>Module configurations can be updated after creation:</p> <pre><code># Create module with default configuration\nmodule = CPSegcheckGenCPipeModule.from_config(data_config, experiment)\n\n# Update a parameter\nmodule.spec.inputs[\"nuclear_channel\"].value = \"New_DAPI_Channel\"\n\n# Regenerate the pipeline\nupdated_pipeline = module.create_pipeline()\n</code></pre> <p>This allows for dynamic reconfiguration.</p>"},{"location":"architecture/06_configuration_layer/#serialization-and-deserialization","title":"Serialization and Deserialization","text":"<p>Experiment configurations can be serialized and deserialized using Pydantic's built-in JSON capabilities:</p> <pre><code># Serialize experiment to JSON\nexperiment_json = pcp_experiment.model_dump_json()\n\n# Save to file\nwith open(\"experiment_config.json\", \"w\") as f:\n    f.write(experiment_json)\n\n# Later, load from file\nwith open(\"experiment_config.json\", \"r\") as f:\n    experiment_json = f.read()\n\n# Deserialize experiment\npcp_experiment = PCPGenericExperiment.model_validate_json(experiment_json)\n</code></pre> <p>This allows experiment configurations to be saved and restored.</p>"},{"location":"architecture/06_configuration_layer/#creating-custom-module-configurations","title":"Creating Custom Module Configurations","text":"<p>To create custom module configurations:</p> <ol> <li>Create a ModuleSpec with the desired inputs and outputs</li> <li>Set values for all input parameters</li> <li>Set paths for all output parameters</li> <li>Create the module with the custom spec</li> </ol> <pre><code># Create custom spec\nspec = bl.ModuleSpec(\n    name=\"Custom CP Segcheck Pipeline Generator\",\n    inputs={\n        \"load_data\": bl.PortSpec(type=\"file\", value=\"/path/to/custom/load_data.csv\"),\n        \"workspace_path\": bl.PortSpec(type=\"directory\", value=\"/path/to/custom/workspace\"),\n        \"nuclear_channel\": bl.PortSpec(type=\"string\", value=\"CustomDAPI\"),\n        \"cell_channel\": bl.PortSpec(type=\"string\", value=\"CustomCellMask\")\n    },\n    outputs={\n        \"pipeline\": bl.PortSpec(type=\"file\", value=\"/path/to/custom/pipeline.cppipe\")\n    }\n)\n\n# Create module with custom spec\nmodule = CPSegcheckGenCPipeModule.from_config(\n    data_config=data_config,\n    experiment=None,  # Not needed since spec is fully configured\n    spec=spec\n)\n</code></pre>"},{"location":"architecture/06_configuration_layer/#data-analysis-utilities","title":"Data Analysis Utilities","text":"<p>StarryNight includes powerful utilities to extract configuration from complex data structures:</p> <pre><code># Extract configurations from data\ndef get_channels_by_batch_plate(\n    df: pl.LazyFrame, batch_id: str, plate_id: str\n) -&gt; list[str]:\n    channels = (\n        df.filter(\n            pl.col(\"batch_id\").eq(batch_id)\n            &amp; pl.col(\"plate_id\").eq(plate_id)\n            &amp; pl.col(\"channel_dict\").is_not_null()\n        )\n        .select(pl.col(\"channel_dict\").explode().unique(maintain_order=True))\n        .to_series()\n        .to_list()\n    )\n    return channels\n\n# Detecting hierarchical structure in data\ndef gen_image_hierarchy(df: pl.LazyFrame) -&gt; dict:\n    hierarchy_dict = {}\n    batches = get_batches(df)\n    for batch in batches:\n        plates = get_plates_by_batch(df, batch)\n        hierarchy_dict[batch] = {}\n        for plate in plates:\n            wells = get_wells_by_batch_plate(df, batch, plate)\n            hierarchy_dict[batch][plate] = {}\n            for well in wells:\n                sites = get_sites_by_batch_plate_well(df, batch, plate, well)\n                hierarchy_dict[batch][plate][well] = sites\n    return hierarchy_dict\n</code></pre> <p>These utilities help extract structured information from complex datasets.</p>"},{"location":"architecture/06_configuration_layer/#complete-examples","title":"Complete Examples","text":""},{"location":"architecture/06_configuration_layer/#example-complete-experiment-class","title":"Example: Complete Experiment Class","text":"<p>Here's a more complete example of an experiment class using Pydantic for validation:</p> <pre><code>@register_experiment(\"pcp_generic\")\nclass PCPGenericExperiment(ExperimentBase):\n    \"\"\"\n    Experiment configuration for generic Plate Cell Painting.\n    \"\"\"\n    # User-provided parameters\n    nuclear_channel: str\n    cell_channel: str\n    mito_channel: str\n    barcode_csv_path: Path\n    image_overlap_percentage: int\n\n    # Inferred parameters\n    dataset_id: str | None = None\n    cp_images_df: pl.LazyFrame | None = None\n    sbs_images_df: pl.LazyFrame | None = None\n    images_per_well: int | None = None\n    cp_channels: list[str] | None = None\n    cp_channel_count: int | None = None\n    sbs_channels: list[str] | None = None\n    sbs_channel_count: int | None = None\n\n    @staticmethod\n    def from_index(index_path: Path, init_config: dict) -&gt; Self:\n        \"\"\"Create experiment from index and initial config.\"\"\"\n        # Validate initial config with Pydantic\n        init_config_parsed = PCPGenericInitConfig.model_validate(init_config)\n\n        # Load index and extract dataset_id\n        index_df = pl.scan_parquet(index_path)\n        dataset_id = index_df.select(pl.col(\"dataset_id\")).unique().collect().rows()[0][0]\n\n        # Create and configure experiment instance\n        experiment = PCPGenericExperiment(\n            nuclear_channel=init_config_parsed.nuclear_channel,\n            cell_channel=init_config_parsed.cell_channel,\n            mito_channel=init_config_parsed.mito_channel,\n            barcode_csv_path=init_config_parsed.barcode_csv_path,\n            image_overlap_percentage=init_config_parsed.image_overlap_percentage,\n            dataset_id=dataset_id\n        )\n\n        # Infer additional parameters using Polars\n        # (implementation details...)\n\n        return experiment\n\n    def model_post_init(self, __context: Any) -&gt; None:\n        \"\"\"Validate experiment configuration.\"\"\"\n        # Additional validation can be performed here\n        if not self.nuclear_channel:\n            raise ValueError(\"Nuclear channel must be specified\")\n</code></pre>"},{"location":"architecture/06_configuration_layer/#example-notebook-workflow","title":"Example: Notebook Workflow","text":"<p>Here's how configuration fits into a typical notebook workflow:</p> <pre><code># Import necessary components\nfrom starrynight.config import DataConfig\nfrom starrynight.experiments.pcp_generic import PCPGenericExperiment\nfrom starrynight.modules.cp_segcheck import CPSegcheckGenLoadDataModule, CPSegcheckGenCPipeModule\nimport pipecraft as pc\nimport pathlib\n\n# Set up paths\nworkspace_path = pathlib.Path(\"/path/to/workspace\")\ndataset_path = pathlib.Path(\"/path/to/images\")\nstorage_path = pathlib.Path(\"/path/to/scratch\")\n\n# Create data config\ndata_config = DataConfig(\n    workspace_path=workspace_path,\n    dataset_path=dataset_path,\n    storage_path=storage_path\n)\n\n# Configure experiment\npcp_init_config = {\n    \"nuclear_channel\": \"DAPI\",\n    \"cell_channel\": \"CellMask\",\n    \"mito_channel\": \"MitoTracker\",\n    \"barcode_csv_path\": str(workspace_path / \"barcodes.csv\"),\n    \"image_overlap_percentage\": 10\n}\n\n# Create experiment\npcp_experiment = PCPGenericExperiment.from_index(\n    index_path=data_config.workspace_path / \"index.parquet\",\n    init_config=pcp_init_config\n)\n\n# Create modules with configuration\nload_data_module = CPSegcheckGenLoadDataModule.from_config(\n    data_config=data_config,\n    experiment=pcp_experiment\n)\n\npipeline_module = CPSegcheckGenCPipeModule.from_config(\n    data_config=data_config,\n    experiment=pcp_experiment\n)\n\n# Configure backend\nbackend_config = pc.SnakemakeBackendConfig(\n    use_opentelemetry=False,\n    print_exec=True\n)\nexec_backend = pc.SnakemakeBackend(backend_config)\n\n# Run modules\nexec_backend.run(\n    load_data_module.pipeline,\n    config=backend_config,\n    working_dir=data_config.storage_path / \"runs\" / \"segcheck_load_data\"\n)\n\nexec_backend.run(\n    pipeline_module.pipeline,\n    config=backend_config,\n    working_dir=data_config.storage_path / \"runs\" / \"segcheck_pipeline\"\n)\n</code></pre>"},{"location":"architecture/06_configuration_layer/#comparison-with-direct-cli-usage","title":"Comparison with Direct CLI Usage","text":"<p>The configuration approach differs significantly from CLI usage by providing a richer, more automated approach with several benefits:</p> <ol> <li>Reduced Manual Configuration - Many parameters are inferred automatically</li> <li>Consistency - Parameters are defined once and used consistently</li> <li>Validation - Parameters can be validated during inference using Pydantic</li> <li>Extensibility - New experiment types can be added without changing modules</li> <li>Separation of Concerns - Experiment logic is separate from module logic</li> <li>Reduced Boilerplate - Minimal code required to set up modules</li> <li>Discoverability - Clear pattern for how modules are configured</li> <li>Flexibility - Custom specs can override defaults when needed</li> </ol>"},{"location":"architecture/06_configuration_layer/#conclusion","title":"Conclusion","text":"<p>The experiment and module configuration systems in StarryNight provide a powerful approach to managing parameters, inferring settings from data, and automatically configuring modules. By separating experiment-specific logic from module implementation and providing standardized configuration patterns, they enable flexibility, extensibility, and consistency across the pipeline system.</p> <p>Together, these configuration systems form a critical bridge between user input and pipeline execution, reducing manual configuration burden while maintaining flexibility for different experiment types and module implementations. They exemplify the architecture's focus on separation of concerns, allowing each component to focus on its specific role while working together to create a cohesive system.</p> <p>Next: Architecture for Biologists</p>"},{"location":"architecture/07_architecture_for_biologists/","title":"StarryNight Architecture for Biologists","text":""},{"location":"architecture/07_architecture_for_biologists/#why-this-architecture-was-developed","title":"Why This Architecture Was Developed","text":"<p>StarryNight's architecture was designed to address the challenges faced by biologists working with high-throughput microscopy data, and optical pooled screening in particular. This document explains how the system's design supports image analysis workflow requirements.</p>"},{"location":"architecture/07_architecture_for_biologists/#core-challenges-in-high-throughput-microscopy","title":"Core Challenges in High-Throughput Microscopy","text":"<p>Analyzing images from optical pooled screening experiments present several key challenges:</p> <ul> <li>Complex Multistage Processing - Requiring multiple image processing steps that must be executed in specific sequences, often using multiple software packages</li> <li>Parallel Image Tracks - Processing different types of images with varied requirements</li> <li>Critical Quality Assessment - Needing inspection points between processing stages</li> <li>Diverse User Expertise Levels - Supporting both computational experts and biologists with varying technical backgrounds</li> </ul>"},{"location":"architecture/07_architecture_for_biologists/#workflow-in-starrynight","title":"Workflow in StarryNight","text":"<p>StarryNight's architecture enables a research workflow that addresses the requirements of analyzing optical pooled screening data:</p> <ol> <li>Experiment Configuration - Specifying the parameters relevant to the research:<ul> <li>Experimental design settings</li> <li>Image acquisition parameters</li> <li>Processing configuration options</li> </ul> </li> <li>Parameter Transformation - Converting high-level parameters into complete processing pipelines with appropriate settings for each stage</li> <li>Critical Checkpoints - Allowing review and intervention:<ul> <li>Inspection of intermediate results between processing stages</li> <li>Quality validation through integrated visualization tools</li> <li>Parameter adjustments based on observed results</li> </ul> </li> <li>Parallel Processing - Handling multiple analysis paths efficiently:<ul> <li>Running independent processing tracks in parallel when possible</li> <li>Integrating results where workflows converge</li> </ul> </li> <li>Scaling - Processing across different computing environments with consistent results whether running locally or in the cloud</li> </ol> <p>This workflow combines automation for efficiency with critical intervention points where biological expertise is essential, supporting various microscopy research applications.</p>"},{"location":"architecture/07_architecture_for_biologists/#a-laboratory-research-analogy","title":"A Laboratory Research Analogy","text":"<p>To understand how StarryNight works, think of it like a modern research laboratory:</p> <pre><code>flowchart TD\n    A[\"Lab: Core Methods&lt;br/&gt;SN: Algorithms\"] --&gt;|used by| F[\"Lab: Lab Tools&lt;br/&gt;SN: CLI\"]\n    F --&gt;|incorporated into| B[\"Lab: Protocols&lt;br/&gt;SN: Modules\"]\n    B --&gt;|combined into| C[\"Lab: Workflow&lt;br/&gt;SN: Pipelines\"]\n    C --&gt;|run on| D[\"Lab: Equipment&lt;br/&gt;SN: Execution\"]\n    E[\"Lab: Parameters&lt;br/&gt;SN: Configuration\"] --&gt;|guides| A\n    E --&gt;|guides| F\n    E --&gt;|guides| B\n    E --&gt;|guides| C\n    E --&gt;|guides| D</code></pre> <p>The diagram shows how StarryNight's (SN) layers are analogous to a biological laboratory (Lab). Just as labs separate fundamental techniques from protocols, workflows, and execution, StarryNight organizes image processing into similar logical layers.</p>"},{"location":"architecture/07_architecture_for_biologists/#practical-applications-of-the-architecture","title":"Practical Applications of the Architecture","text":"<p>The layered architecture translates to practical benefits for microscopy research:</p>"},{"location":"architecture/07_architecture_for_biologists/#interface-options","title":"Interface Options","text":"<ul> <li>Canvas Web Interface: A point-and-click user interface with both simplified views for non-experts and advanced options for experienced users</li> <li>Jupyter Notebooks: Interactive inspection environments for result visualization and quality control</li> <li>Command-Line Interface: Direct access to algorithms for computational experts and automation scripts</li> </ul>"},{"location":"architecture/07_architecture_for_biologists/#processing-control","title":"Processing Control","text":"<ul> <li>End-to-End Automation: Complete workflows from raw images to results</li> <li>Checkpoint Intervention: Critical points for quality assessment and parameter adjustment</li> <li>Restart Capability: Processing resumption from checkpoints after adjustments</li> </ul>"},{"location":"architecture/07_architecture_for_biologists/#data-organization","title":"Data Organization","text":"<ul> <li>Standardized Structure: Consistent organization patterns for input and output data</li> <li>Metadata Extraction: Flexible path parsing for experimental information</li> </ul>"},{"location":"architecture/07_architecture_for_biologists/#conclusion-architecture-for-optical-pooled-screening","title":"Conclusion: Architecture for Optical Pooled Screening","text":"<p>StarryNight's architecture addresses the requirements of optical pooled screening through:</p> <ol> <li>Flexible, multistage workflows adaptable to various imaging approaches</li> <li>Critical quality control points where biological expertise can be applied</li> <li>Parameter configuration interfaces matched to different technical expertise levels</li> <li>Scalable processing across computing environments without workflow changes</li> </ol> <p>This architectural approach addresses fundamental challenges in high-throughput microscopy research. By separating the system into well-defined layers, StarryNight provides both automation for efficiency and control points for quality assessment, creating a flexible framework that can adapt to evolving research methodologies.</p>"},{"location":"architecture/08_practical_integration/","title":"Practical Integration: Connecting the StarryNight Architecture Layers","text":"<p>This document provides a concrete example of how StarryNight's architectural layers work together in practice by examining <code>exec_pcp_generic_pipe.py</code>, an example pipeline implementation file that demonstrates the PCP Generic workflow. While the architecture overview and individual layer documents (Algorithm, CLI, Module, Pipeline, Execution, Configuration) explain each architectural layer conceptually, this walkthrough shows how these components integrate in a real workflow.</p> <p>Pedagogical Approach</p> <p>This document deliberately uses the step-by-step implementation in <code>exec_pcp_generic_pipe.py</code> to clearly demonstrate individual components and their interactions. This approach:</p> <ul> <li>Allows researchers to inspect intermediate results between pipeline stages</li> <li>Matches biological research workflows where verification at each stage is crucial</li> <li>Provides clearer visibility into how components operate independently</li> </ul> <p>For production use, the more concise pattern in <code>exec_pcp_generic_full.py</code> (which composes all modules at once using the <code>create_pcp_generic_pipeline</code> function) is typically preferred.</p>"},{"location":"architecture/08_practical_integration/#why-this-example-matters","title":"Why This Example Matters","text":"<p>The <code>exec_pcp_generic_pipe.py</code> file demonstrates:</p> <ol> <li>How configuration parameters flow from experiment setup to module execution</li> <li>How modules encapsulate different functional steps in the pipeline</li> <li>How the execution layer transforms abstract workflows into concrete container invocations</li> <li>How StarryNight's layered architecture enables reproducible scientific workflows</li> </ol> <p>This example serves as a practical bridge between the conceptual architecture and its real-world implementation. By understanding this workflow, you'll gain insights into:</p> <ul> <li>How to extend the system with your own modules and algorithms</li> <li>The rationale behind StarryNight's design decisions</li> <li>Patterns for creating maintainable, containerized scientific pipelines</li> </ul> <p>For a detailed mapping between the architecture overview's two-phase sequence diagrams and this concrete implementation, see the Architecture Flow in Action document.</p>"},{"location":"architecture/08_practical_integration/#pipeline-at-a-glance","title":"Pipeline at a Glance","text":"<p>The PCP Generic pipeline processes optical pooled screening data through a series of steps:</p> <ol> <li>Generate inventory and index</li> <li>Calculate illumination correction (CP-Cell Painting images)</li> <li>Apply illumination and segment (CP)</li> <li>Segmentation check (CP)</li> <li>Calculate illumination correction (SBS-Sequencing by Synthesis images)</li> <li>Apply illumination and align (SBS)</li> <li>Preprocess (SBS)</li> <li>Analysis</li> </ol> <p>Each of the eight pipeline steps follows a CellProfiler-specific three-phase pattern (see Anatomy of a Pipeline Step for details on this pattern):</p> <ul> <li>Generate load data (configuration data for CellProfiler)</li> <li>Generate pipeline file (CellProfiler pipeline definition)</li> <li>Execute the pipeline (running CellProfiler)</li> </ul>"},{"location":"architecture/08_practical_integration/#configuration-setup","title":"Configuration Setup","text":"<p>The first section establishes two key configurations:</p> <pre><code># Data configuration (Configuration Layer)\ndata_config = DataConfig(\n    dataset_path=dataset_path,\n    storage_path=dataset_path,\n    workspace_path=workspace_path,\n)\n\n# Execution engine config (Execution Layer)\nbackend_config = SnakeMakeConfig(\n    use_fluent_bit=False, print_exec=True, background=False\n)\n</code></pre> <p>What developers should note:</p> <ul> <li><code>DataConfig</code> is a Python class that defines input/output paths for the entire pipeline</li> <li><code>SnakeMakeBackend</code> provides the execution environment</li> <li>These configurations will be reused across all modules</li> <li>The actual experiment configuration (as a Python object) is created later after building the index</li> </ul>"},{"location":"architecture/08_practical_integration/#pipeline-initialization","title":"Pipeline Initialization","text":"<p>Before running algorithm-specific modules, the pipeline needs two foundational components:</p> <ol> <li>Inventory - Catalogs available data files</li> <li>Index - Builds a queryable index of experimental data</li> </ol> <pre><code># Generate inventory\ngen_inv_mod = GenInvModule(data_config)\nexec_backend = SnakeMakeBackend(\n    gen_inv_mod.pipe, backend_config, exec_runs / \"run001\", exec_mounts\n)\nrun = exec_backend.run()\nrun.wait()\n\n# Generate index\ngen_ind_mod = GenIndexModule(data_config)\nexec_backend = SnakeMakeBackend(\n    gen_ind_mod.pipe, backend_config, exec_runs / \"run002\", exec_mounts\n)\nrun = exec_backend.run()\nrun.wait()\n</code></pre> <p>What developers should note:</p> <ul> <li>The <code>from_config()</code> pattern is consistent across modules</li> <li>Each module produces a \"pipe\" that's executed by the backend</li> <li>The experiment is initialized from the index using type-validated configuration</li> </ul>"},{"location":"architecture/08_practical_integration/#experiment-configuration","title":"Experiment Configuration","text":"<p>After building the index, the notebook configures the experiment using <code>PCPGenericInitConfig</code>:</p> <pre><code>index_path = workspace_path / \"index/index.parquet\"\npcp_exp_init = PCPGenericInitConfig(\n    barcode_csv_path=Path(),\n    cp_acquisition_order=AcquisitionOrderType.SNAKE,\n    cp_img_frame_type=ImageFrameType.ROUND,\n    cp_img_overlap_pct=10,\n    sbs_acquisition_order=AcquisitionOrderType.SNAKE,\n    sbs_img_frame_type=ImageFrameType.ROUND,\n    sbs_img_overlap_pct=10,\n    cp_nuclei_channel=\"DAPI\",\n    cp_cell_channel=\"PhalloAF750\",\n    cp_mito_channel=\"ZO1AF488\",\n    cp_custom_channel_map={\n        \"DAPI\": \"DNA\",\n        \"ZO1AF488\": \"ZO1\",\n        \"PhalloAF750\": \"Phalloidin\",\n    },\n    sbs_nuclei_channel=\"DAPI\",\n    sbs_cell_channel=\"PhalloAF750\",\n    sbs_mito_channel=\"ZO1AF488\",\n    sbs_custom_channel_map={\n        \"DAPI\": \"DNA\",\n        \"A\": \"A\",\n        \"T\": \"T\",\n        \"G\": \"G\",\n        \"C\": \"C\",\n    },\n)\npcp_experiment = PCPGeneric.from_index(index_path, pcp_exp_init.model_dump())\n</code></pre> <p>What developers should note:</p> <ul> <li><code>PCPGenericInitConfig</code> is a Pydantic model that validates experiment parameters</li> <li>Channel names (DAPI, PhalloAF750, etc.) configure which image channels to use for specific purposes</li> <li>Custom channel mappings (<code>cp_custom_channel_map</code>, <code>sbs_custom_channel_map</code>) translate microscope channel names to semantic biological names used in analysis pipelines</li> <li>Acquisition settings (SNAKE, ROUND) define how the microscope captured the images</li> <li>The <code>from_index</code> method loads data from the index and configures the experiment</li> <li>This configuration will drive all subsequent module behavior without requiring repetitive parameter specification</li> </ul>"},{"location":"architecture/08_practical_integration/#anatomy-of-a-pipeline-step","title":"Anatomy of a Pipeline Step","text":"<p>CellProfiler Integration Pattern</p> <p>The three-phase pattern described below (Generate Load Data \u2192 Generate Pipeline File \u2192 Execute Pipeline) is specific to how StarryNight integrates with CellProfiler. This pattern isn't a requirement of the StarryNight architecture, but rather a practical approach for this particular integration. Other tools may use different patterns while still adhering to the module abstraction.</p> <p>With the experiment configured, we can now examine one complete pipeline step (CP calculate illumination). Each step follows a consistent three-phase pattern:</p>"},{"location":"architecture/08_practical_integration/#phase-1-generate-load-data","title":"Phase 1: Generate Load Data","text":"<p>First, a module generates the LoadData CSV file that tells CellProfiler which images to process:</p> <pre><code>cp_calc_illum_load_data_mod = CPCalcIllumGenLoadDataModule(\n    data_config, pcp_experiment\n)\nexec_backend = SnakeMakeBackend(\n    cp_calc_illum_load_data_mod.pipe,\n    backend_config,\n    exec_runs / \"run003\",\n    exec_mounts,\n)\nrun = exec_backend.run()\nrun.wait()\n</code></pre> <p>Notice how the experiment configuration (<code>pcp_experiment</code>) is passed to the module, allowing it to access parameters like channel names that were defined earlier.</p>"},{"location":"architecture/08_practical_integration/#phase-2-generate-pipeline-file","title":"Phase 2: Generate Pipeline File","text":"<p>Next, a module creates the CellProfiler pipeline definition (.cppipe file):</p> <pre><code>cp_calc_illum_cppipe_mod = CPCalcIllumGenCPPipeModule.from_config(\n    data_config, pcp_experiment\n)\nexec_backend = SnakeMakeBackend(\n    cp_calc_illum_cppipe_mod.pipe,\n    backend_config,\n    exec_runs / \"run004\",\n    exec_mounts,\n)\nrun = exec_backend.run()\nrun.wait()\n</code></pre> <p>This module automatically finds the LoadData file created in the previous phase and uses it to configure the pipeline.</p>"},{"location":"architecture/08_practical_integration/#phase-3-execute-pipeline","title":"Phase 3: Execute Pipeline","text":"<p>Finally, a module runs the pipeline on the data (note that module names use \"Invoke\" but we refer to this as the \"Execute\" phase for clarity):</p> <pre><code>cp_calc_illum_invoke_mod = CPCalcIllumInvokeCPModule(\n    data_config, pcp_experiment\n)\nexec_backend = SnakeMakeBackend(\n    cp_calc_illum_invoke_mod.pipe,\n    backend_config,\n    exec_runs / \"run005\",\n    exec_mounts,\n)\nrun = exec_backend.run()\nrun.wait()\n</code></pre> <p>This module finds both the LoadData file and the pipeline file created in the previous phases, then executes the pipeline with CellProfiler inside a container.</p> <p>What developers should note:</p> <ul> <li>Each step follows the same three-phase pattern across all pipeline steps</li> <li>Module names follow a consistent naming convention (LoadData \u2192 CPipe \u2192 Invoke), though we refer to the third phase as \"Execute\" for clarity</li> <li>The same configuration (<code>data_config</code> and <code>pcp_experiment</code>) is used across all phases</li> <li>Each module is independently executable but automatically finds outputs from previous phases</li> <li>This pattern repeats for all eight pipeline steps, with variations in parameter specifics</li> </ul>"},{"location":"architecture/08_practical_integration/#architecture-in-action","title":"Architecture in Action","text":"<p>Looking at this example, we can see how all the architecture layers work together across the two main phases:</p>"},{"location":"architecture/08_practical_integration/#pipeline-composition-phase","title":"Pipeline Composition Phase","text":"<ol> <li>Configuration Layer: <code>DataConfig</code> and experiment configuration drive behavior across all layers</li> <li>Module Layer: Defines standardized components (like <code>CPCalcIllumInvokeCPModule</code>) with specifications and compute graphs</li> <li>Pipeline Layer: In this example, we're executing modules one by one, but they can be composed into a complete pipeline as seen in <code>create_pcp_generic_pipeline</code></li> <li>Execution Layer (design time): <code>SnakeMakeBackend</code> translates module compute graphs into Snakemake rules</li> </ol>"},{"location":"architecture/08_practical_integration/#runtime-execution-phase","title":"Runtime Execution Phase","text":"<ol> <li>Execution Layer (runtime): Schedules container execution based on Snakemake rules</li> <li>Container Runtime: Executes commands in isolated environments</li> <li>CLI Layer: Provides command-line tools that parse arguments and call algorithms</li> <li>Algorithm Layer: Contains pure functions that implement image processing operations</li> </ol>"},{"location":"architecture/08_practical_integration/#module-registry-and-discovery","title":"Module Registry and Discovery","text":"<p>StarryNight uses a registry mechanism to organize and discover available modules. In the Module Registry (implemented in <code>starrynight/modules/registry.py</code>), each module is registered with a unique identifier:</p> <pre><code>MODULE_REGISTRY: dict[str, StarrynightModule] = {\n    # Generate inventory and index for the project\n    GenInvModule.uid(): GenInvModule,\n    GenIndexModule.uid(): GenIndexModule,\n    # CP illum calc\n    CPCalcIllumGenLoadDataModule.uid(): CPCalcIllumGenLoadDataModule,\n    CPCalcIllumGenCPPipeModule.uid(): CPCalcIllumGenCPPipeModule,\n    CPCalcIllumInvokeCPModule.uid(): CPCalcIllumInvokeCPModule,\n    # Additional modules...\n}\n</code></pre> <p>Integration with Broader System</p> <p>The registry is not used in this example, but it serves as a critical integration point with other StarryNight components:</p> <ul> <li>Enables Conductor to discover and invoke available modules dynamically</li> <li>Allows Canvas to present available modules in its user interface</li> <li>Provides extension hooks for integrating new capabilities without modifying core code</li> </ul> <p>This registry enables:</p> <ul> <li>Runtime discovery of available modules</li> <li>Dynamic instantiation based on configuration</li> <li>Integration with experiment classes (like <code>PCPGeneric</code> that define workflow-specific configurations)</li> <li>Extension with new module types</li> </ul> <p>When creating new modules, you must register them in this registry to make them discoverable within the system.</p>"},{"location":"architecture/08_practical_integration/#container-execution-model","title":"Container Execution Model","text":"<p>Modules define containerized operations that are executed by the backend. The container configuration is defined as part of the module's compute graph:</p> <pre><code># From starrynight/modules/cp_illum_calc/calc_cp.py\nContainer(\n    name=\"cp_calc_illum_invoke_cp\",\n    input_paths={\n        \"cppipe_path\": [...],\n        \"load_data_path\": [...],\n    },\n    output_paths={\n        \"cp_illum_calc_dir\": [...]\n    },\n    config=ContainerConfig(\n        image=\"ghrc.io/leoank/starrynight:dev\",\n        cmd=[\"starrynight\", \"cp\", \"-p\", spec.inputs[\"cppipe_path\"].value, ...],\n        env={},\n    ),\n)\n</code></pre> <p>When executed:</p> <ol> <li>The <code>SnakeMakeBackend</code> translates this container definition into a Snakemake rule</li> <li>Snakemake executes the rule in the specified container</li> <li>The CLI command runs inside the container, calling the underlying algorithm functions</li> <li>Results are stored at the specified output paths</li> </ol> <p>This containerization ensures reproducibility and isolation of each pipeline step.</p>"},{"location":"architecture/08_practical_integration/#pipeline-composition-alternative-approach","title":"Pipeline Composition (Alternative Approach)","text":"<p>While this document focuses on executing modules one by one for clarity, StarryNight also supports composing all modules at once through the <code>create_pcp_generic_pipeline</code> function:</p> <pre><code># From starrynight/src/starrynight/pipelines/pcp_generic.py\ndef create_pcp_generic_pipeline(\n    data: DataConfig,\n    experiment: Experiment | None = None,\n) -&gt; tuple[list[StarrynightModule], Pipeline]:\n    # Create all modules in one place\n    module_list = [\n        # cp modules\n        cp_illum_calc_loaddata := init_module(CPCalcIllumGenLoadDataModule),\n        cp_illum_calc_cpipe := init_module(CPCalcIllumGenCPPipeModule),\n        cp_illum_calc_cp := init_module(CPCalcIllumInvokeCPModule),\n        # More modules...\n    ]\n\n    # Compose modules into a pipeline with parallel execution\n    return module_list, Seq(\n        [\n            Parallel(\n                [\n                    Seq([cp_illum_calc_loaddata.pipe, cp_illum_calc_cpipe.pipe, ...]),\n                    Seq([sbs_illum_calc_loaddata.pipe, sbs_illum_calc_cpipe.pipe, ...]),\n                ]\n            ),\n            analysis_loaddata.pipe,\n            analysis_cpipe.pipe,\n            analysis_cp.pipe,\n        ]\n    )\n</code></pre> <p>This approach enables complex parallel execution patterns, where CP and SBS processing can run simultaneously, with analysis running after both complete.</p>"},{"location":"architecture/08_practical_integration/#extension-patterns","title":"Extension Patterns","text":"<p>When implementing your own modules, follow these patterns:</p> <p>Module vs. Algorithm Extension</p> <p>This section focuses on extending StarryNight with new modules rather than new algorithms. Modules provide standardized interfaces to existing algorithms, whether those algorithms are part of StarryNight's core or from external tools. To add your own algorithms to StarryNight, see the \"Adding a New Algorithm\" section below.</p> <ol> <li>Module Structure: Consider your module's specific requirements:<ul> <li>For CellProfiler integrations, use the three-phase pattern shown earlier</li> <li>For other tools, design appropriate module structures based on tool requirements</li> <li>Ensure your modules have clear inputs, outputs, and containerized execution specifications</li> </ul> </li> <li> <p>Registry Integration: Define a unique ID and register your module in the registry:    <pre><code>@staticmethod\ndef uid() -&gt; str:\n    \"\"\"Return module unique id.\"\"\"\n    return \"your_module_unique_id\"\n\n# Then add to MODULE_REGISTRY in registry.py\n</code></pre></p> </li> <li> <p>Configuration: Extend existing configuration classes or create new ones using Pydantic models</p> </li> <li> <p>Container Definition: Define how your module should run in containers using the Container class</p> </li> </ol>"},{"location":"architecture/08_practical_integration/#external-tool-integration-without-algorithm-changes","title":"External Tool Integration Without Algorithm Changes","text":"<p>One powerful capability of StarryNight's architecture is the ability to integrate external tools without modifying core algorithms:</p> <pre><code># Simplified example based on actual codebase patterns\nclass ExternalToolModule(StarrynightModule):\n    @property\n    def uid(self) -&gt; str:\n        return \"external_tool_module\"\n\n    def _spec(self) -&gt; SpecContainer:\n        # Default specification\n        return ExternalToolSpec()\n\n    def _create_pipe(self) -&gt; Pipeline:\n        return Seq(\n            Container(\n                name=\"external_tool\",\n                input_paths={\n                    \"some_input\": [...],\n                },\n                output_paths={\n                    \"some_output\": [...]\n                },\n                config=ContainerConfig(\n                    image=\"externaltoolimage\",\n                    cmd=[\"externaltool\"],\n                    env={},\n                ),\n            )\n        )\n</code></pre> <p>This approach allows StarryNight to leverage existing tools by:</p> <ul> <li>Directly using their CLI interfaces rather than reimplementing algorithms</li> <li>Wrapping them in StarryNight's module abstraction for consistent workflow integration</li> <li>Using containerization to ensure reproducibility and isolation</li> <li>Potentially using Bilayers specifications directly (the external schema system StarryNight uses for module specifications), enabling integration with other Bilayers-compatible tools</li> </ul>"},{"location":"architecture/08_practical_integration/#common-development-tasks","title":"Common Development Tasks","text":"<p>Here are examples of common tasks and how to approach them:</p>"},{"location":"architecture/08_practical_integration/#adding-a-new-algorithm","title":"Adding a New Algorithm","text":"<ol> <li>Implement the algorithm functions in <code>algorithms/</code></li> <li>Create CLI commands that expose the algorithm in <code>cli/</code></li> <li>Create modules (load_data, cppipe, invoke) in a new directory under <code>modules/</code></li> <li>Register your modules in the <code>MODULE_REGISTRY</code></li> <li>Extend experiment configuration to include your algorithm's parameters</li> <li>Update pipeline composition functions to include your modules</li> </ol>"},{"location":"architecture/08_practical_integration/#modifying-an-existing-pipeline","title":"Modifying an Existing Pipeline","text":"<p>To modify how a pipeline works, you typically need to:</p> <ol> <li>For changing module behavior: Find the relevant module implementations in <code>modules/</code></li> <li>Update the module's <code>.from_config()</code> method to handle new configurations</li> <li>Modify the container configuration and CLI command construction</li> <li>For changing pipeline structure: Update the pipeline composition function (e.g., in <code>pipelines/pcp_generic.py</code>)</li> <li>Add or remove modules from the composition</li> <li>Change the execution order or parallelization strategy</li> </ol>"},{"location":"architecture/08_practical_integration/#debugging-and-troubleshooting","title":"Debugging and Troubleshooting","text":"<p>When working with pipelines:</p> <ol> <li>Check run logs using <code>run.print_log()</code></li> <li>Examine workspace output files after each step</li> <li>Inspect the generated Snakefile in the working directory</li> <li>The modular design lets you run steps individually to isolate issues</li> <li>Use <code>.wait()</code> to ensure previous steps complete before continuing</li> </ol>"},{"location":"architecture/08_practical_integration/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>StarryNight follows a consistent, modular pattern for pipeline components</li> <li>Modules invoke CLI commands in containers rather than directly implementing algorithms</li> <li>The module registry enables discovery and composition of standardized components</li> <li>Container execution ensures reproducibility and isolation</li> <li>Pipeline composition enables complex parallel execution patterns</li> <li>Configuration flows from top-level settings to specific module behavior</li> </ol> <p>By understanding this concrete example, you now have a practical view of how StarryNight's architecture functions as an integrated system, from algorithms through CLI commands, modules, pipelines, and execution.</p>"},{"location":"architecture/08_practical_integration/#looking-deeper-architecture-flow-details","title":"Looking Deeper: Architecture Flow Details","text":"<p>For a detailed, code-level mapping between the architecture's sequence diagrams and this concrete implementation, see Architecture Flow in Action. This companion document:</p> <ul> <li>Traces a single pipeline step through all architectural layers</li> <li>Shows exactly how data transforms at each step</li> <li>Explains the relationship between the Pipeline Composition and Runtime Execution phases</li> <li>Provides concrete code examples for each step in the flow</li> </ul> <p>The detailed flow examples will help you understand precisely how StarryNight's layered architecture translates into actual implementation code.</p>"},{"location":"architecture/09_architecture_flow_example/","title":"Architecture Flow in Action: Detailed Code Examples","text":"<p>Experimental Documentation</p> <p>This is an experimental document that offers a very detailed view of the architecture flow. Some readers may find this level of detail overwhelming. If you're new to StarryNight, we recommend starting with the Architecture Overview and Practical Integration before diving into this detailed mapping.</p> <p>This document provides concrete code examples showing how data flows through StarryNight's architectural layers. It complements the Architecture Overview by mapping the abstract sequence diagrams to actual code implementation, and builds on the foundational concepts from the Practical Integration walkthrough.</p>"},{"location":"architecture/09_architecture_flow_example/#purpose-of-this-document","title":"Purpose of This Document","text":"<p>While the architecture overview explains the conceptual flow and the practical integration document shows the overall structure of a pipeline, this document:</p> <ol> <li>Maps Diagrams to Code: Shows exactly how each arrow in the sequence diagrams maps to concrete code</li> <li>Demonstrates Transformations: Illustrates how data transforms at each step between layers</li> <li>Provides Implementation Details: Goes deeper into the technical implementation of each layer</li> <li>Shows Two-Phase Flow: Demonstrates how the Pipeline Composition and Runtime Execution phases work in practice</li> </ol> <p>By studying this document, you'll gain a precise understanding of how StarryNight's architectural components interact in a real implementation.</p>"},{"location":"architecture/09_architecture_flow_example/#tracing-a-single-pipeline-step","title":"Tracing a Single Pipeline Step","text":"<p>We'll trace a single pipeline step (CP illumination calculation) through the complete architecture flow, showing exactly how data transforms at each step between layers.</p>"},{"location":"architecture/09_architecture_flow_example/#pipeline-composition-phase","title":"Pipeline Composition Phase","text":""},{"location":"architecture/09_architecture_flow_example/#configmodule-configuration-flows-into-module","title":"Config\u2192Module: Configuration flows into module","text":"<pre><code># Configuration setup\ndata_config = DataConfig(\n    dataset_path=dataset_path,\n    storage_path=dataset_path,\n    workspace_path=workspace_path,\n)\n\npcp_exp_init = PCPGenericInitConfig(\n    cp_nuclei_channel=\"DAPI\",\n    # other parameters...\n)\n\n# Configuration flows into module\ncp_calc_illum_load_data_mod = CPCalcIllumGenLoadDataModule.from_config(\n    data_config, pcp_experiment\n)\n</code></pre> <p>What happens: Configuration parameters flow into module creation</p> <p>Input \u2192 Output: Parameters (paths, channels) \u2192 Module instance</p>"},{"location":"architecture/09_architecture_flow_example/#modulemodule-generate-compute-graphs","title":"Module\u2192Module: Generate compute graphs","text":"<pre><code># Inside from_config method (not visible in example)\n# This happens within the module's initialization\ncompute_graph = ComputeGraph([container])\nreturn cls(compute_graph)\n</code></pre> <p>What happens: Module internally generates compute graph with container specification</p> <p>Input \u2192 Output: Configuration \u2192 Compute graph with inputs/outputs</p>"},{"location":"architecture/09_architecture_flow_example/#modulepipeline-pass-module-specifications","title":"Module\u2192Pipeline: Pass module specifications","text":"<pre><code># Module pipe passed to backend\nexec_backend = SnakeMakeBackend(\n    cp_calc_illum_load_data_mod.pipe,\n    backend_config,\n    exec_runs / \"run003\",\n    exec_mounts,\n)\n</code></pre> <p>What happens: Module's compute graph becomes available to pipeline/execution</p> <p>Input \u2192 Output: Module compute graph \u2192 Pipeline component</p>"},{"location":"architecture/09_architecture_flow_example/#pipelineexecution-submit-workflow","title":"Pipeline\u2192Execution: Submit workflow","text":"<pre><code># Alternative approach shows this more clearly\nreturn module_list, Seq(\n    [\n        Parallel(\n            [\n                Seq([cp_illum_calc_loaddata.pipe, ...]),\n                # other parallel sequences\n            ]\n        ),\n        # sequential steps\n    ]\n)\n\n# In step-by-step approach:\nrun = exec_backend.run()\n</code></pre> <p>What happens: Pipeline submits workflow for execution</p> <p>Input \u2192 Output: Pipeline specification \u2192 Execution plan</p>"},{"location":"architecture/09_architecture_flow_example/#executionexecution-translate-to-snakemake-rules","title":"Execution\u2192Execution: Translate to Snakemake rules","text":"<pre><code># Inside SnakeMakeBackend.run() (not visible in example)\n# Translates compute graph to Snakemake rules\n</code></pre> <p>What happens: Backend translates compute graph into Snakemake rules</p> <p>Input \u2192 Output: Compute graph \u2192 Snakemake rules</p>"},{"location":"architecture/09_architecture_flow_example/#executionruntime-schedule-container-execution","title":"Execution\u2192Runtime: Schedule container execution","text":"<p>This step transitions us from the Pipeline Composition Phase to the Runtime Execution Phase.</p> <pre><code># Container definition from modules/cp_illum_calc/calc_cp.py\nContainer(\n    name=\"cp_calc_illum_invoke_cp\",\n    input_paths={\n        \"cppipe_path\": [...],\n        \"load_data_path\": [...],\n    },\n    output_paths={\n        \"cp_illum_calc_dir\": [...]\n    },\n    config=ContainerConfig(\n        image=\"ghrc.io/leoank/starrynight:dev\",\n        cmd=[\"starrynight\", \"cp\", \"-p\", spec.inputs[0].path, ...],\n        env={},\n    ),\n)\n</code></pre> <p>What happens: Snakemake executes rules in container environment</p> <p>Input \u2192 Output: Snakemake rule \u2192 Container execution</p>"},{"location":"architecture/09_architecture_flow_example/#runtime-execution-phase","title":"Runtime Execution Phase","text":""},{"location":"architecture/09_architecture_flow_example/#runtimeclialgorithm-command-execution-flow","title":"Runtime\u2192CLI\u2192Algorithm: Command Execution Flow","text":"<p>When the container executes, the CLI layer bridges between runtime containers and algorithm functions:</p> <pre><code># Container definition invokes the starrynight CLI command\ncmd=[\"starrynight\", \"cp\", \"-p\", spec.inputs[0].path, \"-l\", spec.inputs[1].path, \n     \"-o\", spec.outputs[0].path]\n</code></pre> <p>When this container executes:</p> <ol> <li>The <code>starrynight</code> command invokes the main CLI entrypoint</li> <li>The <code>cp</code> subcommand selects the specific command</li> <li>The CLI parses arguments and validates paths</li> <li>The CLI then calls the corresponding algorithm function:</li> </ol> <pre><code># Inside starrynight/cli/cp.py\n@click.command(\"cp\")\n@click.option(\"-p\", \"--pipeline\", required=True, type=click.Path(exists=True))\n@click.option(\"-l\", \"--loaddata\", required=True, type=click.Path(exists=True))\n@click.option(\"-o\", \"--output-dir\", required=True, type=click.Path())\ndef cp_command(pipeline, loaddata, output_dir):\n    \"\"\"Run CellProfiler on a pipeline with a loaddata file.\"\"\"\n    from starrynight.algorithms.cellprofiler import run_cellprofiler\n\n    # Convert string paths to standardized path objects (simplified)\n    pipeline_path = AnyPath(pipeline)\n    loaddata_path = AnyPath(loaddata)\n    output_path = AnyPath(output_dir)\n\n    # CLI command translates parameters and calls algorithm function\n    run_cellprofiler(\n        pipeline_path=pipeline_path,\n        loaddata_path=loaddata_path,\n        output_dir=output_path\n    )\n</code></pre> <p>This in turn calls the pure algorithm function:</p> <pre><code># Inside starrynight/algorithms/cellprofiler.py\ndef run_cellprofiler(pipeline_path, loaddata_path, output_dir):\n    \"\"\"Run CellProfiler with specified pipeline and load data.\"\"\"\n    # Prepare environment and input files\n    prepare_input_files(loaddata_path)\n\n    # Execute core CellProfiler functionality\n    result = execute_cellprofiler_pipeline(pipeline_path, output_dir)\n\n    # Post-process results if needed\n    post_process_results(result, output_dir)\n\n    return result\n</code></pre> <p>What happens: Container command invokes CLI, which parses arguments and calls algorithm</p> <p>Input \u2192 Output: Container command line \u2192 CLI argument parsing \u2192 Algorithm function call \u2192 Processing results</p> <p>This three-layer approach (Container\u2192CLI\u2192Algorithm) provides several benefits:</p> <ol> <li>Algorithms remain pure functions without CLI or container dependencies</li> <li>CLI provides standardized interfaces and path handling</li> <li>Modules can compose different CLI commands into complex workflows</li> <li>The same algorithm can be invoked from different contexts (container, direct CLI, notebooks)</li> </ol> <p>The CLI layer is the essential bridge that allows containerized execution to access the underlying algorithm functionality while maintaining clean separation of concerns.</p> <p>What happens: Algorithm function executes core image processing logic</p> <p>Input \u2192 Output: Function parameters \u2192 Processed data</p>"},{"location":"architecture/09_architecture_flow_example/#algorithmcli-return-results-to-cli","title":"Algorithm\u2192CLI: Return results to CLI","text":"<pre><code># Continues from previous code block\ndef cp_command(pipeline, loaddata, output_dir):\n    # Call algorithm and get results\n    result = run_cellprofiler(\n        pipeline_path=pipeline,\n        loaddata_path=loaddata,\n        output_dir=output_dir\n    )\n\n    # CLI handles results (logging, exit code, etc.)\n    if result.success:\n        click.echo(f\"CellProfiler execution successful. Output in {output_dir}\")\n        return 0\n    else:\n        click.echo(f\"CellProfiler execution failed: {result.error}\")\n        return 1\n</code></pre> <p>What happens: Algorithm function returns results to CLI command</p> <p>Input \u2192 Output: Algorithm result \u2192 CLI output/exit code</p>"},{"location":"architecture/09_architecture_flow_example/#cliruntime-cli-process-completes","title":"CLI\u2192Runtime: CLI process completes","text":"<pre><code># Container execution completes when CLI process exits\n# Exit code from CLI determines container success/failure\n</code></pre> <p>What happens: CLI process exits, container execution completes</p> <p>Input \u2192 Output: CLI exit code \u2192 Container exit status</p>"},{"location":"architecture/09_architecture_flow_example/#runtimestorage-write-results","title":"Runtime\u2192Storage: Write results","text":"<pre><code># Container writes to output paths\noutput_paths={\n    \"cp_illum_calc_dir\": [workspace_path / \"cp_illum_calc\"]\n}\n</code></pre> <p>What happens: Container processes execute CLI commands that write results</p> <p>Input \u2192 Output: Container processing \u2192 Files on disk</p>"},{"location":"architecture/09_architecture_flow_example/#storageruntime-read-previous-outputs","title":"Storage\u2192Runtime: Read previous outputs","text":"<pre><code># Next module reads previous outputs\n# (Not directly visible but implied in dependencies)\ncp_calc_illum_cppipe_mod = CPCalcIllumGenCPPipeModule.from_config(\n    data_config, pcp_experiment\n)\n</code></pre> <p>What happens: Next phase reads outputs from previous phase</p> <p>Input \u2192 Output: Files from previous step \u2192 Input for next step</p>"},{"location":"architecture/09_architecture_flow_example/#flow-patterns-in-three-phase-execution","title":"Flow Patterns in Three-Phase Execution","text":"<p>Each three-phase pattern (LoadData \u2192 CPipe \u2192 Invoke) demonstrates the complete flow through all architecture layers. These phases map to the two architectural phases as follows:</p> <p>Pipeline Composition Phase steps in each CellProfiler phase:</p> <ol> <li>Config\u2192Module: Configuration flows into module</li> <li>Module\u2192Module: Generate compute graphs </li> <li>Module\u2192Pipeline: Pass module specifications</li> <li>Pipeline\u2192Execution: Submit workflow</li> <li>Execution\u2192Execution: Translate to Snakemake rules</li> </ol> <p>Runtime Execution Phase steps in each CellProfiler phase:</p> <ol> <li>Execution\u2192Runtime: Schedule container execution</li> <li>Runtime\u2192CLI: Invoke CLI commands</li> <li>CLI\u2192Algorithm: Call algorithm functions</li> <li>Algorithm\u2192Storage: Write results</li> </ol> <p>The three CellProfiler-specific phases each execute this full cycle but with different inputs/outputs:</p> <ol> <li>LoadData Phase:<ul> <li>Pipeline Composition: Configuration flows into module through to Snakemake rules</li> <li>Runtime Execution: Container executes, CLI generates LoadData CSV</li> <li>Result: CSV file written to disk</li> </ul> </li> <li>CPipe Phase:<ul> <li>Pipeline Composition: Same flow but with new module</li> <li>Runtime Execution: Container executes, reads LoadData CSV, CLI generates pipeline</li> <li>Result: Pipeline file written to disk</li> </ul> </li> <li>Invoke Phase:<ul> <li>Pipeline Composition: Same flow but with new module</li> <li>Runtime Execution: Container executes, reads both CSV and pipeline file, CLI invokes algorithm</li> <li>Result: Processed data written to disk</li> </ul> </li> </ol> <p>When using the pipeline composition approach shown in the \"Pipeline Composition (Alternative Approach)\" section in the Practical Integration document, this flow becomes more explicit since modules are composed in advance rather than executed one by one.</p>"},{"location":"architecture/09_architecture_flow_example/#cli-layer-as-the-bridge-between-container-and-algorithm","title":"CLI Layer as the Bridge Between Container and Algorithm","text":"<p>The CLI layer serves as a critical bridge between the containerized execution environment and the pure algorithm functions:</p> <pre><code># In container definition\ncmd=[\"starrynight\", \"cp\", \"-p\", \"${inputs.cppipe_path}\",\n     \"-l\", \"${inputs.load_data_path}\",\n     \"-o\", \"${outputs.cp_illum_calc_dir}\"]\n\n# Inside CLI implementation (starrynight/cli/main.py)\n@click.group()\ndef cli():\n    \"\"\"StarryNight CLI.\"\"\"\n    pass\n\ncli.add_command(cp.cp_command, name=\"cp\")\n# Other commands...\n\n# Inside algorithm module (starrynight/algorithms/cellprofiler.py)\ndef run_cellprofiler(pipeline_path, loaddata_path, output_dir):\n    \"\"\"\n    Run CellProfiler with the specified pipeline and loaddata.\n\n    This function encapsulates the core image processing logic.\n    \"\"\"\n    # Algorithm implementation\n</code></pre> <p>What happens:</p> <ol> <li>Container executes the <code>starrynight cp</code> command with inputs/outputs</li> <li>CLI parses arguments and provides a standardized interface to algorithms</li> <li>Algorithm functions contain pure implementation without CLI/container concerns</li> </ol> <p>Benefits of this approach:</p> <ul> <li>Separation of concerns: Algorithms focus on core functionality without UI/execution details</li> <li>Testability: Pure algorithm functions can be tested independently from CLI/containers</li> <li>Flexibility: Same algorithms can be accessed through different interfaces (CLI, API, notebook)</li> <li>Composability: CLI commands can combine multiple algorithm functions in useful ways</li> <li>Containerization: CLI provides a standard entrypoint for container execution</li> </ul> <p>This CLI layer pattern is consistent across all StarryNight modules, creating a clean separation between:</p> <ul> <li>Algorithm layer: Pure implementation of image processing functionality</li> <li>CLI layer: Command-line interfaces that parse arguments and call algorithms</li> <li>Module layer: Compute graph specifications that invoke CLI commands in containers</li> </ul> <p>When extending StarryNight with new capabilities, maintaining this separation through well-defined CLI interfaces ensures that algorithms remain reusable across different execution contexts.</p>"},{"location":"architecturev2/","title":"StarryNight Architecture","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p> <p>StarryNight is a layered framework for scientific image processing that transforms raw microscopy data into quantitative measurements. Its architecture separates concerns across six distinct layers, enabling scalable, reproducible analysis of Optical Pooled Screening experiments.</p>"},{"location":"architecturev2/#system-overview","title":"System Overview","text":"<p>StarryNight processes terabytes of microscopy images through a pipeline that progresses from simple functions to complex workflows:</p> <pre><code>flowchart TB\n        A[Algorithm Layer&lt;br/&gt;Standalone Python Functions&lt;br/&gt;starrynight/algorithms/]\n        B[CLI Layer&lt;br/&gt;Command-line Access&lt;br/&gt;starrynight/cli/]\n        C[Module Layer&lt;br/&gt;Specs &amp; Compute Graphs&lt;br/&gt;starrynight/modules/]\n        D[Pipeline Layer&lt;br/&gt;Workflow Composition&lt;br/&gt;starrynight/pipelines/]\n        E[Execution Layer&lt;br/&gt;Backend Runtime&lt;br/&gt;pipecraft/backend/]\n\n        A --&gt;|exposed as| B\n        B --&gt;|invoked by| C\n        C --&gt;|composed into| D\n        D --&gt;|translated to| E\n</code></pre> <p>Note: The Configuration Layer (<code>starrynight/experiments/</code>) operates as a cross-cutting concern, providing experiment parameters and settings that influence behavior across all layers.</p>"},{"location":"architecturev2/#layer-summary","title":"Layer Summary","text":""},{"location":"architecturev2/#algorithm-layer-foundation","title":"Algorithm Layer (Foundation)","text":"<p>Standalone Python functions implementing core image processing logic. No dependencies on other StarryNight components. Organized into algorithm sets that handle specific pipeline stages.</p> <p>Key characteristics:</p> <ul> <li>Complete independence from other layers</li> <li>Clear input/output contracts</li> <li>Support for both local and cloud storage via cloudpathlib</li> </ul>"},{"location":"architecturev2/#cli-layer-direct-access","title":"CLI Layer (Direct Access)","text":"<p>Command-line interfaces wrapping algorithm functions. Uses Click to provide user-friendly access with parameter validation and path handling.</p> <p>Key characteristics:</p> <ul> <li>Direct algorithm imports</li> <li>Consistent command structure</li> <li>Automatic path normalization</li> </ul>"},{"location":"architecturev2/#module-layer-abstraction","title":"Module Layer (Abstraction)","text":"<p>Standardized components combining specifications (via Bilayers) and compute graphs (via Pipecraft). Enables backend-agnostic execution and workflow composition.</p> <p>Key characteristics:</p> <ul> <li>Dual nature: specs define \"what\", compute graphs define \"how\"</li> <li>Container-based execution</li> <li>Three-function pattern: LoadData \u2192 Pipeline \u2192 Execution</li> </ul>"},{"location":"architecturev2/#pipeline-layer-composition","title":"Pipeline Layer (Composition)","text":"<p>Combines modules into complete workflows. Integrates with Pipecraft to create executable compute graphs with defined execution patterns.</p> <p>Key characteristics:</p> <ul> <li>Sequential and parallel execution blocks</li> <li>Backend independence</li> <li>End-to-end workflow definition</li> </ul>"},{"location":"architecturev2/#execution-layer-runtime","title":"Execution Layer (Runtime)","text":"<p>Manages actual execution via backends like Snakemake. Handles resource allocation, dependency management, and parallel processing.</p> <p>Key characteristics:</p> <ul> <li>Backend configuration</li> <li>Workflow translation</li> <li>Container orchestration</li> </ul>"},{"location":"architecturev2/#configuration-layer-cross-cutting","title":"Configuration Layer (Cross-cutting)","text":"<p>Provides experiment configuration and parameter inference. Unlike other layers, it operates orthogonally, influencing behavior across all layers.</p> <p>Key characteristics:</p> <ul> <li>Parameter inference from data</li> <li>Experiment-specific settings</li> <li>Module configuration generation</li> </ul>"},{"location":"architecturev2/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Separation of Concerns: Each layer has distinct responsibilities</li> <li>Backend Independence: Define once, run anywhere</li> <li>Progressive Enhancement: Each layer adds capabilities</li> <li>Explicit Over Implicit: Clear contracts and dependencies</li> <li>Composability: Components combine into larger workflows</li> </ol>"},{"location":"architecturev2/#common-patterns","title":"Common Patterns","text":""},{"location":"architecturev2/#three-function-pattern-cellprofiler","title":"Three-Function Pattern (CellProfiler)","text":"<p>CellProfiler-based algorithm sets implement:</p> <ol> <li>LoadData Generation: Create CSV files for image loading</li> <li>Pipeline Generation: Generate .cppipe pipeline files</li> <li>Execution: Run CellProfiler pipelines</li> </ol> <p>Other algorithm types (indexing, inventory, quality control) follow different patterns suited to their purpose.</p>"},{"location":"architecturev2/#specification-vs-implementation","title":"Specification vs Implementation","text":"<ul> <li>Specifications define interfaces and contracts</li> <li>Implementations provide concrete execution</li> <li>This separation enables multiple backends and inspection</li> </ul>"},{"location":"architecturev2/#container-based-execution","title":"Container-Based Execution","text":"<p>All processing runs in containers for:</p> <ul> <li>Reproducibility across environments</li> <li>Dependency isolation</li> <li>Scalable cloud execution</li> </ul>"},{"location":"architecturev2/#usage-flow","title":"Usage Flow","text":"<p>A typical StarryNight workflow:</p> <ol> <li>Index Generation: Scan and catalog available images</li> <li>Experiment Configuration: Define experimental parameters</li> <li>Module Selection: Choose appropriate processing modules</li> <li>Pipeline Composition: Connect modules into workflows</li> <li>Execution: Run pipelines on compute infrastructure</li> <li>Result Collection: Gather and organize outputs</li> </ol>"},{"location":"architecturev2/#technology-stack","title":"Technology Stack","text":"<ul> <li>Core: Python with type hints</li> <li>CLI: Click framework</li> <li>Paths: cloudpathlib for storage abstraction</li> <li>Specs: Bilayers for interface definition</li> <li>Pipelines: Pipecraft for compute graphs</li> <li>Execution: Snakemake for workflow orchestration</li> <li>Containers: Docker/Singularity for isolation</li> </ul>"},{"location":"architecturev2/#next-steps","title":"Next Steps","text":"<ul> <li>Principles - Fundamental principles and patterns</li> <li>Layer Documentation - See individual layer files (01-algorithm.md through 06-configuration.md)</li> </ul>"},{"location":"architecturev2/00-1-principles/","title":"Core Concepts &amp; Design Decisions","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p> <p>This document explains the fundamental concepts that underpin StarryNight's architecture and the rationale behind key design decisions. Understanding these principles and trade-offs is essential for working effectively with the framework.</p>"},{"location":"architecturev2/00-1-principles/#pipeline-terminology","title":"Pipeline Terminology","text":"<p>The term \"pipeline\" has two distinct meanings in StarryNight:</p> <ul> <li>CellProfiler Pipeline (.cppipe files): Image processing pipeline definition that contains CellProfiler modules and parameters, generated by algorithm functions, and executed by CellProfiler software.</li> <li>StarryNight Pipeline (Pipecraft graphs): Workflow orchestration pipeline that connects multiple modules into workflows, defines execution order and parallelism, and is executed by backends like Snakemake.</li> </ul> <p>Use \"CellProfiler pipeline\" or \".cppipe\" for image processing definitions, \"workflow pipeline\" or \"Pipecraft pipeline\" for StarryNight orchestration, and rely on context to make the distinction clear.</p>"},{"location":"architecturev2/00-1-principles/#core-architecture","title":"Core Architecture","text":""},{"location":"architecturev2/00-1-principles/#layered-architecture","title":"Layered Architecture","text":"<p>StarryNight employs a strict layered architecture where each layer depends only on layers below it:</p> <ul> <li>Upward Dependencies Only: Higher layers import from lower layers, never the reverse</li> <li>Clean Interfaces: Each layer exposes well-defined interfaces</li> <li>Progressive Enhancement: Each layer adds capabilities without modifying lower layers</li> <li>Testability: Lower layers can be tested in isolation</li> </ul> <p>Rationale: This design provides testability (lower layers can be tested without framework setup), reusability (algorithm functions work outside StarryNight), maintainability (changes in lower layers don't break higher layers), and clear contracts (each layer has well-defined interfaces).</p> <p>Trade-offs: More layers mean more abstraction overhead, requires discipline to maintain layer boundaries, and may duplicate some logic across layers.</p>"},{"location":"architecturev2/00-1-principles/#algorithm-independence","title":"Algorithm Independence","text":"<p>The algorithm layer's complete independence is fundamental to StarryNight's design. Algorithm functions are implemented as standalone Python functions that accept standard Python types as inputs and produce outputs using only Python standard library and common third-party libraries.</p> <p>Rationale: Algorithm layer has zero dependencies on other StarryNight components to ensure portability (algorithms can be used in any Python environment), testing (no framework setup required for unit tests), clarity (business logic separated from infrastructure), and evolution (framework can change without affecting algorithms).</p>"},{"location":"architecturev2/00-1-principles/#layer-specific-patterns","title":"Layer-Specific Patterns","text":""},{"location":"architecturev2/00-1-principles/#specification-vs-implementation","title":"Specification vs Implementation","text":"<p>StarryNight separates \"what\" from \"how\" at multiple levels:</p> <ul> <li>Module Level: Specification (Bilayers) defines inputs, outputs, and parameters; compute graph (Pipecraft) defines execution structure; and execution (backend) performs actual computation.</li> <li>Pipeline Level: Definition provides abstract workflow description, translation creates backend-specific execution plan, and runtime performs actual process execution.</li> </ul> <p>Rationale: Modules define both specifications (Bilayers) and compute graphs (Pipecraft) to achieve backend independence (same module runs on different platforms), inspection (can examine workflow before execution), UI generation (specs enable automatic interface creation), and validation (type checking before runtime).</p> <p>Trade-offs: Additional complexity in maintaining both specifications and implementations, requires understanding of multiple abstraction layers.</p>"},{"location":"architecturev2/00-1-principles/#module-layer-as-cli-wrapper","title":"Module Layer as CLI Wrapper","text":"<p>Modules invoke CLI commands rather than calling algorithms directly. This provides consistency (single execution path for all invocations), isolation (container boundaries match CLI boundaries), testing (CLI provides natural integration test points), and debugging (can manually run same commands modules generate).</p> <p>Rationale: Using CLI as the interface layer ensures all execution paths go through the same validated entry points, making behavior predictable and debugging straightforward.</p> <p>Trade-offs: Additional overhead of process creation, string-based command construction.</p>"},{"location":"architecturev2/00-1-principles/#three-function-pattern-cellprofiler-workflows","title":"Three-Function Pattern (CellProfiler Workflows)","text":"<p>CellProfiler-based workflows in StarryNight follow a consistent three-step pattern:</p> <ol> <li>LoadData Generation: Identifies what images to process by reading indexes/inventories, filtering by criteria, and outputting CSV files for CellProfiler.</li> <li>Pipeline Generation: Defines how to process images by inferring parameters from sample data, configuring CellProfiler modules, and outputting .cppipe pipeline definitions.</li> <li>Execution: Runs the CellProfiler pipeline by managing parallel execution and organizing outputs.</li> </ol> <p>Rationale: Organizing CellProfiler algorithm sets around LoadData \u2192 Pipeline \u2192 Execution pattern provides consistency (same pattern across CellProfiler analyses), modularity (each step has clear purpose), flexibility (can mix and match components), and understanding (predictable structure aids comprehension).</p> <p>Clarification: Other algorithm types (indexing, inventory, quality control) follow different patterns based on their purpose.</p>"},{"location":"architecturev2/00-1-principles/#module-pipeline-concepts","title":"Module &amp; Pipeline Concepts","text":""},{"location":"architecturev2/00-1-principles/#module-composition","title":"Module Composition","text":"<p>Modules compose into pipelines through typed interfaces, execution patterns, and resource management:</p> <ul> <li>Input/Output Contracts: Typed interfaces define connections, outputs of one module feed inputs of another, and type checking ensures compatibility.</li> <li>Execution Patterns: Sequential blocks for dependent operations, parallel blocks for independent operations, and nested blocks for complex workflows.</li> <li>Resource Management: Modules declare resource requirements, execution layer manages allocation, and automatic cleanup after completion.</li> </ul> <p>Rationale: Build complex workflows by composing simple modules to achieve flexibility (mix and match components), testing (test modules independently), reuse (share modules across workflows), and understanding (smaller units easier to comprehend).</p>"},{"location":"architecturev2/00-1-principles/#pipecraft-as-separate-package","title":"Pipecraft as Separate Package","text":"<p>Pipeline construction is implemented as a separate package within the monorepo. This provides focus (clean separation of concerns), reusability (could be used by other projects), testing (independent test suite), and evolution (can evolve independently).</p> <p>Rationale: Separating workflow construction from core StarryNight functionality allows the pipeline engine to evolve independently and potentially be reused by other projects with similar needs.</p> <p>Clarification: Pipecraft creates StarryNight workflow pipelines (compute graphs), not CellProfiler pipelines (.cppipe files).</p>"},{"location":"architecturev2/00-1-principles/#cross-cutting-concerns","title":"Cross-Cutting Concerns","text":""},{"location":"architecturev2/00-1-principles/#path-abstraction","title":"Path Abstraction","text":"<p>All StarryNight components use cloudpathlib's <code>AnyPath</code> for storage abstraction. The cloudpathlib AnyPath abstraction provides a unified interface for accessing both local filesystem paths and cloud storage locations (S3, Google Cloud Storage, Azure Blob Storage).</p> <p>Rationale: Using <code>AnyPath</code> abstraction for all file operations provides storage agnostic code (same code for local/S3/GCS), future proofing (easy to add new storage backends), clean code (no if/else for different path types), and testing (can mock storage in tests).</p> <p>Trade-offs: Adds dependency on cloudpathlib and may have slight performance overhead compared to native path operations.</p>"},{"location":"architecturev2/00-1-principles/#container-based-execution","title":"Container-Based Execution","text":"<p>All processing runs in containers to ensure reproducibility (fixed software versions, isolated dependencies, consistent environments), portability (runs on any container platform, cloud-native execution, local development mirrors production), and scalability (parallel container instances, resource isolation, efficient scheduling).</p> <p>Rationale: Container-based execution with explicit specifications ensures reproducibility (fixed versions and dependencies), portability (same containers run locally and in cloud), isolation (no dependency conflicts), and scalability (natural unit for parallel execution).</p> <p>Trade-offs: Container overhead for small operations, requires container runtime, and more complex local development setup.</p>"},{"location":"architecturev2/00-1-principles/#parameter-inference","title":"Parameter Inference","text":"<p>StarryNight minimizes configuration through intelligent inference:</p> <ul> <li>Index Analysis: Extract metadata from data</li> <li>Sample Inspection: Infer parameters from representative files</li> <li>Smart Defaults: Provide sensible defaults for common cases</li> <li>User Override: Allow explicit configuration when needed</li> </ul> <p>Rationale: Infer parameters from data when possible rather than requiring explicit configuration to improve user experience (less configuration burden), correctness (parameters match actual data), adaptability (automatically adjusts to data changes), and defaults (smart defaults for common cases).</p> <p>Trade-offs: More complex implementation logic, potential for incorrect inference in edge cases, and debugging complexity when inference fails.</p> <p>Example: The system infers parameters from data when possible (e.g., counting unique channels from the index) rather than requiring users to specify values that can be determined automatically.</p>"},{"location":"architecturev2/00-1-principles/#cross-cutting-configuration","title":"Cross-Cutting Configuration","text":"<p>The configuration layer operates orthogonally to other layers, influencing behavior across the entire stack. This enables consistent parameters across layers, experiment-specific adaptations, and runtime behavior modification.</p> <p>Trade-offs: More complex parameter resolution logic and potential for configuration conflicts across layers.</p>"},{"location":"architecturev2/00-1-principles/#execution-backend","title":"Execution &amp; Backend","text":""},{"location":"architecturev2/00-1-principles/#backend-abstraction","title":"Backend Abstraction","text":"<p>StarryNight supports multiple execution backends through abstraction:</p> <ul> <li>Current: Snakemake - Rule-based workflow engine with automatic dependency resolution and built-in parallelism.</li> <li>Possible: Others - AWS Batch, Google Cloud Run, Kubernetes, and local execution.</li> </ul> <p>Rationale: Using Snakemake as the first execution backend provides maturity (battle-tested in bioinformatics), features (built-in parallelism, dependency resolution), community (active development and support), and flexibility (supports local and cluster execution). The architecture allows adding other backends (Nextflow, CWL, etc.) in the future.</p> <p>Trade-offs: Initial implementation tied to Snakemake's execution model, requires abstraction layer to support multiple backends effectively.</p>"},{"location":"architecturev2/00-1-principles/#development-practices","title":"Development Practices","text":""},{"location":"architecturev2/00-1-principles/#error-handling-philosophy","title":"Error Handling Philosophy","text":"<p>StarryNight follows consistent error handling principles:</p> <ul> <li>Fail Fast: Detect problems early</li> <li>Clear Messages: Provide actionable error information</li> <li>Graceful Degradation: Continue processing unaffected data</li> <li>Audit Trail: Log all operations for debugging</li> </ul> <p>Rationale: Consistent error handling across all layers improves debugging experience, reduces time to resolution, and enables partial processing of large datasets when some components fail.</p> <p>Trade-offs: Requires discipline to implement consistently and may add verbosity to error handling code.</p>"},{"location":"architecturev2/00-1-principles/#testing-strategy","title":"Testing Strategy","text":"<p>StarryNight employs layer-specific testing approaches, though implementation is currently in progress:</p> <p>Current Implementation:</p> <ul> <li>Algorithm: Unit tests for core algorithms (e.g., index generation)</li> <li>Integration: End-to-end workflow tests for key use cases</li> <li>Parsers: Component tests for data parsing functionality</li> <li>Utilities: Unit tests for helper functions</li> </ul> <p>Planned Implementation:</p> <ul> <li>CLI: Integration tests with command execution</li> <li>Module: Specification validation tests</li> <li>Pipeline: Composition and graph tests</li> <li>Execution: Backend-specific execution tests</li> <li>Configuration: Parameter inference tests</li> </ul> <p>Rationale: Layer-specific testing strategies should match the abstraction level and purpose of each component, enabling comprehensive coverage without unnecessary complexity.</p> <p>Trade-offs: Requires maintaining multiple test suites and understanding different testing approaches for different layers.</p>"},{"location":"architecturev2/00-1-principles/#trade-off-summary","title":"Trade-off Summary","text":"Decision Benefits Costs Layered Architecture Clear boundaries, testability Abstraction overhead Algorithm Independence Portability, simplicity No framework features CLI Wrapping Consistency, isolation Process overhead Container Execution Reproducibility, scalability Setup complexity Parameter Inference Better UX, correctness Complex implementation <p>These decisions create a system that prioritizes:</p> <ol> <li>Reproducibility over convenience</li> <li>Flexibility over simplicity</li> <li>Explicitness over magic</li> <li>Composition over monoliths</li> </ol> <p>Future architectural decisions should align with these principles while pragmatically addressing user needs.</p>"},{"location":"architecturev2/00-1-principles/#next-steps","title":"Next Steps","text":"<p>For implementation details, see the individual layer documentation files (01-algorithm.md through 06-configuration.md).</p>"},{"location":"architecturev2/01-algorithm/","title":"Algorithm Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/01-algorithm/#purpose","title":"Purpose","text":"<p>The Algorithm Layer forms the foundation of StarryNight, providing Python functions that implement core image processing logic. This layer solves the fundamental problem of encapsulating scientific algorithms with minimal dependencies on infrastructure concerns, enabling reuse across different execution contexts and frameworks. By minimizing dependencies to essential utilities and templates, algorithms can be developed, tested, and validated with clear interfaces, ensuring that scientific logic remains focused and maintainable.</p>"},{"location":"architecturev2/01-algorithm/#responsibilities","title":"Responsibilities","text":"<ul> <li>Implement standalone Python functions for image processing tasks</li> <li>Organize algorithms by workflow type in <code>starrynight/src/starrynight/algorithms/</code><ul> <li>Core workflows: <code>illum_calc</code>, <code>illum_apply</code>, <code>analysis</code>, <code>align</code>, <code>preprocess</code></li> <li>Specialized workflows: <code>segcheck</code>, <code>presegcheck</code>, <code>stitchcrop</code>, <code>inventory</code>, <code>index</code></li> <li>Shared execution: <code>cp.py</code> provides common CellProfiler execution</li> </ul> </li> <li>Maintain clear input/output contracts using standard Python types and cloudpathlib paths</li> <li>NOT responsible for: Command-line interfaces, workflow orchestration, execution management, or any infrastructure concerns</li> </ul>"},{"location":"architecturev2/01-algorithm/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Functional Independence (Decision #2): Algorithm functions minimize dependencies, importing only necessary utilities from <code>starrynight.utils</code>, <code>templates</code>, and <code>parsers</code>. This controlled dependency approach balances practical code reuse with maintaining clear boundaries. Core algorithm logic remains independent, while common utilities for data formatting and template rendering are shared.</p> </li> <li> <p>Three-Function Pattern (Decision #7): CellProfiler-based algorithms follow a consistent pattern of LoadData generation (<code>gen_*_load_data</code>), Pipeline generation (<code>gen_*_cppipe</code>), and Execution (<code>run_cp</code>). The execution function is shared across all CellProfiler workflows, promoting code reuse. This standardization enables predictable integration while maintaining flexibility for non-CellProfiler algorithms.</p> </li> </ol>"},{"location":"architecturev2/01-algorithm/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/01-algorithm/#inputs","title":"Inputs","text":"<ul> <li>File paths via cloudpathlib AnyPath (supporting local and cloud storage)</li> <li>Algorithm-specific parameters as primitive Python types</li> <li>Data structures using standard libraries (pandas/polars DataFrames, NumPy arrays)</li> </ul>"},{"location":"architecturev2/01-algorithm/#outputs","title":"Outputs","text":"<ul> <li>Processed data files written to specified paths</li> <li>Return values as standard Python types or None for side-effect operations</li> <li>Generated artifacts (CSV files, .cppipe definitions, processed images)</li> </ul>"},{"location":"architecturev2/01-algorithm/#dependencies","title":"Dependencies","text":"<ul> <li>Internal dependencies: <code>starrynight.utils</code> (data formatting), <code>starrynight.templates</code> (template rendering), <code>starrynight.parsers</code> (file parsing)</li> <li>External dependencies: cloudpathlib, pandas/polars, NumPy, scikit-image, CellProfiler (for specific algorithms)</li> </ul>"},{"location":"architecturev2/01-algorithm/#patterns","title":"Patterns","text":"<p>The algorithm layer follows consistent patterns for implementing image processing workflows. Here's the typical structure found in <code>starrynight/src/starrynight/algorithms/</code>:</p>"},{"location":"architecturev2/01-algorithm/#three-function-pattern","title":"Three-Function Pattern","text":"<p>The pattern consists of three complementary functions that work together for CellProfiler workflows:</p> <ol> <li> <p>LoadData Generation (<code>gen_*_load_data</code>): Reads input data, applies filtering and transformations, then writes a LoadData CSV file. These functions leverage shared utilities from <code>starrynight.utils</code> for consistent data formatting across workflows.</p> </li> <li> <p>Pipeline Generation (<code>gen_*_cppipe</code>): Configures CellProfiler modules, builds the processing pipeline, and saves the configuration. The template system from <code>starrynight.templates</code> ensures consistent pipeline generation while allowing workflow-specific customization.</p> </li> <li> <p>Execution (<code>run_cp</code>): A shared function that loads the pipeline, configures the execution context, and runs the processing. This common execution function is implemented in <code>starrynight/src/starrynight/algorithms/cp.py</code> and used by all CellProfiler-based algorithms.</p> </li> </ol> <p>This pattern enables consistent integration while maintaining flexibility for non-CellProfiler algorithms that may only need subset of these functions.</p>"},{"location":"architecturev2/01-algorithm/#naming-conventions","title":"Naming Conventions","text":"<p>Functions follow predictable naming patterns:</p> <ul> <li><code>gen_*_load_data</code> for LoadData generation functions</li> <li><code>gen_*_cppipe</code> for pipeline generation functions</li> <li><code>run_*</code> for execution functions</li> </ul>"},{"location":"architecturev2/01-algorithm/#path-handling","title":"Path Handling","text":"<p>All paths use cloudpathlib's AnyPath for cloud/local compatibility, ensuring algorithms work seamlessly across different storage backends.</p>"},{"location":"architecturev2/01-algorithm/#utility-delegation","title":"Utility Delegation","text":"<p>Common operations are delegated to <code>starrynight.utils</code> modules, promoting code reuse and consistent behavior across algorithms.</p>"},{"location":"architecturev2/01-algorithm/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Data preparation, pipeline configuration, and execution are kept as distinct functions, allowing flexible composition and testing of individual components.</p>"},{"location":"architecturev2/01-algorithm/#implementation-location","title":"Implementation Location","text":"<p>All algorithm implementations can be found in:</p> <ul> <li>Primary location: <code>starrynight/src/starrynight/algorithms/</code></li> <li>Tests: <code>starrynight/tests/algorithms/</code></li> </ul> <p>Each algorithm file typically contains the relevant functions for its workflow, following the patterns described above.</p>"},{"location":"architecturev2/01-algorithm/#see-also","title":"See Also","text":"<ul> <li>Next: CLI Layer</li> </ul>"},{"location":"architecturev2/02-cli/","title":"CLI Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/02-cli/#purpose","title":"Purpose","text":"<p>The CLI Layer provides command-line access to StarryNight's algorithm functions, solving the problem of making scientific algorithms accessible to users without requiring Python programming knowledge. This layer wraps standalone Python algorithm functions with user-friendly interfaces, parameter validation, and consistent error handling. By maintaining a thin wrapper approach, the CLI layer ensures that all algorithm capabilities remain available through both programmatic and command-line interfaces, supporting both interactive exploration and automated pipeline execution.</p>"},{"location":"architecturev2/02-cli/#responsibilities","title":"Responsibilities","text":"<ul> <li>Wrap algorithm functions with Click-based command-line interfaces</li> <li>Parse and validate command-line arguments before passing to algorithms</li> <li>Provide consistent command structure and user experience across all algorithm sets</li> <li>NOT responsible for: Implementing algorithms, orchestrating workflows, managing execution backends, or handling complex multi-step operations</li> </ul>"},{"location":"architecturev2/02-cli/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Direct Algorithm Imports (Decision #3): The CLI layer imports algorithm functions directly and wraps them without modification. This thin wrapper approach ensures that CLI commands have identical behavior to direct function calls, maintaining consistency and avoiding hidden complexity. The Click framework provides the infrastructure for parameter parsing and command organization.</p> </li> <li> <p>Click Integration (Decision #8): All CLI implementations use the Click framework for its declarative parameter definition, automatic help generation, and robust argument parsing. This standardization ensures consistent user experience across commands while leveraging Click's built-in support for path handling, type conversion, and validation.</p> </li> </ol>"},{"location":"architecturev2/02-cli/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/02-cli/#inputs","title":"Inputs","text":"<ul> <li>Command-line arguments and options parsed by Click</li> <li>File paths as strings converted to cloudpathlib AnyPath objects</li> <li>Algorithm parameters with Click-validated types and constraints</li> </ul>"},{"location":"architecturev2/02-cli/#outputs","title":"Outputs","text":"<ul> <li>Algorithm function return values passed through to shell</li> <li>Status messages and progress indicators to stderr</li> <li>Exit codes following Unix conventions (0 for success, non-zero for errors)</li> </ul>"},{"location":"architecturev2/02-cli/#dependencies","title":"Dependencies","text":"<ul> <li>Internal dependencies: Algorithm layer functions (direct imports following upward-only rule)</li> <li>External dependencies: Click framework, cloudpathlib for path conversion</li> </ul>"},{"location":"architecturev2/02-cli/#patterns","title":"Patterns","text":"<p>The CLI layer follows consistent patterns for wrapping algorithm functions:</p>"},{"location":"architecturev2/02-cli/#command-structure-pattern","title":"Command Structure Pattern","text":"<p>The CLI uses Click's decorator-based approach to create hierarchical command structures. Commands are organized into groups, with each group containing related subcommands. Options are defined with validation rules, while arguments represent required positional parameters. This pattern enables intuitive command-line interfaces like <code>starrynight illum calc</code> or <code>starrynight analysis run</code>.</p>"},{"location":"architecturev2/02-cli/#path-handling-pattern","title":"Path Handling Pattern","text":"<p>All file paths received as command-line strings are converted to cloudpathlib's AnyPath objects before being passed to algorithm functions. This conversion happens at the CLI boundary, ensuring that algorithm functions always receive path objects that work seamlessly with both local filesystems and cloud storage. The pattern maintains consistency across all commands while abstracting storage details from users.</p>"},{"location":"architecturev2/02-cli/#direct-algorithm-wrapping-pattern","title":"Direct Algorithm Wrapping Pattern","text":"<p>CLI commands import algorithm functions directly and wrap them with minimal additional logic. The wrapper handles only CLI-specific concerns: parameter conversion from strings to appropriate types, validation of user inputs, and status reporting. The actual algorithm call passes through unchanged, maintaining the principle that CLI commands are thin wrappers around algorithm functionality.</p>"},{"location":"architecturev2/02-cli/#parameter-patterns","title":"Parameter Patterns","text":"<ul> <li>File paths: Use <code>click.Path()</code> for validation</li> <li>Boolean flags: Use <code>is_flag=True</code></li> <li>Multiple values: Use <code>multiple=True</code> for lists</li> <li>Choices: Use <code>click.Choice()</code> for enums</li> <li>Required options: Use <code>required=True</code></li> </ul>"},{"location":"architecturev2/02-cli/#error-handling-pattern","title":"Error Handling Pattern","text":"<p>The CLI layer relies on Click's built-in error handling:</p> <ul> <li>Invalid arguments produce automatic error messages</li> <li>Missing required options are caught by Click</li> <li>Algorithm errors propagate naturally to the user</li> </ul> <p>This approach ensures that CLI commands remain thin wrappers that focus solely on command-line interface concerns, while the actual implementation logic stays in the algorithm layer.</p>"},{"location":"architecturev2/02-cli/#implementation-location","title":"Implementation Location","text":"<ul> <li>Primary location: <code>starrynight/src/starrynight/cli/</code></li> <li>Main entry point: <code>starrynight/src/starrynight/cli/main.py</code></li> <li>Tests: <code>starrynight/tests/cli/</code></li> </ul> <p>Individual command modules are named after the algorithms they wrap (e.g., <code>illum.py</code> for illumination commands, <code>index.py</code> for barcode indexing).</p>"},{"location":"architecturev2/02-cli/#see-also","title":"See Also","text":"<ul> <li>Previous: Algorithm Layer, Next: Module Layer</li> </ul>"},{"location":"architecturev2/03-module/","title":"Module Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/03-module/#purpose","title":"Purpose","text":"<p>The Module Layer provides standardized, composable units of work that bridge the gap between individual algorithm functions and complete workflows. This layer solves the problem of expressing complex multi-step operations in a backend-agnostic way by combining specifications (what needs to be done) with compute graphs (how execution flows). By maintaining a dual nature of declarative specifications through a schema system (derived from Bilayers) and executable graphs through Pipecraft, modules enable inspection, validation, and execution across different computational backends while preserving the simplicity of CLI-based invocation.</p>"},{"location":"architecturev2/03-module/#responsibilities","title":"Responsibilities","text":"<ul> <li>Define module specifications using a schema system for inputs, outputs, and parameters</li> <li>Create compute graphs via Pipecraft to express execution dependencies and data flow</li> <li>Map to CLI command groups for consistent invocation patterns</li> <li>NOT responsible for: Implementing algorithms, managing workflow orchestration, handling backend-specific execution, or defining complete end-to-end pipelines</li> </ul>"},{"location":"architecturev2/03-module/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Dual Nature Design (Decision #4): Modules combine schema specifications (defining interfaces) with Pipecraft compute graphs (defining execution). The schema is derived from Bilayers but used locally through starrynight.modules.schema. This separation of \"what\" from \"how\" enables multiple critical capabilities: backend-agnostic execution, workflow inspection before running, parameter validation, and graph optimization. The specification defines the contract while the compute graph defines the implementation strategy.</p> </li> <li> <p>CLI Invocation Pattern (Decision #3): Each module maps directly to a CLI command group, maintaining consistency between programmatic and command-line usage. This design ensures that modules can be invoked identically whether called from Python code, shell scripts, or workflow engines. The pattern reinforces the principle that modules are self-contained units executable in isolation.</p> </li> </ol>"},{"location":"architecturev2/03-module/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/03-module/#inputs","title":"Inputs","text":"<ul> <li>Module specifications via schema system defining required parameters</li> <li>Data paths and configuration through standardized interfaces</li> <li>Dependencies from CLI layer commands</li> </ul>"},{"location":"architecturev2/03-module/#outputs","title":"Outputs","text":"<ul> <li>Compute graphs consumable by Pipecraft</li> <li>Execution artifacts as defined in specifications</li> <li>Module metadata for pipeline composition</li> </ul>"},{"location":"architecturev2/03-module/#dependencies","title":"Dependencies","text":"<p>Internal dependencies:</p> <ul> <li>CLI layer for command implementations (following upward-only rule)</li> <li><code>starrynight.modules.schema</code> (derived from Bilayers)</li> <li>Module base class: <code>starrynight.modules.common.StarrynightModule</code></li> </ul> <p>External dependencies:</p> <ul> <li>Pipecraft for compute graphs</li> </ul>"},{"location":"architecturev2/03-module/#patterns","title":"Patterns","text":"<p>Modules in StarryNight follow a consistent pattern that combines declarative specifications with executable compute graphs. Every module inherits from <code>StarrynightModule</code> and implements four key methods. This dual nature pattern demonstrates how modules separate the contract (what inputs/outputs are expected) from the execution strategy (how work gets done through containerized steps). The pattern ensures CLI commands are invoked consistently, execution remains backend-agnostic through containerization, interfaces can be validated before execution, and modules remain composable into larger pipelines.</p>"},{"location":"architecturev2/03-module/#module-identity-pattern","title":"Module Identity Pattern","text":"<p>A unique name and identifier for the module that establishes its identity within the system and enables registration in the module registry.</p>"},{"location":"architecturev2/03-module/#specification-pattern","title":"Specification Pattern","text":"<p>Declarative interface definition using the schema system that defines inputs, outputs, parameters, and citations. This pattern provides a formal contract that can be validated before execution.</p>"},{"location":"architecturev2/03-module/#pipeline-creation-pattern","title":"Pipeline Creation Pattern","text":"<p>Compute graph construction via Pipecraft that specifies container-based execution with CLI commands. This pattern translates the declarative specification into an executable workflow.</p>"},{"location":"architecturev2/03-module/#unit-of-work-pattern","title":"Unit of Work Pattern","text":"<p>Optional task decomposition for parallel execution. This pattern allows modules to split work into independent units that can be processed concurrently.</p>"},{"location":"architecturev2/03-module/#implementation-location","title":"Implementation Location","text":"<p>Primary location: <code>starrynight/src/starrynight/modules/</code> Tests: <code>starrynight/tests/modules/</code></p> <p>Modules are organized by workflow type in subdirectories:</p> <ul> <li><code>cp_illum_calc/</code> - Cell Painting illumination calculation modules</li> <li><code>sbs_illum_calc/</code> - SBS illumination calculation modules</li> <li><code>analysis/</code> - Analysis modules</li> <li><code>stitchcrop/</code> - Stitching and cropping modules</li> </ul> <p>Each module directory typically contains load data generation, pipeline generation, and execution modules. All available modules are registered in <code>starrynight/src/starrynight/modules/registry.py</code>.</p>"},{"location":"architecturev2/03-module/#see-also","title":"See Also","text":"<ul> <li>Previous: CLI Layer, Next: Pipeline Layer</li> </ul>"},{"location":"architecturev2/04-pipeline/","title":"Pipeline Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/04-pipeline/#purpose","title":"Purpose","text":"<p>The Pipeline Layer orchestrates complete scientific workflows by composing individual modules into end-to-end processing pipelines. This layer solves the problem of expressing complex, multi-stage analyses that span from raw microscopy data to final measurements, while maintaining clarity about execution order, parallelism opportunities, and data dependencies. By building on Pipecraft's compute graph capabilities, the Pipeline Layer enables researchers to define reproducible workflows that can execute across different computational backends without modification.</p>"},{"location":"architecturev2/04-pipeline/#responsibilities","title":"Responsibilities","text":"<ul> <li>Compose modules into complete workflow definitions</li> <li>Define execution patterns including sequential and parallel processing blocks</li> <li>Integrate with Pipecraft to create executable compute graphs</li> <li>NOT responsible for: Implementing processing logic, managing actual execution, handling backend-specific details, or defining module internals</li> </ul>"},{"location":"architecturev2/04-pipeline/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Pipecraft Integration (Decision #9): Pipelines are expressed as Pipecraft compute graphs rather than backend-specific formats. This abstraction enables critical capabilities: backend portability (same pipeline runs on local machines or cloud), execution optimization (automatic parallelization where possible), and workflow inspection (visualize before running). The integration maintains StarryNight's principle of separating workflow definition from execution strategy.</p> </li> <li> <p>Module Composition (Decision #1): Following the layered architecture principle, pipelines compose modules without bypassing layers or accessing lower-level components directly. This ensures that pipelines remain high-level workflow definitions rather than detailed execution scripts. The composition pattern enables reuse of validated modules across different experimental contexts.</p> </li> </ol>"},{"location":"architecturev2/04-pipeline/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/04-pipeline/#inputs","title":"Inputs","text":"<ul> <li>Module definitions from the Module Layer</li> <li>Pipeline configuration specifying module connections</li> <li>Execution patterns (sequential/parallel blocks)</li> </ul>"},{"location":"architecturev2/04-pipeline/#outputs","title":"Outputs","text":"<ul> <li>Complete Pipecraft compute graphs ready for execution</li> <li>Pipeline metadata for validation and inspection</li> <li>Workflow documentation and visualization</li> </ul>"},{"location":"architecturev2/04-pipeline/#dependencies","title":"Dependencies","text":"<ul> <li>Internal dependencies: Module Layer for component definitions (following upward-only rule)</li> <li>External dependencies: Pipecraft for workflow representation</li> </ul>"},{"location":"architecturev2/04-pipeline/#patterns","title":"Patterns","text":"<p>The Pipeline Layer follows a consistent pattern for composing modules into workflows:</p>"},{"location":"architecturev2/04-pipeline/#pipeline-definition","title":"Pipeline Definition","text":"<p>Pipelines are defined as functions that return both a module list and a Pipecraft pipeline structure. The pattern initializes modules with configuration, composes them using Pipecraft's sequential and parallel primitives, and returns both the module list and pipeline structure for execution.</p>"},{"location":"architecturev2/04-pipeline/#module-composition","title":"Module Composition","text":"<p>The Pipeline Layer uses Pipecraft's composition primitives:</p> <ul> <li>Seq: Sequential execution of pipeline stages</li> <li>Parallel: Concurrent execution of independent branches</li> <li>Pipeline: Base abstraction for all pipeline components</li> </ul>"},{"location":"architecturev2/04-pipeline/#registry-pattern","title":"Registry Pattern","text":"<p>All available pipelines are registered in a central registry, enabling dynamic pipeline discovery and instantiation based on experiment requirements.</p>"},{"location":"architecturev2/04-pipeline/#composition-structure","title":"Composition Structure","text":"<p>A typical Cell Painting pipeline demonstrates the composition pattern:</p> <ol> <li>Module Initialization: Create module instances with experiment-specific parameters</li> <li>Parallel Preprocessing: Cell Painting and SBS workflows run concurrently</li> <li>Sequential Analysis: Final analysis steps execute after preprocessing completes</li> <li>Module Communication: Data flows between modules through defined interfaces</li> </ol> <p>The actual module types and their specific parameters vary based on the experiment configuration, but the composition pattern remains consistent across all pipeline implementations.</p>"},{"location":"architecturev2/04-pipeline/#implementation-location","title":"Implementation Location","text":"<ul> <li>Primary location: <code>starrynight/src/starrynight/pipelines/</code></li> <li>Tests: <code>starrynight/tests/pipelines/</code></li> <li>Registry: <code>starrynight/src/starrynight/pipelines/registry.py</code></li> <li>Pipecraft primitives: <code>pipecraft/src/pipecraft/pipeline.py</code></li> </ul>"},{"location":"architecturev2/04-pipeline/#see-also","title":"See Also","text":"<ul> <li>Previous: Module Layer, Next: Execution Layer</li> </ul>"},{"location":"architecturev2/05-execution/","title":"Execution Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/05-execution/#purpose","title":"Purpose","text":"<p>The Execution Layer manages the actual runtime execution of StarryNight pipelines on computational infrastructure. This layer is primarily implemented within the Pipecraft package and orchestrated by the Conductor service, solving the critical problem of translating abstract workflow definitions into concrete executions across diverse environments\u2014from local workstations to cloud platforms. By abstracting backend-specific details through Pipecraft's backend system, the Execution Layer enables researchers to focus on scientific workflows rather than infrastructure management, while maintaining the flexibility to leverage different computational resources as needed.</p>"},{"location":"architecturev2/05-execution/#responsibilities","title":"Responsibilities","text":"<ul> <li>Translate Pipecraft graphs into backend-specific execution plans</li> <li>Manage container-based execution for reproducibility</li> <li>Handle resource allocation, parallelization, and failure recovery</li> <li>NOT responsible for: Defining workflows, implementing algorithms, composing modules, or making scientific decisions</li> </ul>"},{"location":"architecturev2/05-execution/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Backend Abstraction (Decision #10): The execution functionality is provided through Pipecraft's backend system <code>pipecraft/src/pipecraft/backend/</code>, offering a uniform interface to multiple execution backends. Currently implemented backends include Snakemake (fully functional) and AWS Batch (partial implementation), with extensibility for others. This abstraction ensures that pipeline definitions remain portable and that infrastructure choices can evolve without impacting scientific workflows. The translation from Pipecraft graphs to backend-specific formats happens within Pipecraft's backend implementations.</p> </li> <li> <p>Container-Based Execution (Decision #6): All computational work executes within containers (Docker/Singularity/Apptainer) to ensure reproducibility across environments. This decision addresses the critical challenge of scientific reproducibility by guaranteeing that analyses produce identical results regardless of the host system. Container specifications are managed at the module level but orchestrated by the Execution Layer.</p> </li> </ol>"},{"location":"architecturev2/05-execution/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/05-execution/#inputs","title":"Inputs","text":"<ul> <li>Pipecraft compute graphs from the Pipeline Layer</li> <li>Execution configuration (backend selection, resource limits)</li> <li>Runtime parameters (parallelism settings, retry policies)</li> </ul>"},{"location":"architecturev2/05-execution/#outputs","title":"Outputs","text":"<ul> <li>Execution status and progress information</li> <li>Workflow artifacts as defined by pipeline outputs</li> <li>Execution logs and metrics for debugging</li> </ul>"},{"location":"architecturev2/05-execution/#dependencies","title":"Dependencies","text":"<ul> <li>Internal dependencies: Pipeline Layer for workflow definitions (following upward-only rule)</li> <li>External dependencies: Workflow orchestrators (e.g., Snakemake), container runtimes</li> </ul>"},{"location":"architecturev2/05-execution/#patterns","title":"Patterns","text":"<p>The Execution Layer follows a standard pattern for backend abstraction and pipeline execution:</p>"},{"location":"architecturev2/05-execution/#backend-pattern","title":"Backend Pattern","text":"<p>Each backend implementation <code>pipecraft/src/pipecraft/backend/</code> follows this structure:</p> <ul> <li>Inherits from a base Backend class that defines the interface</li> <li>Implements <code>compile()</code> to translate Pipecraft graphs to backend-specific formats</li> <li>Implements <code>run()</code> to execute the compiled workflow</li> <li>Returns a run object for monitoring and log access</li> </ul>"},{"location":"architecturev2/05-execution/#execution-flow-pattern","title":"Execution Flow Pattern","text":"<p>The Execution Layer follows a four-phase pattern for translating abstract pipelines into concrete executions:</p> <ol> <li> <p>Backend Selection: The system instantiates the appropriate backend based on configuration, with each backend providing a uniform interface despite different underlying orchestrators.</p> </li> <li> <p>Pipeline Compilation: Backends translate Pipecraft graphs into their native formats by iterating through pipeline nodes and generating backend-specific execution instructions.</p> </li> <li> <p>Execution Management: The backend configures the runtime environment, manages resource allocation, and launches workflows with appropriate parameters.</p> </li> <li> <p>Monitoring and Control: Standardized run objects provide interfaces for checking execution status, accessing logs, and controlling running workflows.</p> </li> </ol>"},{"location":"architecturev2/05-execution/#integration-pattern","title":"Integration Pattern","text":"<p>The Conductor service <code>conductor/src/conductor/handlers/execute.py</code> orchestrates execution:</p> <ol> <li>Creates data configuration from project/job settings</li> <li>Instantiates the appropriate module or pipeline</li> <li>Creates a backend instance with the compiled pipeline</li> <li>Executes and tracks the run in the database</li> <li>Provides status updates through the API</li> </ol>"},{"location":"architecturev2/05-execution/#configuration-pattern","title":"Configuration Pattern","text":"<p>Execution configuration typically includes:</p> <ul> <li>Resource limits (cores, memory, time)</li> <li>Container platform selection</li> <li>Retry and failure handling policies</li> <li>Output and scratch directory paths</li> <li>Backend-specific options</li> </ul> <p>This pattern-based approach ensures that new backends can be added without modifying existing code, and that execution details can evolve without breaking the abstraction.</p>"},{"location":"architecturev2/05-execution/#implementation-location","title":"Implementation Location","text":"<ul> <li>Primary location: <code>pipecraft/src/pipecraft/backend/</code></li> <li>Secondary location: <code>conductor/src/conductor/handlers/execute.py</code></li> <li>Tests: <code>pipecraft/tests/backend/</code></li> </ul>"},{"location":"architecturev2/05-execution/#see-also","title":"See Also","text":"<ul> <li>Previous: Pipeline Layer, Next: Configuration Layer</li> </ul>"},{"location":"architecturev2/06-configuration/","title":"Configuration Layer","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p>"},{"location":"architecturev2/06-configuration/#purpose","title":"Purpose","text":"<p>The Configuration Layer provides experiment-specific parameters and intelligent defaults across all StarryNight components, solving the problem of adapting generic image processing workflows to specific experimental contexts. Unlike other layers that follow a strict vertical hierarchy, the Configuration Layer operates orthogonally, influencing behavior at every level from algorithm parameters to execution resources. By combining automated parameter inference with explicit configuration options, this layer minimizes manual setup while maintaining full control when needed.</p>"},{"location":"architecturev2/06-configuration/#responsibilities","title":"Responsibilities","text":"<ul> <li>Infer processing parameters from experimental data and metadata</li> <li>Provide experiment-specific adaptations across all layers</li> <li>Generate module configurations based on data characteristics</li> <li>NOT responsible for: Implementing algorithms, executing workflows, managing infrastructure, or enforcing layer boundaries</li> </ul>"},{"location":"architecturev2/06-configuration/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li> <p>Cross-Cutting Concern (Decision #5): Configuration operates orthogonally to the layered architecture rather than as another sequential layer. This design recognizes that experimental parameters influence every aspect of processing\u2014from algorithm thresholds to resource allocation. The orthogonal nature enables consistent configuration management without violating layer independence or creating circular dependencies.</p> </li> <li> <p>Parameter Inference (Decision #11): The system automatically infers structural parameters from data wherever possible, reducing configuration burden while maintaining reproducibility. This inference currently focuses on data organization: extracting images per well, cycle counts, channel lists, and dataset identifiers from index files. While the architecture supports more sophisticated inference (like analyzing intensity distributions), current implementation provides basic structural inference with plans for enhancement. Users can always override inferred values when domain knowledge suggests better choices.</p> </li> </ol>"},{"location":"architecturev2/06-configuration/#interfaces","title":"Interfaces","text":""},{"location":"architecturev2/06-configuration/#inputs","title":"Inputs","text":"<ul> <li>Experimental metadata (plate layouts, channel configurations)</li> <li>Sample data for parameter inference</li> <li>User-provided configuration overrides</li> </ul>"},{"location":"architecturev2/06-configuration/#outputs","title":"Outputs","text":"<ul> <li>Algorithm-specific parameters for each processing step</li> <li>Module configurations with inferred settings</li> <li>Execution hints for resource allocation</li> </ul>"},{"location":"architecturev2/06-configuration/#dependencies","title":"Dependencies","text":"<ul> <li>No direct layer dependencies (cross-cutting nature)</li> <li>External dependencies: Configuration libraries (YAML/JSON parsing), data analysis tools for inference</li> </ul>"},{"location":"architecturev2/06-configuration/#patterns","title":"Patterns","text":"<p>The Configuration Layer follows consistent patterns across the codebase:</p>"},{"location":"architecturev2/06-configuration/#experiment-configuration-pattern","title":"Experiment Configuration Pattern","text":"<p>The system uses validated data models to define experiment types with configuration schemas that capture dataset metadata, experiment-specific parameters (images per well, cycle counts), channel mappings and acquisition details, and processing preferences with override capabilities.</p>"},{"location":"architecturev2/06-configuration/#module-configuration-pattern","title":"Module Configuration Pattern","text":"<p>Each module receives configuration through standardized interfaces: a <code>DataConfig</code> object containing paths to dataset, storage, and workspace; an <code>Experiment</code> object with experiment-specific parameters; and module-specific spec containers for algorithm parameters.</p>"},{"location":"architecturev2/06-configuration/#parameter-inference-pattern","title":"Parameter Inference Pattern","text":"<p>The system implements parameter inference through index analysis (reading experimental metadata), data sampling (analyzing representative samples), smart defaults (based on experiment type), and an override mechanism for explicit user control.</p>"},{"location":"architecturev2/06-configuration/#resource-estimation-pattern","title":"Resource Estimation Pattern","text":"<p>Processing requirements are dynamically estimated based on data volume (images, plates, wells), processing complexity, historical performance metrics, and module-specific scaling factors. This enables appropriate resource allocation without manual tuning.</p>"},{"location":"architecturev2/06-configuration/#implementation-location","title":"Implementation Location","text":"<ul> <li>Primary location: <code>starrynight/src/starrynight/experiments/</code></li> <li>Configuration handling: <code>starrynight/src/starrynight/modules/common.py</code></li> <li>Tests: <code>starrynight/tests/experiments/</code></li> </ul>"},{"location":"architecturev2/06-configuration/#see-also","title":"See Also","text":"<ul> <li>Previous: Execution Layer</li> <li>Note: Configuration is a cross-cutting concern that influences all layers</li> </ul>"},{"location":"architecturev2/99-0-examples/","title":"StarryNight Architecture: End-to-End Flow Examples","text":"<p>Experimental - Not Reviewed</p> <p>Content may be incomplete or inaccurate.</p> <p>This document demonstrates how StarryNight's six architectural layers work together to accomplish real-world scientific workflows. Each example traces the complete path from user command to final results, showing how data and control flow through the system.</p> <p>Note: Code examples in this document illustrate patterns and concepts rather than exact implementations. Refer to the actual source files for current signatures and parameters.</p>"},{"location":"architecturev2/99-0-examples/#example-1-illumination-correction-workflow","title":"Example 1: Illumination Correction Workflow","text":"<p>This example shows how a researcher corrects illumination artifacts in microscopy images\u2014a common preprocessing step required before analysis.</p>"},{"location":"architecturev2/99-0-examples/#user-command","title":"User Command","text":"<pre><code>starrynight illum calc loaddata [options]\n</code></pre> <p>This command prepares illumination correction data for a specific plate from an experiment index.</p>"},{"location":"architecturev2/99-0-examples/#flow-through-layers","title":"Flow Through Layers","text":""},{"location":"architecturev2/99-0-examples/#1-cli-layer-entry-point","title":"1. CLI Layer Entry Point","text":"<p>File: <code>starrynight/src/starrynight/cli/illum.py</code></p> <p>The CLI layer receives the command and parses arguments. The pattern follows:</p> <ul> <li>Click decorators define command structure and options</li> <li>Arguments are converted to appropriate types (e.g., paths to AnyPath objects)</li> <li>Direct delegation to algorithm functions for business logic</li> </ul>"},{"location":"architecturev2/99-0-examples/#2-algorithm-layer-processing","title":"2. Algorithm Layer Processing","text":"<p>File: <code>starrynight/src/starrynight/algorithms/illum_calc.py</code></p> <p>The algorithm function performs the actual computation following this pattern:</p> <ul> <li>Read input data from specified paths</li> <li>Filter and transform data based on experiment parameters</li> <li>Utilize shared utilities for consistent data formatting</li> <li>Write results to specified output paths</li> </ul>"},{"location":"architecturev2/99-0-examples/#3-module-layer-encapsulation","title":"3. Module Layer Encapsulation","text":"<p>File: <code>starrynight/src/starrynight/modules/cp_illum_calc/</code> (module implementation)</p> <p>The module layer wraps CLI functionality for pipeline use:</p> <ul> <li>Extends <code>StarrynightModule</code> base class (from <code>modules/common.py</code>)</li> <li>Defines container-based execution with appropriate image</li> <li>Maps pipeline parameters to CLI command arguments</li> <li>Provides specification for inputs and outputs</li> </ul>"},{"location":"architecturev2/99-0-examples/#4-pipeline-layer-composition","title":"4. Pipeline Layer Composition","text":"<p>File: <code>starrynight/src/starrynight/pipelines/pcp_generic.py</code></p> <p>The pipeline layer assembles modules into workflows using:</p> <ul> <li>Sequential (<code>Seq</code>) containers for dependent steps</li> <li>Parallel (<code>Parallel</code>) containers for independent operations</li> <li>Module references through a registry pattern (<code>modules/registry.py</code>)</li> <li>Named workflows for clarity and debugging</li> </ul>"},{"location":"architecturev2/99-0-examples/#5-execution-layer-runtime","title":"5. Execution Layer Runtime","text":"<p>Backend: Pipecraft with Snakemake (<code>pipecraft/src/pipecraft/backend/snakemake.py</code>)</p> <p>Execution patterns include:</p> <ul> <li>Backend configuration with resource specifications</li> <li>Container runtime selection (Docker/Singularity)</li> <li>Work directory management</li> <li>Asynchronous execution with status monitoring</li> </ul>"},{"location":"architecturev2/99-0-examples/#6-configuration-layer-cross-cutting","title":"6. Configuration Layer Cross-Cutting","text":"<p>Pattern: Configuration flows orthogonally through all layers</p> <p>Configuration provides:</p> <ul> <li>Experiment metadata (plates, channels, wells)</li> <li>Resource requirements (CPU, memory)</li> <li>Runtime parameters (paths, identifiers)</li> <li>Protocol-specific settings</li> </ul>"},{"location":"architecturev2/99-0-examples/#result","title":"Result","text":"<p>The workflow produces:</p> <ul> <li>LoadData CSV files for CellProfiler</li> <li>Illumination correction functions for each channel</li> <li>Corrected images ready for analysis</li> </ul>"},{"location":"architecturev2/99-0-examples/#example-2-complete-cell-painting-pipeline","title":"Example 2: Complete Cell Painting Pipeline","text":"<p>This example demonstrates a full Cell Painting analysis from raw images to extracted features.</p>"},{"location":"architecturev2/99-0-examples/#pipeline-initialization","title":"Pipeline Initialization","text":"<p>File: <code>starrynight/notebooks/pypct/exec_pcp_generic_pipe.py</code></p> <p>The initialization pattern involves:</p> <ul> <li>Creating data configuration with dataset and workspace paths</li> <li>Loading experiment configuration from index files</li> <li>Optional metadata enrichment (e.g., barcode mappings)</li> </ul>"},{"location":"architecturev2/99-0-examples/#modular-pipeline-execution","title":"Modular Pipeline Execution","text":""},{"location":"architecturev2/99-0-examples/#step-1-generate-index","title":"Step 1: Generate Index","text":"<p>Module: <code>CPGenIndexModule</code> (from <code>modules/gen_index.py</code>)</p> <p>The pattern for module parameterization:</p> <ul> <li>Instantiate module class</li> <li>Define parameter dictionary with required values</li> <li>Apply parameters to create configured module instance</li> </ul>"},{"location":"architecturev2/99-0-examples/#step-2-parallel-preprocessing","title":"Step 2: Parallel Preprocessing","text":"<p>The pipeline uses parallel execution for independent workflows:</p> <p>Pattern: Parallel container with named sequences</p> <ul> <li>Cell Painting illumination correction sequence</li> <li>SBS (Sequencing by Synthesis) illumination correction sequence</li> <li>Each sequence contains ordered preprocessing steps</li> <li>Parallel execution maximizes resource utilization</li> </ul>"},{"location":"architecturev2/99-0-examples/#step-3-sequential-analysis","title":"Step 3: Sequential Analysis","text":"<p>After preprocessing completes, analysis runs sequentially:</p> <p>Pattern: Sequential execution for dependent operations</p> <ul> <li>Generate analysis-specific LoadData files</li> <li>Create CellProfiler pipeline configuration</li> <li>Invoke CellProfiler for feature extraction</li> </ul>"},{"location":"architecturev2/99-0-examples/#configuration-adaptation","title":"Configuration Adaptation","text":"<p>The configuration layer adapts parameters for each experiment type:</p> <p>Cell Painting Configuration:</p> <ul> <li>Maps biological targets to fluorescent channels</li> <li>Defines organelle-specific imaging parameters</li> <li>Supports multiple wavelengths per target</li> </ul> <p>SBS Configuration:</p> <ul> <li>Maps sequencing cycles to detection channels</li> <li>Defines base-specific fluorophores</li> <li>Supports multi-cycle imaging protocols</li> </ul>"},{"location":"architecturev2/99-0-examples/#execution-backend-translation","title":"Execution Backend Translation","text":"<p>The backend translates abstract pipelines into executable workflows:</p> <p>Translation patterns:</p> <ul> <li>Pipeline containers become workflow rules</li> <li>Module specifications define inputs/outputs</li> <li>Resource requirements map to cluster constraints</li> <li>Container definitions ensure reproducibility</li> </ul>"},{"location":"architecturev2/99-0-examples/#example-3-custom-algorithm-integration","title":"Example 3: Custom Algorithm Integration","text":"<p>This example shows how to add a new algorithm to StarryNight.</p>"},{"location":"architecturev2/99-0-examples/#step-1-implement-algorithm-function","title":"Step 1: Implement Algorithm Function","text":"<p>Location: <code>starrynight/src/starrynight/algorithms/</code></p> <p>Algorithm implementation patterns:</p> <ul> <li>Accept <code>AnyPath</code> objects for file system abstraction</li> <li>Separate parameter generation from execution</li> <li>Use type hints for clarity</li> <li>Leverage utilities for common operations (<code>utils/</code>)</li> <li>Write outputs to specified paths</li> </ul>"},{"location":"architecturev2/99-0-examples/#step-2-create-cli-wrapper","title":"Step 2: Create CLI Wrapper","text":"<p>Location: <code>starrynight/src/starrynight/cli/</code></p> <p>CLI patterns:</p> <ul> <li>Use Click groups for related commands (see <code>cli/main.py</code>)</li> <li>Define clear option names with short forms</li> <li>Convert string paths to <code>AnyPath</code> objects</li> <li>Delegate to algorithm functions</li> <li>Keep CLI logic minimal</li> </ul>"},{"location":"architecturev2/99-0-examples/#step-3-create-module","title":"Step 3: Create Module","text":"<p>Location: <code>starrynight/src/starrynight/modules/</code></p> <p>Module patterns:</p> <ul> <li>Extend <code>StarrynightModule</code> base class (from <code>modules/common.py</code>)</li> <li>Define input/output specifications</li> <li>Create pipeline with container sequences (using pipecraft)</li> <li>Reference CLI commands in containers</li> <li>Use parameter substitution with <code>$</code> prefix</li> </ul>"},{"location":"architecturev2/99-0-examples/#step-4-integration-in-pipeline","title":"Step 4: Integration in Pipeline","text":"<p>Location: <code>starrynight/src/starrynight/pipelines/</code></p> <p>Integration patterns:</p> <ul> <li>Insert new modules into existing workflows</li> <li>Maintain clear sequential or parallel relationships</li> <li>Use descriptive names for pipeline sections</li> <li>Consider data dependencies between modules</li> <li>Register in pipeline registry (<code>pipelines/registry.py</code>)</li> </ul>"},{"location":"architecturev2/99-0-examples/#key-architectural-patterns","title":"Key Architectural Patterns","text":"<p>These examples demonstrate several key patterns:</p> <ol> <li>Layer Independence: Each layer has clear responsibilities and interfaces</li> <li>Bottom-Up Data Flow: Algorithms \u2192 CLI \u2192 Modules \u2192 Pipelines \u2192 Execution</li> <li>Cross-Cutting Configuration: Parameters flow orthogonally through all layers</li> <li>Container Isolation: All computation happens in reproducible containers</li> <li>Backend Abstraction: Same pipeline runs on different infrastructure</li> </ol>"},{"location":"architecturev2/99-0-examples/#summary","title":"Summary","text":"<p>StarryNight's architecture enables:</p> <ul> <li>Modularity: Add new algorithms without changing the framework</li> <li>Reusability: Compose existing modules into new pipelines</li> <li>Portability: Run anywhere from laptops to cloud clusters</li> <li>Reproducibility: Container-based execution ensures consistent results</li> <li>Flexibility: Adapt to different experimental protocols through configuration</li> </ul> <p>The layered design separates concerns while maintaining cohesion, allowing researchers to focus on science while the framework handles infrastructure complexity.</p>"},{"location":"developer/assets/pcpip-notebooks/","title":"PCPIP Notebooks","text":"<p>This directory contains Jupyter notebooks and Python scripts used to process, analyze, and visualize data from the PCPIP (Pooled Cell Painting Image-based Profiling) pipeline.</p>"},{"location":"developer/assets/pcpip-notebooks/#available-notebooksscripts","title":"Available Notebooks/Scripts","text":"<ul> <li><code>6_Barcode_Align.py</code> - Alignment utility for barcoding images</li> <li><code>7_BarcodePreprocessing.py</code> - Preprocessing script for barcode data</li> <li><code>Visualize_stitched_images.py</code> - Visualization tool for stitched microscopy images</li> <li><code>make_fiji_montages_std.py</code> - Script to create standard montages for FIJI</li> </ul>"},{"location":"developer/assets/pcpip-notebooks/#usage","title":"Usage","text":"<p>These notebooks/scripts are referenced in the PCPIP Specifications document.</p>"},{"location":"developer/guides/system-maintenance-guide/","title":"System Maintenance Guide","text":"<p>This guide provides comprehensive instructions for maintaining and extending the StarryNight system.</p>"},{"location":"developer/guides/system-maintenance-guide/#system-dependencies-and-requirements","title":"System Dependencies and Requirements","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#maintenance-procedures","title":"Maintenance Procedures","text":""},{"location":"developer/guides/system-maintenance-guide/#updating-components","title":"Updating Components","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#testing-procedures","title":"Testing Procedures","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#deployment-procedures","title":"Deployment Procedures","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#system-health-verification","title":"System Health Verification","text":"<p>To be completed</p>"},{"location":"developer/guides/system-maintenance-guide/#references","title":"References","text":"<p>To be completed</p>"},{"location":"developer/guides/workflow-implementation-guide/","title":"Workflow Implementation Guide","text":"<p>This guide provides step-by-step instructions for implementing new workflows in StarryNight.</p>"},{"location":"developer/guides/workflow-implementation-guide/#what-is-a-workflow","title":"What is a Workflow?","text":"<p>In the context of StarryNight, a workflow refers to a specific scientific image processing sequence implemented using StarryNight modules. Workflows are flexible and can be implemented in different ways:</p> <ol> <li> <p>Step-by-step implementation: As seen in <code>exec_pcp_generic_pipe.py</code>, where modules are individually created and executed in sequence, allowing for inspection of intermediate results.</p> </li> <li> <p>Pipeline composition: As seen in <code>exec_pcp_generic_full.py</code>, where modules are connected into a StarryNight pipeline that can be executed as a single unit, potentially with parallel operations.</p> </li> </ol> <p>Both approaches are valid ways to implement workflows, with the pipeline composition approach being more concise and efficient for production use, while the step-by-step approach provides more visibility and control for development and debugging.</p>"},{"location":"developer/guides/workflow-implementation-guide/#implementation-as-creation-and-modification","title":"Implementation as Creation and Modification","text":"<p>In practice, implementing a workflow in StarryNight rarely means starting from scratch. Most researchers begin with an existing workflow (like those in our reference implementations) and modify it to suit their specific needs. This guide addresses both aspects of implementation:</p> <ul> <li>Modifying existing workflows: Making changes to established workflows, from simple parameter adjustments to significant restructuring</li> <li>Creating new workflows: Building workflows by combining existing modules in new ways or developing entirely new modules</li> </ul> <p>The skills for both overlap significantly, and many researchers progress from making small modifications to creating entirely custom workflows as their expertise grows. Throughout this guide, we'll address both scenarios, recognizing that implementation typically involves elements of both creation and modification.</p>"},{"location":"developer/guides/workflow-implementation-guide/#understanding-workflow-flexibility","title":"Understanding Workflow Flexibility","text":"<p>StarryNight is designed with flexibility in mind, allowing researchers to create and modify workflows at different levels of complexity. Depending on your needs, you can:</p>"},{"location":"developer/guides/workflow-implementation-guide/#simple-changes-no-code-required","title":"Simple Changes (No Code Required)","text":"<p>These changes involve internal modifications to CellProfiler pipelines that are wrapped by StarryNight modules, without requiring any Python code changes:</p> <ul> <li>Change CellProfiler Modules: Replace one CellProfiler module with another inside a pipeline file (e.g., switching from <code>IdentifyPrimaryObject</code> to <code>RunCellPose</code> for segmentation)</li> <li>Adjust Module Parameters: Modify thresholds, filters, or measurement settings within the CellProfiler pipeline file</li> </ul>"},{"location":"developer/guides/workflow-implementation-guide/#moderate-changes-python-workflow-code","title":"Moderate Changes (Python Workflow Code)","text":"<p>These changes involve modifying the Python workflow code to adjust how modules are configured and connected:</p> <ul> <li>Experiment Configuration Updates: Modify channel mappings, experiment parameters, and module inputs in the workflow Python script</li> <li>Module Input/Output Connections: Change how modules' outputs are connected to other modules' inputs in the workflow code</li> <li>Pipeline Composition: Alter the order and arrangement of modules in the pipeline definition</li> </ul>"},{"location":"developer/guides/workflow-implementation-guide/#advanced-changes-code-required","title":"Advanced Changes (Code Required)","text":"<p>Advanced changes can be divided into two levels of complexity:</p>"},{"location":"developer/guides/workflow-implementation-guide/#advanced-level-1-module-development","title":"Advanced Level 1 (Module Development)","text":"<p>This guide covers these changes, which involve extending the system while working within the existing architecture:</p> <ul> <li>Add New Modules: Create entirely new modules for custom processing steps</li> <li>Implement Custom Resource Configurations: Specify compute resources at the task level for performance optimization</li> <li>Integrate External Tools: Connect new external processing tools while maintaining the module interface pattern</li> </ul>"},{"location":"developer/guides/workflow-implementation-guide/#advanced-level-2-core-architecture-development","title":"Advanced Level 2 (Core Architecture Development)","text":"<p>These changes are beyond the scope of this workflow implementation guide, as they involve modifying the core architecture itself:</p> <ul> <li>Modify Execution Engine: Change how StarryNight executes workflows</li> <li>Implement New Backends: Add support for new execution environments</li> <li>Create New Pipeline Patterns: Develop new ways to compose and orchestrate pipelines</li> <li>Develop Algorithm Libraries: Build new algorithm foundations that integrate with the core system</li> </ul>"},{"location":"developer/guides/workflow-implementation-guide/#reference-implementations","title":"Reference Implementations","text":"<p>Throughout this guide, we'll reference two complementary practical implementations that demonstrate different approaches to workflow creation:</p> <ol> <li> <p>Step-by-step approach: <code>starrynight/notebooks/pypct/exec_pcp_generic_pipe.py</code> - Implements each module individually with explicit execution of each step, providing maximum visibility and control</p> </li> <li> <p>Pipeline composition approach: <code>starrynight/notebooks/pypct/exec_pcp_generic_full.py</code> - Implements the same workflow using the pipeline composition pattern for a more concise implementation</p> </li> </ol> <p>Both implementations are explained in detail in the Practical Integration architecture document.</p> <p>The step-by-step implementation demonstrates the complete PCP Generic pipeline workflow with the following steps:</p> <ol> <li>Generate inventory and index</li> <li>Calculate illumination correction (CP)</li> <li>Apply illumination and align (CP)</li> <li>Segmentation check (CP)</li> <li>Calculate illumination correction (SBS)</li> <li>Apply illumination and align (SBS)</li> <li>Preprocess (SBS)</li> <li>Analysis</li> </ol> <p>Each step follows a consistent three-phase pattern for CellProfiler integration:</p> <ul> <li>Generate load data (configuration data for CellProfiler)</li> <li>Generate pipeline file (CellProfiler pipeline definition)</li> <li>Execute the pipeline (running CellProfiler)</li> </ul> <p>Rather than duplicating all the code examples here, we'll focus on explaining the key concepts and scenarios for implementing and modifying workflows, with references to the existing examples where appropriate.</p>"},{"location":"developer/guides/workflow-implementation-guide/#key-workflow-components","title":"Key Workflow Components","text":"<p>Workflows in StarryNight are built from several key components that work together:</p>"},{"location":"developer/guides/workflow-implementation-guide/#data-configuration","title":"Data Configuration","text":"<p>The <code>DataConfig</code> object defines paths for input data, storage, and workspace. It configures where the pipeline will look for input data and where it will store outputs and intermediate files. This configuration is used throughout the pipeline.</p>"},{"location":"developer/guides/workflow-implementation-guide/#execution-backend","title":"Execution Backend","text":"<p>The execution backend (typically <code>SnakeMakeBackend</code>) handles running the pipeline. It translates module definitions into executable operations and manages their execution, including scheduling, parallelization, and resource allocation.</p>"},{"location":"developer/guides/workflow-implementation-guide/#experiment-configuration","title":"Experiment Configuration","text":"<p>The experiment configuration contains metadata about your dataset and processing settings, including:</p> <ul> <li>Acquisition settings (order, frame type, overlap percentage)</li> <li>Channel mappings (which microscope channels correspond to nucleus, cell, and mitochondria)</li> <li>Other experiment-specific parameters</li> </ul> <p>This configuration drives all subsequent module behavior without requiring repetitive parameter specification.</p>"},{"location":"developer/guides/workflow-implementation-guide/#modules","title":"Modules","text":"<p>Modules are the building blocks of workflows. Each module:</p> <ul> <li>Is configured via its <code>from_config()</code> method</li> <li>Produces a \"pipe\" that's executed by the backend</li> <li>Has defined inputs and outputs</li> <li>May require outputs from previous modules</li> <li>Is executed in a containerized environment</li> </ul>"},{"location":"developer/guides/workflow-implementation-guide/#choosing-an-implementation-approach","title":"Choosing an Implementation Approach","text":"<p>Both implementation approaches can create the same workflows, but they have different strengths that make them suitable for different scenarios:</p>"},{"location":"developer/guides/workflow-implementation-guide/#step-by-step-approach","title":"Step-by-Step Approach","text":"<p>Best for development, debugging, and learning:</p> <ul> <li>Provides maximum visibility into intermediate results</li> <li>Allows inspection of outputs between steps</li> <li>Simplifies debugging by isolating failures to specific modules</li> <li>Makes the data flow more explicit</li> </ul> <p>Example use case: When implementing a new workflow for the first time or troubleshooting issues with specific modules</p>"},{"location":"developer/guides/workflow-implementation-guide/#pipeline-composition-approach","title":"Pipeline Composition Approach","text":"<p>Best for production use and complex execution patterns:</p> <ul> <li>More concise implementation with less boilerplate code</li> <li>Enables parallel execution of independent operations</li> <li>Provides a clearer view of the entire workflow structure</li> <li>Better represents dependencies between modules</li> </ul> <p>Example use case: When implementing a proven workflow for production use or when optimizing performance with parallel execution</p>"},{"location":"developer/guides/workflow-implementation-guide/#common-implementation-scenarios","title":"Common Implementation Scenarios","text":"<p>The following scenarios apply to both modifying existing workflows and creating new ones. Whether you're adapting a reference implementation or building something new, these patterns will help you implement workflows effectively:</p>"},{"location":"developer/guides/workflow-implementation-guide/#scenario-1-simple-changes-cellprofiler-module-settings","title":"Scenario 1: Simple Changes - CellProfiler Module Settings","text":"<p>As described in the \"Simple Changes\" section, you can modify CellProfiler pipelines without changing Python code. When working in this scenario:</p> <ol> <li>Directly edit the CellProfiler pipeline (.cppipe) files</li> <li>No Python workflow code changes are needed</li> </ol> <p>For example, you can change module parameters or replace modules (such as switching from <code>IdentifyPrimaryObject</code> to <code>RunCellPose</code>) directly in the pipeline file. These changes will automatically be used the next time the workflow executes with no additional code changes required.</p>"},{"location":"developer/guides/workflow-implementation-guide/#scenario-2-moderate-changes-experiment-configuration","title":"Scenario 2: Moderate Changes - Experiment Configuration","text":"<p>As covered in the \"Moderate Changes\" section, this requires modifying the Python workflow code. For example, configuring different channels requires updating the experiment configuration in the <code>PCPGenericInitConfig</code> object:</p> <pre><code># Original experiment configuration with default channel mappings\npcp_exp_init = PCPGenericInitConfig(\n    barcode_csv_path=barcode_csv_path,\n    cp_acquisition_order=AcquisitionOrderType.SNAKE,\n    cp_img_frame_type=ImageFrameType.ROUND,\n    cp_img_overlap_pct=10,\n    # Default channel mappings\n    cp_nuclei_channel=\"DAPI\",\n    cp_cell_channel=\"PhalloAF750\",\n    cp_mito_channel=\"ZO1AF488\",\n    # SBS settings omitted for brevity\n)\n\n# Modified experiment configuration with different channel mappings\npcp_exp_init = PCPGenericInitConfig(\n    barcode_csv_path=barcode_csv_path,\n    cp_acquisition_order=AcquisitionOrderType.SNAKE,\n    cp_img_frame_type=ImageFrameType.ROUND,\n    cp_img_overlap_pct=10,\n    # Updated channel mappings to match your dataset\n    cp_nuclei_channel=\"Hoechst\",    # Changed from DAPI\n    cp_cell_channel=\"CellMask\",     # Changed from PhalloAF750\n    cp_mito_channel=\"MitoTracker\",  # Changed from ZO1AF488\n    # SBS settings omitted for brevity\n)\n\n# Initialize experiment with updated channel mappings\npcp_experiment = PCPGeneric.from_index(index_path, pcp_exp_init.model_dump())\n</code></pre> <p>After making these changes:</p> <ol> <li>The LoadData module will automatically handle the new channel mappings when generating the LoadData CSV</li> <li>When using automatically generated pipelines, they will adapt to include the channels with the new names</li> <li>If using legacy pipelines, ensure they are designed to handle the differently named channels</li> </ol> <p>The system is designed to flexibly adapt to channel configuration changes, and these modifications are isolated to the experiment configuration while affecting the entire workflow.</p> <p>Future Enhancement</p> <p>This section will be updated when enhanced channel mapping capabilities are implemented. Currently, only specific channel mappings (nuclei, cell, mito) are configurable, but future versions will support more flexible mapping of any channel type.</p>"},{"location":"developer/guides/workflow-implementation-guide/#scenario-3-moderate-changes-module-arrangement","title":"Scenario 3: Moderate Changes - Module Arrangement","text":"<p>Another moderate change involves modifying the arrangement of modules in the Python workflow code. Here we'll focus on the step-by-step implementation approach, while noting how these changes would translate to the pipeline composition approach.</p>"},{"location":"developer/guides/workflow-implementation-guide/#step-by-step-implementation-approach","title":"Step-by-Step Implementation Approach","text":"<ol> <li> <p>Adding a module:</p> <ul> <li>Import the new module class</li> <li>Create an instance using its <code>from_config()</code> method, connecting to outputs from preceding modules     <pre><code>from starrynight.modules.new_module import NewModule\n\n# Connect the new module to outputs from an existing module\nnew_module = NewModule.from_config(\n    data_config,\n    experiment,\n    input_path=existing_module.outputs[\"output_path\"]\n)\n</code></pre></li> <li>Set up execution with <code>SnakeMakeBackend(module.pipe, backend_config, run_dir, mounts)</code></li> <li>Run the module with <code>run = exec_backend.run()</code> and <code>run.wait()</code></li> </ul> </li> <li> <p>Removing a module:</p> <ul> <li>Remove the module import statement (if no longer needed)</li> <li>Remove the module creation code (<code>module = ModuleClass.from_config(...)</code>)</li> <li>Remove the execution code (<code>exec_backend = SnakeMakeBackend(...)</code>, <code>run = exec_backend.run()</code>, etc.)</li> <li>Update any dependent modules that used this module's outputs:     <pre><code># Before removal - module_b depends on module_a's output\nmodule_b = ModuleB.from_config(data_config, experiment, input_path=module_a.outputs[\"output_path\"])\n\n# After removal - module_b needs an alternative input source\nmodule_b = ModuleB.from_config(data_config, experiment, input_path=alternative_input_path)\n</code></pre></li> </ul> </li> <li> <p>Reordering modules:</p> <ul> <li>Move the entire module creation and execution block to the desired position in your code</li> <li>Ensure data dependencies are respected (modules must be executed after any modules they depend on)</li> <li>Update input/output connections if necessary:     <pre><code># Original order: A \u2192 B \u2192 C\nmodule_a = ModuleA.from_config(data_config, experiment)\n# Execute module_a...\n\nmodule_b = ModuleB.from_config(data_config, experiment, input=module_a.outputs[\"output\"])\n# Execute module_b...\n\nmodule_c = ModuleC.from_config(data_config, experiment, input=module_b.outputs[\"output\"])\n# Execute module_c...\n\n# Reordered to: A \u2192 C \u2192 B (assuming C doesn't depend on B)\nmodule_a = ModuleA.from_config(data_config, experiment)\n# Execute module_a...\n\nmodule_c = ModuleC.from_config(data_config, experiment, input=module_a.outputs[\"output\"])\n# Execute module_c...\n\nmodule_b = ModuleB.from_config(data_config, experiment, input=module_a.outputs[\"output\"])\n# Execute module_b...\n</code></pre></li> </ul> </li> </ol>"},{"location":"developer/guides/workflow-implementation-guide/#how-this-translates-to-pipeline-composition","title":"How This Translates to Pipeline Composition","text":"<p>In the pipeline composition approach, these same changes would be implemented differently. Here's a simplified example based on the pipeline composition approach used in <code>exec_pcp_generic_full.py</code>:</p> <pre><code># Pipeline composition showing module arrangement\ndef create_custom_pipeline(data_config, experiment):\n    # Define all modules in one place\n    module_list = [\n        # Create modules with connections between them\n        module_a := ModuleA.from_config(data_config, experiment),\n        module_b := ModuleB.from_config(data_config, experiment),\n        module_c := ModuleC.from_config(data_config, experiment,\n                                       input_path=module_a.outputs[\"output_path\"]),\n        # Add a new module connected to existing ones\n        module_d := NewModule.from_config(data_config, experiment,\n                                         input_path=module_b.outputs[\"output_path\"])\n    ]\n\n    # Define execution pattern (sequential and parallel components)\n    return module_list, Seq([\n        # Sequential execution of A and B\n        Seq([module_a.pipe, module_b.pipe]),\n        # Then parallel execution of C and D (both depend on previous modules)\n        Parallel([module_c.pipe, module_d.pipe])\n    ])\n</code></pre> <p>Changes in the pipeline composition approach involve:</p> <ul> <li>Adding or removing modules: Update both the module list and the pipeline structure that defines execution flow</li> <li>Reordering modules: Change the arrangement in the <code>Seq</code> or <code>Parallel</code> composition structures</li> <li>Module connections: Still configured through the <code>from_config()</code> method parameters, but execution order is defined by the pipeline structure</li> </ul> <p>The step-by-step reference implementation demonstrates these patterns throughout, with each module being configured based on the outputs of previous modules, creating a connected workflow.</p>"},{"location":"developer/guides/workflow-implementation-guide/#advanced-implementation-creating-new-modules","title":"Advanced Implementation: Creating New Modules","text":"<p>Work in Progress</p> <p>The detailed documentation for creating new modules is under development and will be added in a future update. This section will cover Advanced Level 1 changes including:</p> <ul> <li>Creating module classes that follow the StarryNight patterns</li> <li>Defining module specifications with proper inputs and outputs</li> <li>Implementing the required methods like <code>uid()</code> and <code>from_config()</code></li> <li>Registering modules in the module registry</li> <li>Best practices for module development and testing</li> </ul> <p>In the meantime, refer to the Practical Integration document for some guidance on module structure, and examine existing modules in the codebase as reference implementations.</p>"},{"location":"developer/guides/workflow-implementation-guide/#advanced-implementation-resource-configuration","title":"Advanced Implementation: Resource Configuration","text":"<p>Work in Progress</p> <p>The detailed documentation for resource configuration is under development and will be added in a future update. This feature provides ways to specify compute resources (CPU cores, memory, GPU) at the module level, allowing for more efficient execution of resource-intensive tasks.</p> <p>This section will be expanded when the resource configuration API is finalized.</p>"},{"location":"developer/guides/workflow-implementation-guide/#references","title":"References","text":"<ul> <li>StarryNight Architecture: For more detailed information on the layers and their interactions, see the Architecture Overview</li> <li>Practical Integration: The Practical Integration document explains the <code>exec_pcp_generic_pipe.py</code> implementation in detail</li> <li>Architecture Flow: The Architecture Flow in Action document shows how data flows through all architectural layers</li> <li>Reference Implementations: Two complementary implementations:<ul> <li><code>starrynight/notebooks/pypct/exec_pcp_generic_pipe.py</code>: Step-by-step approach with individual module execution</li> <li><code>starrynight/notebooks/pypct/exec_pcp_generic_full.py</code>: Pipeline composition approach for concise implementation</li> </ul> </li> <li>Module Registry: Central registry for modules in <code>starrynight/modules/registry.py</code></li> <li>Pipeline Composition: Functions for creating complete pipelines in <code>starrynight/pipelines/</code></li> </ul> <p>For Document Contributors</p> <p>This section contains editorial guidelines for maintaining this document. These guidelines are intended for contributors and maintainers, not end users.</p>"},{"location":"developer/guides/workflow-implementation-guide/#guiding-structure-principles","title":"Guiding Structure Principles","text":"<ol> <li>Define the concept first - Begin each section by clearly defining what the concept means in the StarryNight context, as done with \"What is a Workflow?\"</li> <li>Dual perspective approach - Present both \"creating new workflows\" and \"modifying existing workflows\" perspectives, recognizing most users start by modifying existing workflows</li> <li>Progressive complexity levels - Organize content in increasing complexity levels:<ul> <li>Simple changes (no code required)</li> <li>Moderate changes (Python workflow code)</li> <li>Advanced changes (code required)</li> </ul> </li> <li>Explicit reference implementations - Consistently reference the two implementation patterns:<ul> <li>Step-by-step approach (<code>exec_pcp_generic_pipe.py</code>)</li> <li>Pipeline composition approach (<code>exec_pcp_generic_full.py</code>)</li> </ul> </li> <li>Scenario-based examples - Structure examples as concrete scenarios that users will encounter</li> </ol>"},{"location":"developer/guides/workflow-implementation-guide/#content-style-principles","title":"Content Style Principles","text":"<ol> <li>Code example patterns - Include both original and modified code with explanatory comments (see channel mapping example)</li> <li>Contrast implementation approaches - When showing how to accomplish a task, explain both approaches:<ul> <li>Step-by-step implementation first (more explicit)</li> <li>Pipeline composition second (more concise)</li> </ul> </li> <li>Context before code - Provide contextual explanation before code examples</li> <li>Mark future work consistently - Use <code>!!! note \"Work in Progress\"</code> or <code>!!! note \"Future Enhancement\"</code> admonitions</li> <li>Show non-obvious consequences - Explain the downstream effects of changes (e.g., how channel mapping affects LoadData generation)</li> </ol>"},{"location":"developer/guides/workflow-implementation-guide/#terminology-consistency","title":"Terminology Consistency","text":"<ul> <li>\"Workflow\" - A specific scientific image processing sequence implemented using StarryNight modules</li> <li>\"Module\" - Building blocks with defined inputs/outputs that are executed in a containerized environment</li> <li>\"Pipeline\" - Connected arrangement of modules that can be executed as a unit</li> <li>\"Step-by-step implementation\" - Creating and executing modules individually</li> <li>\"Pipeline composition\" - Defining the entire pipeline structure with sequential and parallel components</li> </ul>"},{"location":"developer/legacy/pcpip-specs/","title":"PCPIP: CellProfiler Pipelines and AWS Lambda Orchestration for Pooled Cell Painting","text":""},{"location":"developer/legacy/pcpip-specs/#background-and-relationship-to-starrynight","title":"Background and Relationship to StarryNight","text":"<p>Before StarryNight existed, the Broad Institute Imaging Platform iteratively developed a workflow using CellProfiler, ImageJ, and AWS Lambda functions to process optical pooled screening data, most notably as published in our collaboration to process three genome-scale CRISPR screens (Ramezani et al.,2023)</p> <p>PCPIP (Pooled Cell Painting Image Processing) represents this legacy system whose functionality and features served as the foundation for developing StarryNight. While PCPIP successfully processed large-scale experiments, it had limitations in modularity, maintainability, and extensibility that StarryNight was designed to address.</p> <p>This documentation is preserved to:</p> <ul> <li>Document the proven workflow patterns that inform StarryNight's design</li> <li>Provide reference for researchers familiar with the original PCPIP system</li> <li>Support ongoing maintenance of legacy experiments processed with PCPIP</li> </ul> <p>For the modern StarryNight system architecture, see the Developer Architecture documentation.</p>"},{"location":"developer/legacy/pcpip-specs/#system-overview","title":"System Overview","text":"<p>PCPIP is a specialized image processing system that automates the analysis of large-scale optical pooled screening experiments. The system integrates CellProfiler's image analysis capabilities with AWS Lambda-based orchestration to process thousands of microscopy images through a series of coordinated pipelines. PCPIP handles two parallel processing tracks - one for Cell Painting images that capture cellular morphology and another for barcoding images that identify genetic perturbations - before combining them for comprehensive phenotypic analysis. This document details the complete implementation of this workflow, including pipeline specifications, cloud infrastructure configuration, and execution patterns that enable reproducible, scalable image processing for high-content screening.</p> <p>See Requirements for a specification of the new system we want to build, based on PCPIP.</p>"},{"location":"developer/legacy/pcpip-specs/#overview","title":"Overview","text":"<p>The image processing workflow consists of two parallel tracks followed by a combined analysis:</p> <ol> <li>Cell Painting Processing (Pipelines 1-4)</li> <li>Barcoding Processing (Pipelines 5-8)</li> <li>Combined Analysis (Pipeline 9)</li> </ol> <p>Each pipeline is launched by a corresponding AWS Lambda function that configures input/output paths, prepares metadata, and creates CSV files to drive the CellProfiler analysis.</p> <p></p> <p>The diagram above illustrates the complete workflow with both Cell Painting (top row) and Barcoding (middle row) image processing tracks, followed by the combined analysis step. The pipeline numbers correspond to the CellProfiler pipelines described in this document.</p> <p>The workflow is orchestrated by AWS Lambda functions, with each function responsible for a specific pipeline stage. These Lambda functions serve as the automation backbone that coordinates pipeline execution, handles configuration, and manages the processing of thousands of images across AWS resources.</p>"},{"location":"developer/legacy/pcpip-specs/#lambda-function-architecture-and-implementation","title":"Lambda Function Architecture and Implementation","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-flow-and-triggers","title":"Pipeline Flow and Triggers","text":"<pre><code>flowchart TD\n    %% Main pipelines with detailed descriptions\n    subgraph \"Cell Painting Track\"\n        PCP1[\"PCP-1-CP-IllumCorr\n        Calculate illum functions\"] --&gt;\n        PCP2[\"PCP-2-CP-ApplyIllum\n        Apply correction\n        Segment cells\n        Get thresholds\"] --&gt;\n        PCP3[\"PCP-3-CP-SegmentCheck\n        Verify segmentation quality\n        on subset of images\"]\n        PCP3 --&gt; PCP4[\"PCP-4-CP-Stitching\n        Stitch FOVs into whole-well\n        Crop into tiles\"]\n    end\n\n    subgraph \"Barcoding Track\"\n        PCP5[\"PCP-5-BC-IllumCorr\n        Calculate illum functions\"] --&gt;\n        PCP6[\"PCP-6-BC-ApplyIllum\n        Apply correction\n        Align cycles\"] --&gt;\n        PCP7[\"PCP-7-BC-Preprocess\n        Compensate channels\n        Identify &amp; call barcodes\"] --&gt;\n        PCP8[\"PCP-8-BC-Stitching\n        Stitch FOVs into whole-well\n        Crop into tiles\n        (ensure match to CP crops)\"]\n    end\n\n    PCP4 &amp; PCP8 --&gt; PCP9[\"PCP-9-Analysis\n        Align CP &amp; BC images\n        Segment cells\n        Measure features\n        Call barcodes\n        Measure QC\"]\n\n    %% Troubleshooting/specialized pipelines\n    PCP8Y[\"PCP-8Y-BC-CheckAlignmentPostStitch\n    Validate alignment b/w\n    stitched CP &amp; BC images\"] -.-&gt; PCP8\n    PCP8Z[\"PCP-8Z-StitchAlignedBarcoding\n    Stitch aligned images\n    (not corrected images)\"] -.-&gt; PCP8\n\n    PCP7A[\"PCP-7A-BC-PreprocessTroubleshoot\n    Specialized version with\n    additional diagnostics\"] -.-&gt; PCP7\n\n    PCP6A[\"PCP-6A-BC-ApplyIllum-DebrisMask\n    Alternative version that\n    identifies &amp; masks debris\"] -.-&gt; PCP6\n\n    %% Processing platforms\n    classDef cellprofiler fill:#e6f3ff,stroke:#0066cc\n    classDef fiji fill:#e6ffe6,stroke:#009900\n\n    class PCP1,PCP2,PCP3,PCP5,PCP6,PCP7,PCP7A,PCP8Y,PCP9,PCP6A cellprofiler\n    class PCP4,PCP8,PCP8Z fiji</code></pre> <p>Each pipeline in the workflow is orchestrated by a corresponding AWS Lambda function (PCP-1 through PCP-9). These Lambda functions automate the pipeline execution and handle the transition of data between stages in a sequential workflow:</p> <ol> <li>The workflow begins with two parallel tracks: Cell Painting processing (PCP-1 through PCP-4) and barcoding processing (PCP-5 through PCP-8)</li> <li>Each Lambda function is triggered by the output of the previous step (typically a file upload to S3)</li> <li>For example, PCP-1-CP-IllumCorr is triggered by the upload of the 1_CP_Illum.cppipe file, and produces illumination function files that then trigger PCP-2-CP-ApplyIllum</li> <li>The final Lambda function (PCP-9-Analysis) integrates the outputs from both tracks for comprehensive analysis</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#lambda-function-implementation-pattern","title":"Lambda Function Implementation Pattern","text":"<p>All Lambda functions in the workflow follow a common implementation pattern:</p> <ol> <li>Trigger Processing: Responds to S3 event or manual invocation<ul> <li>Lambda's <code>lambda_handler</code> function is the entry point</li> <li>For S3 triggers: <code>event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]</code> extracts the triggering file path</li> <li>For manual triggers (like PCP-9-Analysis): Empty event object is passed with hardcoded parameters</li> </ul> </li> <li>Configuration Loading:<ul> <li>Loads experiment configuration from metadata.json using <code>download_and_read_metadata_file()</code></li> <li>Contains pipeline-specific AWS resource requirements in <code>config_dict</code></li> <li>May check if previous step's completion using <code>check_if_run_done()</code></li> </ul> </li> <li>Pipeline Selection and Plate Filtering:<ul> <li>Selects appropriate pipeline variant based on experiment configuration</li> <li>Applies optional plate inclusion/exclusion filters</li> </ul> </li> <li>Input Discovery and CSV Generation:<ul> <li>Uses <code>paginate_a_folder()</code> to list all input files efficiently</li> <li>Parses image names using <code>parse_image_names()</code> to extract metadata (wells, sites, channels)</li> <li>Creates pipeline-specific CSV file using the appropriate <code>create_CSV_pipeline*()</code> function</li> <li>Uploads the generated CSV to S3 for the CellProfiler pipeline to consume</li> </ul> </li> <li>AWS EC2 Job Configuration and Execution:<ul> <li>Sets up AWS environment with <code>run_setup()</code></li> <li>Configures batch jobs with the pipeline-specific <code>create_batch_jobs_*()</code> function</li> <li>Launches EC2 instances with Docker containers via <code>run_cluster()</code></li> <li>Sets up job completion monitoring with <code>run_monitor()</code></li> </ul> </li> </ol>"},{"location":"developer/legacy/pcpip-specs/#key-utility-functions","title":"Key Utility Functions","text":"<p>Each Lambda function relies on a common set of utility modules:</p>"},{"location":"developer/legacy/pcpip-specs/#from-helpful_functionspy","title":"From helpful_functions.py:","text":"<ul> <li>download_and_read_metadata_file(): Retrieves and parses experiment configuration</li> <li>paginate_a_folder(): Lists S3 objects with pagination for large image sets</li> <li>parse_image_names(): Extracts metadata from image filenames</li> <li>write_metadata_file(): Updates metadata with processing results</li> <li>check_if_run_done(): Validates job completion status</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#from-create_csvspy","title":"From create_CSVs.py:","text":"<ul> <li>create_CSV_pipelineN(): Pipeline-specific CSV generators</li> <li>Each translates experiment parameters into CellProfiler-compatible format</li> <li>Handles different acquisition modes (fast/slow, one/many files)</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#from-run_dcppy-and-create_batch_jobspy","title":"From run_DCP.py and create_batch_jobs.py:","text":"<ul> <li>Functions for creating and monitoring AWS EC2 jobs</li> <li>Configures EC2 instances based on pipeline requirements</li> <li>Handles container setup and execution</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-configuration-system","title":"Pipeline Configuration System","text":"<p>This section details the configuration parameters and settings needed to set up the CellProfiler pipelines for Pooled Cell Painting experiments.</p>"},{"location":"developer/legacy/pcpip-specs/#multi-layered-configuration-architecture","title":"Multi-layered Configuration Architecture","text":"<p>The system employs a three-tiered configuration approach that separates experimental, computational, and infrastructure concerns. This separation allows independent modification of experimental parameters, resource allocation, and AWS infrastructure.</p> <pre><code>flowchart TD\n    %% Color-coded configuration sources by layer\n    subgraph \"Configuration Sources\"\n        metadata[\"metadata.json\n        WHAT data to process\n        HOW to process it\n        (painting_rows, barcoding_cycles, Channeldict)\"]:::experimentConfig\n        config_dict[\"Lambda config_dict\n        HOW MUCH compute power\n        WHEN jobs complete\n        (MACHINE_TYPE, MEMORY, EXPECTED_NUMBER_FILES)\"]:::computeConfig\n        aws_config[\"AWS config files\n        WHERE in AWS to run\n        (AWS_REGION, ECS_CLUSTER, IamFleetRole)\"]:::infraConfig\n    end\n\n    subgraph \"Configuration Processing\"\n        download_metadata[\"download_and_read_metadata_file\n        (in every Lambda function)\"]\n        grab_config[\"grab_batch_config\n        (loads from S3)\"]\n        loadConfig[\"loadConfig\n        (processes infrastructure settings)\"]\n    end\n\n    subgraph \"Configuration Consumers\"\n        csv_gen[\"CSV Generation\n        Uses image grid, channels,\n        acquisition mode, cycles\"]\n        batch_creation[\"Batch Job Creation\n        Experiment parameters +\n        Resource allocation +\n        Infrastructure location\"]\n        pipeline_selection[\"Pipeline Selection\n        Based on cycle count, channels,\n        experiment type\"]\n        ec2_config[\"EC2 Configuration\n        Subnet, security groups,\n        AMI, instance profile\"]\n    end\n\n    %% Configuration flow with numbered sequence\n    metadata --&gt;|1 First loaded| download_metadata\n    config_dict --&gt;|2 Defined in each Lambda| batch_creation\n    aws_config --&gt;|3 Loaded when needed| grab_config\n\n    download_metadata --&gt; pipeline_selection\n    download_metadata --&gt; csv_gen\n    grab_config --&gt; loadConfig\n    loadConfig --&gt; ec2_config\n\n    pipeline_selection --&gt; batch_creation\n    csv_gen --&gt; batch_creation\n    ec2_config --&gt; batch_creation\n\n    %% Style definitions\n    classDef experimentConfig fill:#e6f3ff,stroke:#0066cc\n    classDef computeConfig fill:#e6ffe6,stroke:#009900\n    classDef infraConfig fill:#fff2e6,stroke:#ff8c1a</code></pre>"},{"location":"developer/legacy/pcpip-specs/#configuration-layer-relationships","title":"Configuration Layer Relationships","text":"<p>Each layer serves a distinct purpose in the overall system:</p> <ul> <li>Experiment Configuration (metadata.json): Controls WHAT data is processed and HOW it's processed</li> <li>Computing Resource Configuration (Lambda config_dict): Specifies HOW MUCH computing power is allocated and WHEN jobs are considered complete</li> <li>Infrastructure Configuration (AWS config files): Determines WHERE in AWS the processing happens</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#detailed-configuration-parameters","title":"Detailed Configuration Parameters","text":""},{"location":"developer/legacy/pcpip-specs/#1-experiment-configuration-metadatajson","title":"1. Experiment Configuration (metadata.json)","text":"<p>The <code>metadata.json</code> file (based on <code>configs/metadatatemplate.json</code>) defines all experiment-specific parameters:</p>"},{"location":"developer/legacy/pcpip-specs/#image-grid-configuration","title":"Image Grid Configuration","text":"<ul> <li><code>painting_rows</code>, <code>painting_columns</code>: Define Cell Painting image layout for square acquisitions</li> <li><code>painting_imperwell</code>: Defines total images per well for circular acquisitions (overrides rows/columns)</li> <li><code>barcoding_rows</code>, <code>barcoding_columns</code>: Define barcoding image layout for square acquisitions</li> <li><code>barcoding_imperwell</code>: Defines total images per well for circular acquisitions (overrides rows/columns)</li> <li>Note: Use rows/columns for square acquisition patterns; use imperwell for circular acquisition patterns</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#channel-dictionary-configuration","title":"Channel Dictionary Configuration","text":"<ul> <li>Maps microscope channel names to biological stains and frame indices</li> <li>For multiple rounds (SABER) experiments:<ul> <li>Keys are the folder names of the rounds (e.g., '20X_c0-SABER-0', '20X_c1-SABER-1')</li> <li>Common stains between rounds should include round identifiers (e.g., 'DNA_round0', 'DNA_round1')</li> <li>Example:   <pre><code>  {\n  \"20X_c0-SABER-0\": {\n      \"DAPI\": [\n      \"DNA_round0\",\n      0\n      ],\n      \"GFP\": [\n      \"Phalloidin\",\n      1\n      ]\n  },\n  \"20X_c1-SABER-1\": {\n      \"DAPI\": [\n      \"DNA_round1\",\n      0\n      ],\n      \"GFP\": [\n      \"GM130\",\n      1\n      ],\n      \"A594\": [\n      \"Tubulin\",\n      2\n      ],\n      \"Cy5\": [\n      \"Calnexin\",\n      3\n      ]\n  },\n  \"20X_c2-SABER-2\": {\n      \"750\": [\n      \"LAMP1\",\n      4\n      ],\n      \"DAPI\": [\n      \"DNA_round2\",\n      0\n      ],\n      \"GFP\": [\n      \"COX-IV\",\n      1\n      ],\n      \"A594\": [\n      \"TDP-43\",\n      2\n      ],\n      \"Cy5\": [\n      \"G3BP1\",\n      3\n      ]\n  }\n  }\n</code></pre></li> </ul> </li> <li>For single-round experiments:<ul> <li>Single key matching the Cell Painting folder name (typically '20X_CP')</li> <li>Example:   <pre><code>  {\n  \"20X_CP\": {\n      \"750\": [\n      \"WGA\",\n      4\n      ],\n      \"DAPI\": [\n      \"DNA\",\n      0\n      ],\n      \"GFP\": [\n      \"Phalloidin\",\n      1\n      ],\n      \"A594\": [\n      \"Mito\",\n      2\n      ],\n      \"Cy5\": [\n      \"ER\",\n      3\n      ]\n  }\n  }\n</code></pre></li> </ul> </li> <li>First value in each array is the stain name, second is the frame index (0-based)</li> <li>Used to determine the pipeline variant (SABER vs. standard)</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#processing-configuration","title":"Processing Configuration","text":"<ul> <li><code>one_or_many_files</code>: Controls if each well is stored as a single file (<code>\"one\"</code>) or multiple files (<code>\"many\"</code>)<ul> <li>Should be locked to <code>\"many\"</code> for production runs</li> </ul> </li> <li><code>fast_or_slow_mode</code>: Determines CSV generation strategy and processing path<ul> <li>Should be locked to <code>\"slow\"</code> for production runs</li> </ul> </li> <li><code>barcoding_cycles</code>: Sets the number of barcoding cycles to process</li> <li><code>range_skip</code>: Sets sampling frequency for Pipeline 3 (SegmentCheck), to process subset of images for validation<ul> <li>Typically doesn't need to be changed from default</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#stitching-configuration","title":"Stitching Configuration","text":"<ul> <li><code>overlap_pct</code>: Controls image overlap percentage between adjacent fields</li> <li><code>stitchorder</code>: Specifies tile arrangement<ul> <li>For square acquisitions: \"Grid: snake by rows\" or \"Grid: row-by-row\"</li> <li>For round acquisitions: \"Filename defined position\"</li> </ul> </li> <li><code>tileperside</code>: Number of tiles along each side of the stitched image grid<ul> <li>Typically doesn't need to be changed from default</li> </ul> </li> <li><code>final_tile_size</code>: Pixel dimensions of each output tile after cropping<ul> <li>Typically doesn't need to be changed from default</li> </ul> </li> <li><code>round_or_square</code>: Shape of the well for cropping calculations (<code>\"round\"</code> or <code>\"square\"</code>)</li> <li><code>quarter_if_round</code>: Whether to divide round wells into quarters for processing (<code>\"True\"</code> or <code>\"False\"</code>)</li> <li><code>*_xoffset_tiles</code>, <code>*_yoffset_tiles</code>: Optional offsets for troubleshooting stitching misalignments<ul> <li>Should be 0 unless troubleshooting gross stitching misalignments</li> </ul> </li> <li><code>compress</code>: Whether to compress output files (<code>\"True\"</code> or <code>\"False\"</code>)<ul> <li>Should be set to <code>\"True\"</code> to save out compressed files from stitch-crop pipelines</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#2-computing-resource-configuration-lambda-config_dict","title":"2. Computing Resource Configuration (Lambda config_dict)","text":"<p>Each Lambda function contains a specific <code>config_dict</code> with pipeline-appropriate settings:</p> <pre><code>config_dict = {\n    \"APP_NAME\": \"2018_11_20_Periscope_X_IllumPainting\",\n    \"DOCKERHUB_TAG\": \"cellprofiler/distributed-cellprofiler:2.0.0_4.2.1\",\n    \"TASKS_PER_MACHINE\": \"1\",\n    \"MACHINE_TYPE\": [\"c5.xlarge\"],\n    \"MEMORY\": \"7500\",\n    \"DOCKER_CORES\": \"4\",\n    \"CHECK_IF_DONE_BOOL\": \"True\",\n    \"EXPECTED_NUMBER_FILES\": \"5\",  # Varies by pipeline\n    # Additional parameters...\n}\n</code></pre> <p>The key parameters that need configuration for each pipeline are:</p> <ul> <li>APP_NAME: Unique identifier for the specific experiment</li> <li>MACHINE_TYPE: EC2 instance type appropriate for the pipeline's computational needs</li> <li>MEMORY: RAM allocation for the Docker container</li> <li>EXPECTED_NUMBER_FILES: Number of output files to expect</li> <li>CHECK_IF_DONE_BOOL: Controls validation of job completion</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#3-pipeline-specific-csv-configuration","title":"3. Pipeline-Specific CSV Configuration","text":"<p>Each pipeline is driven by a CSV file with a specific structure generated by functions in <code>create_CSVs.py</code>:</p> Pipeline CSV Generator Function 1:  CP-Illum <code>create_CSV_pipeline1()</code> 2:  CP-ApplyIllum <code>create_CSV_pipeline1()</code> 3:  SegmentCheck <code>create_CSV_pipeline3()</code> 4:  CP-Stitching (external FIJI script) 5:  BC-Illum <code>create_CSV_pipeline5()</code> 6:  BC-ApplyIllum <code>create_CSV_pipeline6()</code> 7:  BC-Preprocess <code>create_CSV_pipeline7()</code> 7A: BC-PreprocessTroubleshoot <code>create_CSV_pipeline7()</code> 8:  BC-Stitching (external FIJI script) 8Y: BC-CheckAlignment <code>create_CSV_pipeline8Y()</code> 9:  Analysis <code>create_CSV_pipeline9()</code> <p>The CSV files translate configuration parameters into CellProfiler-compatible format using:</p> <ul> <li>FileName_X and PathName_X columns for each channel (essential for LoadData module)</li> <li>Metadata_X columns for grouping and organization (Plate, Well, Site)</li> <li>Frame_X and Series_X columns for multi-channel files in fast mode</li> <li>Cycle-prefixed columns for barcoding data (Cycle01, Cycle02, etc.)</li> <li>_Orig suffix for raw images and _Illum suffix for illumination files</li> <li>_Corr suffix for corrected images that have had illumination applied</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-specifications","title":"Pipeline Specifications","text":"<p>This section provides detailed specifications for all nine primary CellProfiler and Fiji pipelines and their specialized variants, including their purposes, Lambda function triggers, key operations, inputs/outputs, and configuration requirements for implementing the complete Pooled Cell Painting workflow.</p>"},{"location":"developer/legacy/pcpip-specs/#cell-painting","title":"Cell Painting","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-1-cell-painting-illumination-correction-1_cp_illumcppipe","title":"Pipeline 1: Cell Painting Illumination Correction (1_CP_Illum.cppipe)","text":"<p>Purpose: Calculate per-plate illumination correction functions for each Cell Painting channel</p> <p>Lambda Function: <code>PCP-1-CP-IllumCorr</code></p> <ul> <li>Trigger: S3 upload of 1_CP_Illum.cppipe</li> <li>CSV Generator: <code>create_CSV_pipeline1()</code></li> <li>Output: Illumination function files (.npy)</li> </ul> <p>Key Operations:</p> <ol> <li>Loads raw images via CSV configuration</li> <li>For each channel (DNA, ER, Phalloidin, Mito, WGA):<ul> <li>Downsample images to 25% size for faster processing</li> <li>Calculate illumination function across all images using median filtering</li> <li>Upsample correction back to original size</li> </ul> </li> <li>Save correction functions as .npy files with naming convention: <code>{Plate}_Illum{Channel}.npy</code></li> </ol>"},{"location":"developer/legacy/pcpip-specs/#pipeline-2-cell-painting-illumination-application-2_cp_apply_illumcppipe","title":"Pipeline 2: Cell Painting Illumination Application (2_CP_Apply_Illum.cppipe)","text":"<p>Purpose: Apply illumination correction and segment cells for quality control</p> <p>Lambda Function: <code>PCP-2-CP-ApplyIllum</code></p> <ul> <li>Trigger: S3 upload of IllumMito.npy (output from Pipeline 1)</li> <li>CSV Generator: Created by <code>create_CSV_pipeline1()</code> (generates two CSVs at once)</li> <li>Output: Corrected cell images (.tiff) and segmentation parameters</li> </ul> <p>Key Operations:</p> <ol> <li>Apply illumination correction to all channels via division method</li> <li>Identify confluent regions to mask out problem areas</li> <li>Segment nuclei in DNA channel (10-80 pixel diameter)</li> <li>Identify cell boundaries from nuclei using watershed segmentation</li> <li>Export segmentation thresholds for quality control</li> <li>Save corrected images as TIFF files</li> </ol> <p>Configuration Details:</p> <ul> <li>Segmentation thresholds are automatically calculated and stored for Pipeline 3</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-3-cell-painting-segmentation-check-3_cp_segmentationcheckcppipe","title":"Pipeline 3: Cell Painting Segmentation Check (3_CP_SegmentationCheck.cppipe)","text":"<p>Purpose: Verify segmentation quality on a subset of images</p> <p>Lambda Function: <code>PCP-3-CP-SegmentCheck</code></p> <ul> <li>Trigger: S3 upload of PaintingIllumApplication_Image.csv (from Pipeline 2)</li> <li>CSV Generator: <code>create_CSV_pipeline3()</code></li> <li>Output: Quality control overlay images showing segmentation</li> </ul> <p>Key Operations:</p> <ol> <li>Load a subset of corrected images (skipping some sites per Lambda configuration)</li> <li>Apply segmentation using min/max thresholds from Pipeline 2</li> <li>Create color overlay images showing segmentation results</li> <li>Export metrics to validate segmentation quality</li> </ol> <p>Configuration Details:</p> <ul> <li>Uses <code>range_skip</code> parameter to process only a subset of images</li> <li>Lambda reads segmentation thresholds from Pipeline 2 output</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-4-cell-painting-stitching-and-cropping","title":"Pipeline 4: Cell Painting Stitching and Cropping","text":"<p>Purpose: Stitch field-of-view images into whole-well montages and create manageable tiles</p> <p>Lambda Function: <code>PCP-4-CP-Stitching</code></p> <ul> <li>Trigger: S3 upload of SegmentationCheck_Experiment.csv (from Pipeline 3)</li> <li>Implementation: Uses FIJI scripts rather than CellProfiler</li> <li>Output: Stitched and cropped Cell Painting images</li> </ul> <p>Key Operations:</p> <ol> <li>Stitch multiple fields of view into single whole-well image</li> <li>Generate a smaller (10x) version for preview</li> <li>Crop stitched image into standardized tiles</li> <li>Save output in tiered directory structure by batch and well</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#barcoding","title":"Barcoding","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-5-barcoding-illumination-correction-5_bc_illumcppipe","title":"Pipeline 5: Barcoding Illumination Correction (5_BC_Illum.cppipe)","text":"<p>Purpose: Calculate illumination correction for barcoding images in each cycle</p> <p>Lambda Function: <code>PCP-5-BC-IllumCorr</code></p> <ul> <li>Trigger: S3 upload of 5_BC_Illum.cppipe</li> <li>CSV Generator: <code>create_CSV_pipeline5()</code></li> <li>Output: Cycle-specific illumination function files (.npy)</li> </ul> <p>Key Operations:</p> <ol> <li>Load barcoding images from all cycles</li> <li>For each channel (DNA, A, C, G, T) in each cycle and plate:<ul> <li>Downsample for faster processing</li> <li>Calculate illumination function with cycle-specific and plate-specific correction</li> <li>Upsample back to original size</li> </ul> </li> <li>Save per-cycle, per-channel, per-plate correction functions</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#pipeline-6-barcoding-illumination-application-6_bc_apply_illumcppipe","title":"Pipeline 6: Barcoding Illumination Application (6_BC_Apply_Illum.cppipe)","text":"<p>Purpose: Apply illumination correction and align images across channels and cycles</p> <p>Lambda Function: <code>PCP-6-BC-ApplyIllum</code></p> <ul> <li>Trigger: S3 upload of Cycle1_IllumA.npy (from Pipeline 5)</li> <li>CSV Generator: <code>create_CSV_pipeline6()</code></li> <li>Output: Aligned barcoding images</li> </ul> <p>Key Operations:</p> <ol> <li>Apply illumination correction by channel, cycle, and plate</li> <li>No alignment of A, C, G, T channels to DAPI within each cycle (fast mode is no longer supported)</li> <li>Align all cycle DAPI images to Cycle 1 DAPI</li> <li>Shift A, C, G, T channels by same amount as their DAPI image</li> <li>Save corrected and aligned images</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#pipeline-7-barcoding-preprocessing-7_bc_preprocesscppipe","title":"Pipeline 7: Barcoding Preprocessing (7_BC_Preprocess.cppipe)","text":"<p>Purpose: Process aligned barcoding images to identify and characterize barcode foci</p> <p>Lambda Function: <code>PCP-7-BC-Preprocess</code></p> <ul> <li>Trigger: S3 upload of BarcodingApplication_Experiment.csv (from Pipeline 6)</li> <li>CSV Generator: <code>create_CSV_pipeline7()</code></li> <li>Output: Processed barcoding images with foci identification</li> </ul> <p>Key Operations:</p> <ol> <li>Perform per-image illumination correction grouped by cycle</li> <li>Calculate average and standard deviation images across cycles for quality control</li> <li>Identify nuclei using the DAPI channel and cells using propagation from nuclei. Note: <code>IdentifySecondaryObjects</code> module remains in the pipeline but cell segmentations are not used for meaningful analysis because:<ul> <li>Cells identified without phenotypic stains are unreliable and have poor boundaries</li> <li>Getting accurate cell segmentations would require significant per-experiment tuning</li> <li>Barcode-only QC metrics provide sufficient data quality indicators that correlate well with downstream cell-based metrics</li> <li>Proper cell segmentation is performed in Pipeline 9 with full phenotypic stains</li> </ul> </li> <li>Identify potential barcode foci in each channel</li> <li>The CompensateColors module includes many configurable parameters including:<ul> <li>Histogram matching options (pre/post-masking, template selection)</li> <li>Channel compensation to correct for spectral bleed-through</li> <li>Filter-based processing (Laplacian of Gaussian, Tophat, Difference of Gaussians)</li> <li>Channel-specific background subtraction</li> <li>Processing sequence controls (pre/post masking rescaling)</li> <li>Within-object vs. whole-image processing options</li> <li>Percentile-based intensity normalization</li> <li>Spot size and intensity thresholding</li> </ul> </li> <li>Analyze foci intensities and call barcodes</li> <li>Relate barcode foci to cell objects for spatial organization</li> <li>Create composite images for QC visualization</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#pipeline-8-barcoding-stitching-and-cropping","title":"Pipeline 8: Barcoding Stitching and Cropping","text":"<p>Purpose: Stitch and crop barcoding images similar to Cell Painting images</p> <p>Lambda Function: <code>PCP-8-BC-Stitching</code></p> <ul> <li>Trigger: S3 upload of BarcodePreprocessing_Experiment.csv (from Pipeline 7)</li> <li>Implementation: Uses FIJI scripts rather than CellProfiler</li> <li>Output: Stitched barcoding images</li> </ul> <p>Key Operations:</p> <ol> <li>Similar to Pipeline 4, but operates on barcoding images</li> <li>Stitches according to same grid layout as Cell Painting</li> <li>Produces consistent tile naming for alignment with Cell Painting tiles</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#final-analysis-pipeline","title":"Final Analysis Pipeline","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-9-analysis-9_analysiscppipe","title":"Pipeline 9: Analysis (9_Analysis.cppipe)","text":"<p>Purpose: Integrate Cell Painting and barcoding data for downstream analysis</p> <p>Lambda Function: <code>PCP-9-Analysis</code></p> <ul> <li>Trigger: Manual trigger (triggered after both tracks complete)</li> <li>CSV Generator: <code>create_CSV_pipeline9()</code></li> <li>Output: Integrated analysis results and segmentation masks</li> </ul> <p>Key Operations:</p> <ol> <li>Align Cell Painting images to barcoding images using DAPI channels</li> <li>Identify and mask overly-confluent regions</li> <li>Segment nuclei, cells, cytoplasm in Cell Painting images</li> <li>Locate barcode foci in aligned images</li> <li>Measure Cell Painting features across all compartments</li> <li>Call barcodes and annotate quality metrics</li> <li>Export segmentation masks and merged, annotated images for visualization</li> </ol>"},{"location":"developer/legacy/pcpip-specs/#special-purpose-pipelines","title":"Special-Purpose Pipelines","text":"<p>In addition to the main pipeline sequence, there are specialized pipelines for troubleshooting:</p>"},{"location":"developer/legacy/pcpip-specs/#7a_bc_preprocess_troubleshootingcppipe","title":"7A_BC_Preprocess_Troubleshooting.cppipe","text":"<p>Purpose: Specialized version of Pipeline 7 with additional diagnostics</p> <p>Lambda Function: <code>PCP-7A-BC-PreprocessTroubleshoot</code></p> <ul> <li>Trigger: Manual trigger for troubleshooting</li> <li>Output: Diagnostic images and measurements</li> </ul> <p>Key Differences:</p> <ul> <li>Includes additional QC measurements</li> <li>Outputs more diagnostic images</li> <li>May use alternative image processing parameters</li> <li>Used when standard pipeline produces unexpected results</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#6_bc_apply_illum_debrismaskcppipe","title":"6_BC_Apply_Illum_DebrisMask.cppipe","text":"<p>Purpose: Alternative version of Pipeline 6 that identifies and masks debris</p> <p>Key Differences:</p> <ul> <li>Adds debris identification and masking</li> <li>Prevents debris from interfering with alignment</li> <li>Used for samples with high debris content</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#8y_checkalignmentpoststitching","title":"8Y_CheckAlignmentPostStitching","text":"<p>Purpose: Validate alignment between stitched Cell Painting and Barcoding images</p> <p>Lambda Function: <code>PCP-8Y-BC-CheckAlignmentPostStitch</code></p> <ul> <li>Trigger: Manual trigger or after completion of Pipelines 4 and 8</li> <li>CSV Generator: <code>create_CSV_pipeline8Y()</code></li> <li>Implementation: Uses CellProfiler pipeline for alignment validation</li> <li>Output: Alignment validation images and metrics</li> </ul> <p>Key Operations:</p> <ol> <li>Takes the stitched Cell Painting DNA images and Cycle01 DAPI images from Barcoding</li> <li>Cross-references these images to validate their alignment</li> <li>Produces diagnostic images showing alignment quality</li> <li>Identifies any systematic misalignments that may need adjustment in the metadata</li> </ol> <p>Configuration Details:</p> <ul> <li>Uses the <code>Cycle01_DAPI</code> channel from barcoding as the reference</li> <li>Compares with <code>CorrDNA</code> from Cell Painting track</li> <li>Creates a CSV that links corresponding tiles from both imaging modalities</li> <li>Runs on a site-by-site basis for detailed validation</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#8z_stitchalignedbarcoding","title":"8Z_StitchAlignedBarcoding","text":"<p>Purpose: Stitch and crop barcoding images from the aligned images directory instead of the corrected images directory</p> <p>Lambda Function: <code>PCP-8Z-StitchAlignedBarcoding</code></p> <ul> <li>Trigger: Manual trigger, typically after alignment issues are discovered</li> <li>Implementation: Uses the same FIJI script as Pipeline 8 but on different inputs</li> <li>Output: Stitched and cropped barcoding images from aligned sources</li> </ul> <p>Key Operations:</p> <ol> <li>Similar to Pipeline 8, but takes input from the <code>images_aligned</code> directory instead of <code>images_corrected</code></li> <li>Uses the FIJI stitching script to create stitched whole-well images</li> <li>Also creates a smaller (10x) version for preview and visualization</li> <li>Crops stitched images into standardized tiles for downstream analysis</li> <li>Handles both square and round wells using the same configurable parameters as Pipeline 8</li> </ol> <p>Configuration Details:</p> <ul> <li>Uses identical configuration parameters to Pipeline 8</li> <li>Allows separate adjustment of x/y offset tiles through metadata</li> <li>Supports compression of output images when configured</li> <li>Can divide round wells into quarters for more manageable processing</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>This section explains how CellProfiler pipelines interact with the configuration system and detailed implementation considerations for each pipeline step.</p>"},{"location":"developer/legacy/pcpip-specs/#pipeline-variant-selection","title":"Pipeline Variant Selection","text":"<p>Lambda functions select specific pipeline variants based on experimental configuration:</p> <pre><code># Sample code from PCP-1-CP-IllumCorr\nif len(Channeldict.keys()) == 1:  # Standard experiment\n    pipeline_name = \"1_CP_Illum.cppipe\"\nif len(Channeldict.keys()) &gt; 1:   # SABER experiment\n    pipeline_name = \"1_SABER_CP_Illum.cppipe\"\n</code></pre> <p>This dynamic selection allows the same Lambda function to handle different experimental designs without code changes.</p>"},{"location":"developer/legacy/pcpip-specs/#csv-generation-implementation","title":"CSV Generation Implementation","text":"<p>Each pipeline stage has a specialized CSV generator function that translates metadata parameters into CellProfiler-compatible input:</p> <p>Channel Dictionary Parsing:</p> <pre><code># From create_CSV_pipeline1()\nChanneldict = ast.literal_eval(Channeldict)\nrounddict = {}\nChannelrounds = list(Channeldict.keys())\nfor eachround in Channelrounds:\n    templist = []\n    templist += Channeldict[eachround].values()\n    channels += list(i[0] for i in templist)\n</code></pre> <p>Acquisition Mode Handling:</p> <pre><code># From create_CSV_pipeline6()\nif one_or_many == \"one\" and fast_or_slow == \"fast\":\n    # One file structure\nelif one_or_many == \"many\" and fast_or_slow == \"slow\":\n    # Many file structure\n</code></pre> <p>Cycle-Aware Configuration:</p> <pre><code># From create_CSV_pipeline7()\nfor cycle in range(1, (expected_cycles + 1)):\n    this_cycle = \"Cycle%02d_\" % cycle\n    # Configure cycle-specific columns\n</code></pre>"},{"location":"developer/legacy/pcpip-specs/#cellprofiler-pipeline-parameterization","title":"CellProfiler Pipeline Parameterization","text":"<p>CellProfiler pipelines are parameterized through CSV columns that control their behavior:</p> <ol> <li> <p>Metadata Grouping: Controls how images are processed together    <pre><code>Group images by metadata?:Yes\nSelect metadata tags for grouping:Plate\n</code></pre></p> </li> <li> <p>Channel Selection: Driven by metadata-derived CSV columns    <pre><code>Select the input image:OrigDNA  # Comes from CSV FileName_OrigDNA column\n</code></pre></p> </li> <li> <p>Output Naming: Uses metadata variables from the CSV    <pre><code>Enter single file name:\\g&lt;Plate&gt;_IllumDNA  # \\g&lt;&gt; syntax references metadata\n</code></pre></p> </li> </ol> <p>This parameterization approach enables the same pipeline code to process different experimental designs based on the configuration-derived CSV input.</p>"},{"location":"developer/legacy/pcpip-specs/#pipeline-input-output-specifications","title":"Pipeline Input Output Specifications","text":"<p>Note:</p> <ul> <li>The input / outputs are also in parseable form <code>pcpip-io.json</code>. Note that it uses <code>Tile</code> instead of <code>Site</code> when referring to the outputs of Pipelines 4 and 8.</li> <li>In the jsons, <code>{1_CP_Illum|2_CP_Apply_Illum|5_BC_Illum|6_BC_Apply_Illum}.input.images.pattern</code> are mockups</li> <li>Sample outputs of paths and LoadData CSVs generated using <code>pcpip-io.json</code> are here.</li> <li>FIXME: We also need to specify as input, the channels that (1) will be used for nuclear segmentation (2) cell segmentation (3) mitochondria (for some filtering that happens in Pipeline 9)</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#cell-painting_1","title":"Cell Painting","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-1-cell-painting-illumination-correction","title":"Pipeline 1: Cell Painting Illumination Correction","text":"<ul> <li>Input Images: Raw Cell Painting images</li> <li>LoadData CSV Fields:<ul> <li><code>FileName_Orig{Channel}</code>, <code>PathName_Orig{Channel}</code> (raw Cell Painting images, where Channel = DNA, Phalloidin, etc.)</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> </ul> </li> <li>Output Files:<ol> <li>Per-plate illumination correction functions for each channel</li> </ol> </li> <li>Output Directory:<ul> <li><code>{Batch}/illum/{Plate}/</code></li> </ul> </li> <li>NPY Naming Pattern:<ul> <li><code>{Plate}_Illum{Channel}.npy</code>: Illumination function</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-2-cell-painting-illumination-application","title":"Pipeline 2: Cell Painting Illumination Application","text":"<ul> <li>Input Images:<ul> <li>Raw Cell Painting images</li> <li>Illumination correction files (<code>.npy</code>) from Pipeline 1</li> </ul> </li> <li>LoadData CSV Fields:<ul> <li><code>FileName_Orig{Channel}</code>, <code>PathName_Orig{Channel}</code> (raw Cell Painting images, where Channel = DNA, Phalloidin, etc.)</li> <li><code>FileName_Illum{Channel}</code>, <code>PathName_Illum{Channel}</code> (corresponding illumination correction files)</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> </ul> </li> <li>Output Files:<ol> <li>Illumination-corrected images for each channel and site</li> <li>CSV files with measurements</li> </ol> </li> <li>Output Directory:<ul> <li><code>{Batch}/images_corrected/painting/{Plate}-{Well}/</code></li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_Corr{Channel}.tiff</code>: Illumination corrected image</li> </ul> </li> <li>CSV Naming Pattern:<ul> <li><code>PaintingIllumApplication_Image.csv</code></li> <li><code>PaintingIllumApplication_Cells.csv</code></li> <li><code>PaintingIllumApplication_Nuclei.csv</code></li> <li><code>PaintingIllumApplication_ConfluentRegions.csv</code></li> <li><code>PaintingIllumApplication_Experiment.csv</code></li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-3-cell-painting-segmentation-check","title":"Pipeline 3: Cell Painting Segmentation Check","text":"<ul> <li>Input Images: Corrected Cell Painting images from Pipeline 2</li> <li>LoadData CSV Fields:<ul> <li><code>FileName_{Channel}</code>, <code>PathName_{Channel}</code> (corrected Cell Painting images, where Channel = DNA, Phalloidin, etc.)</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> </ul> </li> <li>Output Files:<ol> <li>Quality control overlay images showing segmentation results</li> <li>CSV files with measurements</li> </ol> </li> <li>Output Directory:<ul> <li><code>{Batch}/images_segmentation/{Plate}-{Well}/</code></li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_Corr{Channel}_SegmentCheck.png</code>: Overlay image</li> </ul> </li> <li>CSV Naming Pattern:<ul> <li><code>SegmentationCheck_Cells.csv</code></li> <li><code>SegmentationCheck_ConfluentRegions.csv</code></li> <li><code>SegmentationCheck_Experiment.csv</code></li> <li><code>SegmentationCheck_Image.csv</code></li> <li><code>SegmentationCheck_Nuclei.csv</code></li> <li><code>SegmentationCheck_PreCells.csv</code></li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-4-cell-painting-stitching-and-cropping_1","title":"Pipeline 4: Cell Painting Stitching and Cropping","text":"<ul> <li>Input Images: Corrected Cell Painting images from Pipeline 2</li> <li>FIJI Script Parameters (instead of LoadData CSV):<ul> <li><code>input_file_location</code>: Path to corrected images directory</li> <li><code>subdir</code>: Specific subfolder containing images to process (e.g., <code>{batch}/images_corrected/painting/{Plate}-{Well}</code>)</li> <li><code>filterstring</code>: Pattern to match image files (typically contains well identifier)</li> <li><code>channame</code>: Channel name for processing (e.g., \"DNA\")</li> <li><code>rows</code>, <code>columns</code> or <code>imperwell</code>: Image grid layout</li> <li><code>stitchorder</code>: Tile arrangement method</li> <li><code>overlap_pct</code>: Image overlap percentage</li> <li><code>size</code>: Size of individual tiles</li> <li><code>round_or_square</code>: Well shape for processing</li> <li><code>tileperside</code>: Number of tiles to create along each axis</li> <li><code>final_tile_size</code>: Pixel dimensions for output tiles</li> </ul> </li> <li>Output Files:<ol> <li>Stitched whole-well images for each channel</li> <li>Cropped tiles from stitched images</li> <li>Downsampled (10x) previews of stitched images</li> </ol> </li> <li>Output Directories:<ul> <li><code>{Batch}/images_corrected_stitched/cellpainting/{Plate}/{Plate}_{Well}/</code>: Stitched whole-well images</li> <li><code>{Batch}/images_corrected_cropped/cellpainting/{Plate}/{Plate}_{Well}/{Channel}/</code>: Cropped tile images</li> <li><code>{Batch}/images_corrected_stitched_10X/cellpainting/{Plate}/{Plate}_{Well}/Corr_{Channel}/</code>: Downsampled previews</li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Stitched{Channel}.tiff</code>: Stitched whole-well image (for square wells)</li> <li><code>Stitched[TopLeft|TopRight|BottomLeft|BottomRight]{Channel}.tiff</code>: Stitched quadrant images (for round wells)</li> <li><code>Corr_{Channel}_Site_{TileNumber}.tiff</code>: Cropped tile image</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#barcoding_1","title":"Barcoding","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-5-barcoding-illumination-correction","title":"Pipeline 5: Barcoding Illumination Correction","text":"<ul> <li>Input Images: Raw Barcoding images</li> <li>LoadData CSV Fields:<ul> <li><code>FileName_Orig{Channel}</code>, <code>PathName_Orig{Channel}</code> (raw barcoding images, where Channel = DNA, A, C, G, T)</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code>, <code>Metadata_SBSCycle</code></li> </ul> </li> <li>Output Files: Per-plate, per-cycle illumination correction functions for each channel</li> <li>Output Directory:<ul> <li><code>{Batch}/illum/{Plate}/</code></li> </ul> </li> <li>NPY Naming Pattern:<ul> <li><code>{Plate}_Cycle{K}_Illum{Channel}.npy</code>: Illumination function</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-6-barcoding-illumination-application-and-alignment","title":"Pipeline 6: Barcoding Illumination Application and Alignment","text":"<ul> <li>Input Images:<ul> <li>Raw Barcoding images</li> <li>Illumination correction files from Pipeline 5</li> </ul> </li> <li>LoadData CSV Fields:<ul> <li><code>FileName_Cycle{K}_{Channel}</code>, <code>PathName_Cycle{K}_{Channel}</code> (raw barcoding images, where Channel = DNA, A, C, G, T; and K = 1...N)</li> <li><code>FileName_Illum_Cycle{K}_{Channel}</code>, <code>PathName_Illum_Cycle{K}_{Channel}</code> (corresponding illumination correction files)</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> <li>Note: Pipeline 5 uses <code>Metadata_SBSCycle</code> while Pipelines 6 and 7 drop that and instead encode the cycle in the <code>FileName</code> and <code>PathName</code> column names. This reflects a deliberate pivot transformation of the data structure between pipelines. Pipeline 5 uses a narrow format CSV where each row represents one image from one cycle (with cycle as a row value). Pipeline 6 pivots this into a wide format where each row contains all cycle data for a site (with cycle embedded in column names). This transformation is necessary because Pipeline 6 needs all cycle data simultaneously to perform cross-cycle alignment.</li> </ul> </li> <li>Output Files:<ol> <li>Illumination-corrected and aligned images for each cycle, channel, and site</li> <li>CSV files with measurements</li> </ol> </li> <li>Output Directory:<ul> <li><code>{Batch}/images_aligned/barcoding/{Plate}-{Well}-{Site}/</code></li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_Cycle{K}_{Channel}.tiff</code>: Illumination corrected and aligned image</li> </ul> </li> <li>CSV Naming Pattern:<ul> <li><code>BarcodingApplication_Image.csv</code></li> <li><code>BarcodingApplication_Experiment.csv</code></li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-7-barcoding-preprocessing","title":"Pipeline 7: Barcoding Preprocessing","text":"<ul> <li>Input Images: Aligned Barcoding images from Pipeline 6</li> <li>LoadData CSV Fields:<ul> <li><code>FileName_Cycle{K}_{Channel}</code>, <code>PathName_Cycle{K}_{Channel}</code> (aligned barcoding images; where Channel = DAPI, A, C, G, T; and K = 1...N, except for DAPI where K = 1).</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> <li>Note: <code>DNA</code> channel is called <code>DAPI</code> here to avoid name clash later when Cell Painting channels are brought it.</li> </ul> </li> <li>Output Files:<ol> <li>Processed barcoding images with color compensation, background correction, etc.</li> <li>CSV files with barcode calling results</li> <li>Overlay images showing identified foci</li> </ol> </li> <li>Output Directory:<ul> <li><code>{Batch}/images_corrected/barcoding/{Plate}-{Well}-{Site}/overlay/</code>: Overlay image</li> <li><code>{Batch}/images_corrected/barcoding/{Plate}-{Well}-{Site}/</code>: Everything else</li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_Cycle{K}_{Channel}.tiff</code>: Processed image</li> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_StDev_Overlay.png</code>: Overlay image</li> </ul> </li> <li>CSV Naming Pattern:<ul> <li><code>BarcodePreprocessing_BarcodeFoci.csv</code></li> <li><code>BarcodePreprocessing_Image.csv</code></li> <li><code>BarcodePreprocessing_Experiment.csv</code></li> <li><code>BarcodePreprocessing_Nuclei.csv</code></li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-8-barcoding-stitching-and-cropping_1","title":"Pipeline 8: Barcoding Stitching and Cropping","text":"<ul> <li>Input Images: Processed Barcoding images from Pipeline 7</li> <li>FIJI Script Parameters: Same as Pipeline 4, with these key differences:<ul> <li><code>subdir</code>: Points to barcoding images (e.g., <code>{batch}/images_corrected/barcoding</code>)</li> <li><code>channame</code>: Uses \"DAPI\" instead of \"DNA\"</li> <li><code>rows</code>, <code>columns</code>, <code>imperwell</code>: Uses barcoding grid layout parameters</li> <li><code>scalingstring</code>: Set to \"1.99\" (vs. \"1\" for Cell Painting)</li> </ul> </li> <li>Output Files:<ol> <li>Stitched whole-well images for each channel and cycle</li> <li>Cropped tiles from stitched images</li> <li>Downsampled (10x) previews of stitched images</li> </ol> </li> <li>Output Directories:<ul> <li><code>{Batch}/images_corrected_stitched/barcoding/{Plate}/{Plate}_{Well}/</code>: Stitched whole-well images</li> <li><code>{Batch}/images_corrected_cropped/barcoding/{Plate}/{Plate}_{Well}/{Channel}/</code>: Cropped tile images</li> <li><code>{Batch}/images_corrected_stitched_10X/barcoding/{Plate}/{Plate}_{Well}/Cycle{Cycle}_{bc_channel}/</code>: Downsampled previews</li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Stitched_Cycle{K}_{Channel}.tiff</code>: Stitched whole-well image (for square wells)</li> <li><code>StitchedTopLeft_Cycle{K}_{Channel}.tiff</code>, etc.: Stitched quadrant images (for round wells)</li> <li><code>Cycle{K}_{Channel}_Site_{TileNumber}.tiff</code>: Cropped tile image (includes cycle information)</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#final-analysis-pipeline_1","title":"Final Analysis Pipeline","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-9-combined-analysis","title":"Pipeline 9: Combined Analysis","text":"<ul> <li>Input Images:<ul> <li>Cropped Cell Painting tiles from Pipeline 4</li> <li>Cropped Barcoding tiles from Pipeline 8</li> </ul> </li> <li>LoadData CSV Fields:<ul> <li><code>FileName_{Channel}</code>, <code>PathName_{Channel}</code> (cropped Cell Painting images, where Channel = DNA, Phalloidin, etc.)</li> <li><code>FileName_Cycle{K}_{Channel}</code>, <code>PathName_Cycle{K}_{Channel}</code> (cropped barcoding images; where Channel = DAPI, A, C, G, T; and K = 1...N, except for DAPI where K = 1).</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code></li> </ul> </li> <li>Additional Input:<ul> <li><code>Barcodes.csv</code>: Contains reference barcode sequences for calling. Must include two case-sensitive columns: <code>sgRNA</code> (barcode sequences) and <code>gene_symbol</code> (gene names)</li> <li>Output Files:<ol> <li>CSV files with measurements</li> <li>Segmentation mask images</li> <li>Overlay images showing segmentation and barcode foci</li> </ol> </li> </ul> </li> <li>Output Directories:<ul> <li><code>{Batch}/workspace/analysis/{Plate}-{Well}-{Site}/</code>: CSV and overlay</li> <li><code>{Batch}/workspace/analysis/{Plate}-{Well}-{Site}/segmentation_masks/</code>: segmentation mask</li> </ul> </li> <li>CSV Naming Pattern:<ul> <li><code>BarcodeFoci.csv</code></li> <li><code>Cells.csv</code></li> <li><code>ConfluentRegions.csv</code></li> <li><code>Cytoplasm.csv</code></li> <li><code>Experiment.csv</code></li> <li><code>Foci.csv</code></li> <li><code>Foci_NonCellEdge.csv</code></li> <li><code>Foci_PreMask.csv</code></li> <li><code>Image.csv</code></li> <li><code>Nuclei.csv</code></li> <li><code>PreCells.csv</code></li> <li><code>RelateObjects.csv</code></li> <li><code>Resize_Foci.csv</code></li> </ul> </li> <li>Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_{ObjectType}_Objects.tiff</code>: Segmentation mask</li> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_CorrCh{ChannelNumber}_Overlay.png</code>: Overlay image</li> <li><code>Plate_{Plate}_Well_{Well}_Site_{Site}_CorrCh{ChannelNumber}_SpotOverlay.png</code>: Overlay image</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#special-purpose-pipeline-outputs","title":"Special-Purpose Pipeline Outputs","text":""},{"location":"developer/legacy/pcpip-specs/#pipeline-7a-barcoding-preprocessing-troubleshooting","title":"Pipeline 7A: Barcoding Preprocessing Troubleshooting","text":"<ul> <li>Similar to Pipeline 7 but with additional diagnostic outputs:</li> <li><code>BarcodePreprocessingTroubleshoot_Foci.csv</code>: Enhanced foci metrics</li> <li>Additional overlay images with more detailed visualization</li> <li>Histogram plots of intensity distributions</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-8y-barcoding-alignment-check","title":"Pipeline 8Y: Barcoding Alignment Check","text":"<ul> <li>Output Files:</li> <li>Alignment verification images showing overlay of Cell Painting and Barcoding channels</li> <li>CSV files with cross-modality alignment metrics</li> <li>Example: <code>AlignmentCheck_Plate1_A01_Site_1.png</code></li> </ul>"},{"location":"developer/legacy/pcpip-specs/#pipeline-8z-stitching-of-aligned-images","title":"Pipeline 8Z: Stitching of Aligned Images","text":"<ul> <li>Similar to Pipeline 8 but operating on aligned images directory rather than corrected images</li> <li>Output naming follows the same pattern as Pipeline 8</li> </ul>"},{"location":"developer/legacy/pcpip-specs/#quality-control-steps-and-scripts","title":"Quality Control Steps and Scripts","text":"<p>Each pipeline stage includes specific quality control measures to ensure data quality and processing accuracy. The following scripts provide visualization and analysis tools for QC:</p> <p>Location: <code>pcpip-notebooks</code></p> <p>NOTE: These notebooks are temporarily located in <code>/docs/developer/../assets/pcpip-notebooks/</code> but should be moved to <code>/examples/notebooks/</code> in the future.</p>"},{"location":"developer/legacy/pcpip-specs/#qc-scripts-and-their-applications","title":"QC Scripts and Their Applications","text":"<ol> <li><code>illum_montage.py</code> (Not currently implemented)<ul> <li>Purpose: Visualizes illumination correction functions across plates</li> <li>Functionality:<ul> <li>Creates montages of illumination correction functions (.npy files)</li> <li>Shows correction patterns for each channel</li> <li>Compares correction functions across plates</li> <li>Highlights potential issues in illumination correction</li> </ul> </li> <li>Used in:<ul> <li>Pipeline 1 (Cell Painting Illumination Correction): Validates illumination correction functions for Cell Painting channels</li> <li>Pipeline 5 (Barcoding Illumination Correction): Validates illumination correction functions for barcoding channels</li> </ul> </li> <li>QC Focus: Ensures consistent and appropriate illumination correction</li> <li>QC Metrics implemented:<ul> <li>None</li> </ul> </li> <li>QC performed by expert:<ul> <li>Compare correction patterns across channels</li> <li>Check for uniform illumination patterns</li> <li>Verify no extreme variations in correction functions</li> <li>Identify potential plate-specific issues</li> </ul> </li> </ul> </li> <li><code>make_fiji_montages_std.py</code><ul> <li>Purpose: Creates visual montages of segmentation check results</li> <li>Functionality:<ul> <li>Organizes PNG images by plate</li> <li>Creates a grid montage</li> <li>Uses FIJI's montage tool with appropriate scaling and borders</li> <li>Saves output as TIFF files named after each plate</li> </ul> </li> <li>Used in:<ul> <li>Pipeline 3 (Segmentation Check): Visual assessment of segmentation quality across wells</li> </ul> </li> <li>QC Focus: Visual assessment of segmentation quality across wells</li> <li>QC Metrics implemented:<ul> <li>None</li> </ul> </li> <li>QC performed by expert:<ul> <li>Visual inspection of segmentation across wells</li> <li>Check for consistent segmentation patterns</li> <li>Verify no systematic failures</li> </ul> </li> </ul> </li> <li><code>Visualize_stitched_images.py</code><ul> <li>Purpose: Validates stitching quality for both Cell Painting and barcoding images</li> <li>Functionality:<ul> <li>Creates 2x2 grid visualization of stitched quadrants</li> <li>Shows TopLeft, TopRight, BottomLeft, BottomRight sections</li> <li>Uses square root transformation for better contrast</li> <li>Works with 10X downsampled images for efficient QC</li> <li>Removes axis ticks for cleaner visualization</li> </ul> </li> <li>Used in:<ul> <li>Pipeline 4 (Cell Painting Stitching): Validates stitching quality for Cell Painting images, specifically examining 10X downscaled whole-well stitches of the DAPI channel only</li> <li>Pipeline 8 (Barcoding Stitching): Validates stitching quality for barcoding images, specifically examining 10X downscaled whole-well stitches of the DAPI channel across all cycles</li> </ul> </li> <li>QC Focus: Verifies proper stitching and alignment of image quadrants</li> <li>QC Metrics implemented:<ul> <li>None</li> </ul> </li> <li>QC performed by expert:<ul> <li>Verify quadrant alignment</li> <li>Check for stitching artifacts</li> <li>Validate image continuity</li> </ul> </li> </ul> </li> <li><code>6_Barcode_Align.py</code><ul> <li>Purpose: Validates alignment between barcoding cycles</li> <li>Functionality:<ul> <li>Analyzes pixel shifts between cycles</li> <li>Calculates correlation scores between cycles</li> <li>Creates visualizations of:<ul> <li>Pixel shifts (with -200 to 200 pixel limits)</li> <li>Correlation scores (with 0.9 threshold line)</li> </ul> </li> <li>Provides statistical analysis:<ul> <li>Counts of sites with large shifts (&gt;50 pixels)</li> <li>Sites with poor correlations (&lt;0.9, &lt;0.8)</li> <li>Spatial distribution of alignment issues</li> </ul> </li> </ul> </li> <li>Used in:<ul> <li>Pipeline 6 (Barcoding Alignment): Ensures proper alignment between barcoding cycles</li> </ul> </li> <li>QC Focus: Ensures proper alignment between barcoding cycles</li> <li>QC Metrics implemented:<ul> <li>See detailed specifications in QC Input Output Specifications section</li> </ul> </li> <li>QC performed by expert:<ul> <li>Monitor spatial distribution of alignment issues</li> </ul> </li> </ul> </li> <li><code>7_BarcodePreprocessing.py</code><ul> <li>Purpose: Analyzes barcode preprocessing quality</li> <li>Functionality:<ul> <li>Analyzes barcode library composition:<ul> <li>Nucleotide frequency by cycle</li> <li>Repeat sequence analysis (5-7 nucleotide repeats)</li> </ul> </li> <li>Evaluates barcode calling quality:<ul> <li>Perfect match percentages</li> <li>Score distributions</li> <li>Per-well performance visualization</li> </ul> </li> <li>Creates spatial visualizations:<ul> <li>Per-plate heatmaps of barcode quality</li> <li>Nucleotide frequency plots across cycles</li> </ul> </li> </ul> </li> <li>Used in:<ul> <li>Pipeline 7 (Barcoding Preprocessing): Validates barcode detection and calling accuracy</li> </ul> </li> <li>QC Focus: Validates barcode detection and calling accuracy</li> <li>QC Metrics implemented:<ul> <li>See detailed specifications in QC Input Output Specifications section</li> </ul> </li> <li>QC performed by expert:<ul> <li>Inspect the metrics</li> </ul> </li> </ul> </li> </ol>"},{"location":"developer/legacy/pcpip-specs/#qc-input-output-specifications","title":"QC Input Output Specifications","text":""},{"location":"developer/legacy/pcpip-specs/#6_barcode_alignpy","title":"<code>6_Barcode_Align.py</code>","text":"<ul> <li>Input CSV Files: CSV output files from Pipeline 6 (Barcoding Illumination Application and Alignment)</li> <li>Input Directory:<ul> <li><code>{Batch}/images_aligned/barcoding/</code></li> </ul> </li> <li>Input CSV Names:<ul> <li><code>BarcodingApplication_Image.csv</code></li> </ul> </li> <li>Required CSV Fields:<ul> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code>: Image identifiers</li> <li><code>Align_Xshift_Cycle{N}_DAPI</code>, <code>Align_Yshift_Cycle{N}_DAPI</code>: Pixel shift values for DAPI alignment (where N = 2...M for all cycles)</li> <li><code>Correlation_Correlation_Cycle{N}_DAPI_Cycle{M}_DAPI</code>: Correlation scores between cycle pairs (where N, M = 1...total cycles)</li> </ul> </li> <li>Configuration Parameters:<ul> <li><code>numcycles</code>: Number of barcoding cycles in the experiment</li> <li><code>imperwell</code>: Number of images per well</li> <li><code>row_widths</code>: Array defining the circular acquisition pattern layout</li> </ul> </li> <li>Analysis Outputs: (displayed interactively, not saved as files)<ol> <li>Catplots of pixel shifts between cycles</li> <li>Catplots of correlation scores between cycles</li> <li>Statistics on sites with large shifts (&gt;50 pixels)</li> <li>Spatial visualization of alignment issues</li> <li>Lists of problematic sites (poor correlation or extreme shifts)</li> </ol> </li> <li>QC Thresholds:<ul> <li>Pixel shifts: Should be within \u00b1200 pixels range</li> <li>Correlation scores: Should be &gt;0.9 (0.8 minimum threshold)</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#7_barcodepreprocessingpy","title":"<code>7_BarcodePreprocessing.py</code>","text":"<ul> <li>Input CSV Files:<ol> <li>CSV output files from Pipeline 7 (Barcoding Preprocessing)</li> <li>Reference barcode library file</li> </ol> </li> <li>Input Directories:<ul> <li><code>{Batch}/images_corrected/barcoding/</code>: For Pipeline 7 outputs</li> <li>Reference directory for barcode library (configurable)</li> </ul> </li> <li>Input CSV Names:<ul> <li><code>BarcodePreprocessing_Foci.csv</code>: Contains barcode calling results</li> <li><code>Barcodes.csv</code>: Reference file containing expected barcode sequences and gene annotations</li> </ul> </li> <li>Required CSV Fields:<ul> <li>From BarcodePreprocessing_Foci.csv:<ul> <li><code>ImageNumber</code>, <code>ObjectNumber</code>: Object identifiers</li> <li><code>Metadata_Plate</code>, <code>Metadata_Well</code>, <code>Metadata_Site</code>, <code>Metadata_Well_Value</code>: Image identifiers</li> <li><code>Barcode_BarcodeCalled</code>: Raw called barcode sequence</li> <li><code>Barcode_MatchedTo_Barcode</code>: Best matched reference barcode</li> <li><code>Barcode_MatchedTo_GeneCode</code>: Gene name for matched barcode</li> <li><code>Barcode_MatchedTo_ID</code>: Barcode identifier</li> <li><code>Barcode_MatchedTo_Score</code>: Match quality score (0-1)</li> </ul> </li> <li>From Barcodes.csv:<ul> <li><code>sgRNA</code>: Barcode sequence</li> <li><code>Gene</code>: Gene name/symbol</li> </ul> </li> </ul> </li> <li>Configuration Parameters:<ul> <li><code>BATCH_ID</code>: Batch identifier</li> <li><code>numcycles</code>: Number of barcoding cycles in the experiment</li> <li><code>imperwell</code>: Number of images per well</li> <li><code>row_widths</code>: Array defining the circular acquisition pattern layout</li> </ul> </li> <li>Analysis Outputs: (displayed interactively, not saved as files)<ol> <li>Barcode library analysis:<ul> <li>Nucleotide frequency by position plots</li> <li>Statistics on repeat sequences (5-7 nucleotide repeats)</li> </ul> </li> <li>Barcode calling quality:<ul> <li>Perfect match percentage overall and by well</li> <li>Score distribution histograms (overall and per-well)</li> <li>Per-cycle nucleotide frequency analysis</li> <li>Mismatch cycle identification for near-matches</li> </ul> </li> <li>Spatial analysis:<ul> <li>Heatmaps showing percent perfect barcodes by well and site</li> <li>Spatial distribution of quality metrics</li> </ul> </li> <li>Gene/barcode coverage:<ul> <li>Statistics on gene and barcode detection coverage</li> <li>Lists of most frequently detected genes and barcodes</li> </ul> </li> </ol> </li> <li>QC Metrics Evaluated:<ul> <li>Percentage of perfect barcode matches (Score = 1)</li> <li>Distribution and patterns of barcode calling quality</li> <li>Nucleotide frequency comparison between expected and observed</li> <li>Cycle-specific error patterns</li> <li>Gene and barcode coverage metrics</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-specs/#make_fiji_montages_stdpy","title":"<code>make_fiji_montages_std.py</code>","text":"<ul> <li>Input Image Files: PNG overlay images from Pipeline 3 (Segmentation Check)</li> <li>Input Directory:<ul> <li><code>{Batch}/images_segmentation/</code>: Root directory containing segmentation check results</li> <li>Subdirectories follow pattern <code>{Plate}-{Well}/</code></li> </ul> </li> <li>Input Image Naming Pattern:<ul> <li><code>Plate_{Plate}_Well_Well{Well}_Site_{Site}_Corr{Channel}_SegmentCheck.png</code>: Segmentation overlay images</li> </ul> </li> <li>Script Parameters:<ul> <li><code>topdir</code>: Root directory for processing (e.g., C:\\Users\\Administrator\\Desktop\\assaydev)</li> <li><code>ncols</code>: Number of columns in the montage grid (10 for 96-well plates, 24 for 384-well plates)</li> <li><code>nrows</code>: Number of rows in the montage grid (6 for 96-well plates, 16 for 384-well plates)</li> <li><code>scale</code>: Scaling factor for images in the montage (default: 0.75)</li> <li><code>border</code>: Border width between images in pixels (default: 1)</li> </ul> </li> <li>Processing Steps:<ol> <li>Reorganizes images by plate (moves images from well folders to plate folders)</li> <li>For each plate folder:<ul> <li>Loads all PNG images in sequence</li> <li>Creates a montage grid with configured dimensions</li> <li>Saves as a single TIFF file</li> </ul> </li> </ol> </li> <li>Output Files:<ul> <li>Single TIFF montage per plate showing segmentation results across wells</li> </ul> </li> <li>Output Directory:<ul> <li>Same as input root directory</li> </ul> </li> <li>Output Naming Pattern:<ul> <li><code>{Plate}.tif</code>: Montage file named after the plate</li> </ul> </li> <li>QC Purpose:<ul> <li>Enables visual assessment of segmentation quality across entire plates</li> <li>Allows quick identification of problematic wells or patterns in segmentation</li> <li>Provides overview of experiment quality for expert review</li> </ul> </li> </ul>"},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/","title":"CLAUDE.md","text":"<p>This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.</p>"},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/#starrynight-pcpip-pipelines-reference-collection","title":"StarryNight PCPIP-Pipelines Reference Collection","text":""},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/#purpose","title":"Purpose","text":"<p>This folder contains a curated collection of CellProfiler pipeline files that serve as references for the PCPIP workflow. While located in the tests directory, these are primarily reference pipelines that may be used as test fixtures for the StarryNight pipeline parser and validation tools.</p>"},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/#commands","title":"Commands","text":"<ul> <li>Compare pipelines: <code>diff -w pipeline1.cppipe pipeline2.cppipe &gt; comparison.diff</code></li> <li>Visualize pipelines: Use cp_graph tool (see README.md)</li> </ul>"},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>ref_*.cppipe</code>: Reference pipeline files used for testing</li> <li><code>_pcpip_12cycles/</code>: Original 12-cycle pipelines from PCPIP</li> <li><code>_refsource/</code>: Source pipeline variants with comparison diffs</li> <li><code>_ref_graph_format/</code>: Pipeline visualizations (JSON, DOT, SVG, PNG)</li> </ul>"},{"location":"developer/legacy/pcpip-pipelines/CLAUDE/#working-with-reference-pipelines","title":"Working with Reference Pipelines","text":"<ul> <li>These files are carefully selected reference implementations of the PCPIP workflow</li> <li>Reference pipelines document important pipeline configurations and variations</li> <li>When comparing pipelines, use <code>diff -w</code> to ignore whitespace differences</li> <li>Any modifications should be documented in commit messages</li> <li>Visualizations help understand pipeline structure without requiring CellProfiler</li> </ul>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/","title":"PCPIP Reference Pipelines","text":"<p>Reference Pipeline Maintenance</p> <p>This directory contains legacy reference pipelines that are no longer actively maintained.</p> <p>For current reference pipelines, use: <code>starrynight/src/starrynight/templates/cppipe/</code></p> <ul> <li>The templates directory is the single source of truth for production pipeline templates</li> <li>This test directory was synchronized one final time on 2025-01-06 for historical consistency</li> <li>Future updates will only be made to the templates directory</li> <li>This directory remains useful for historical reference and specialized test fixtures</li> </ul> <p>A curated collection of CellProfiler pipelines that serve as references for the Pooled Cell Painting Image Processing (PCPIP) workflow.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#introduction","title":"Introduction","text":"<p>This repository contains carefully selected reference implementations of CellProfiler pipelines for the PCPIP workflow. These reference pipelines document standard configurations and variations for processing pooled cell painting images, and may be used as test fixtures for the StarryNight pipeline parser and validation tools.</p> <p>For full context, see the original PCPIP repository pipelines.</p> <p>Note on Pipeline Structures: These pipelines have been reviewed as suitable test fixtures by the PCPIP workflow experts. While some pipelines use unusual approaches to object identification and handling (particularly in pipelines 3, 7, and 9), these aspects don't affect their suitability as test fixtures and reference implementations.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#directory-structure","title":"Directory Structure","text":"<p>Directory structure of https://github.com/broadinstitute/starrynight/tree/main/docs/developer/legacy/pcpip-pipelines:</p> Directory Description <code>/</code> (root) Reference pipeline files (prefixed with <code>ref_</code>) <code>_pcpip_12cycles/</code> Original 12-cycle pipelines from PCPIP <code>_refsource/</code> Source pipeline variants with comparison diffs <code>_ref_graph_format/</code> Pipeline visualizations (JSON, DOT, SVG, PNG)"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#reference-pipelines-overview","title":"Reference Pipelines Overview","text":"<p>The PCPIP workflow consists of the following pipeline stages, represented by the reference pipelines in this repository:</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#1-cp_illum-ref_1_cp_illumcppipe","title":"1. CP_Illum (<code>ref_1_CP_Illum.cppipe</code>)","text":"<p>Creates illumination correction functions for all cycles and channels. This is the first step in the workflow, which calculates illumination correction functions used in subsequent steps.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#2-cp_apply_illum-ref_2_cp_apply_illumcppipe","title":"2. CP_Apply_Illum (<code>ref_2_CP_Apply_Illum.cppipe</code>)","text":"<p>Applies illumination correction functions to all cycles and channels. This pipeline corrects for uneven illumination in the raw images.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#3-cp_segmentationcheck-ref_3_cp_segmentationcheckcppipe","title":"3. CP_SegmentationCheck (<code>ref_3_CP_SegmentationCheck.cppipe</code>)","text":"<p>Quality control step to verify cell segmentation. This pipeline identifies primary objects (nuclei) and checks segmentation quality. Note: This pipeline uses two <code>IdentifySecondaryObjects</code> modules for PreCells =&gt; Cells, which is an unusual approach but doesn't impact its effectiveness as a test fixture.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#5-bc_illum-ref_5_bc_illumcppipe","title":"5. BC_Illum (<code>ref_5_BC_Illum.cppipe</code>)","text":"<p>Creates barcode-specific illumination correction functions. This pipeline calculates illumination correction specifically for barcode images.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#6-bc_apply_illum-ref_6_bc_apply_illumcppipe","title":"6. BC_Apply_Illum (<code>ref_6_BC_Apply_Illum.cppipe</code>)","text":"<p>Applies illumination correction to barcode cycle images across all channels (DAPI, A, C, G, T). This pipeline not only corrects for uneven illumination but also performs image alignment between cycles to ensure proper barcode reading.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#7-bc_preprocess-ref_7_bc_preprocesscppipe","title":"7. BC_Preprocess (<code>ref_7_BC_Preprocess.cppipe</code>)","text":"<p>Preprocesses barcode images for analysis. This pipeline prepares barcode images for subsequent analysis steps. Note: Contains some non-standard object handling approaches that don't affect outputs.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#9-analysis-ref_9_analysiscppipe","title":"9. Analysis (<code>ref_9_Analysis.cppipe</code>)","text":"<p>Performs cellular analysis measurements. This pipeline identifies cells, measures features, and exports results. Similar to Pipeline 3, this uses some unusual object identification approaches that are appropriate for test fixtures but not necessarily representative of common workflows.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#specialized-pipeline-variants","title":"Specialized Pipeline Variants","text":"<p>Some specialized variants demonstrate specific processing requirements:</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#6_bc_apply_illum_debrismask","title":"6_BC_Apply_Illum_DebrisMask","text":"<p>A specialized variant of pipeline 6 developed for datasets with significant debris that requires masking. This pipeline demonstrates the extensibility requirements for StarryNight, showing how specialized processing steps can be swapped in to address dataset-specific challenges. While valuable as a reference for extensibility design, it is not required as a standard test fixture. See PCPIP <code>12cycle</code> version.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#7a_bc_preprocess_troubleshooting","title":"7A_BC_Preprocess_Troubleshooting","text":"<p>A variant of pipeline 7 created for testing multiple CompensateColors parameter settings within a single pipeline execution. If StarryNight successfully implements module iteration capabilities, pipeline 7A becomes largely redundant. The 7A pipeline was created as a workaround for testing multiple parameters within a single pipeline execution. A truly modular system would allow iterative parameter testing without requiring specialized pipeline variants. See PCPIP <code>12cycle</code> version.</p>"},{"location":"developer/legacy/pcpip-pipelines/pcpip-pipelines/#visualization-resources","title":"Visualization Resources","text":"<p>The <code>_ref_graph_format/</code> directory contains pipeline visualizations to aid in understanding pipeline structure:</p> <ul> <li>JSON files: Pipeline data exported from CellProfiler 4.2.8</li> <li>DOT files: Graph definitions for pipeline structure</li> <li>SVG/PNG files: Visual representations of pipeline workflows</li> </ul> <p>These visualizations help understand pipeline structure without requiring CellProfiler installation.</p> <p>Reference Pipeline Update 2025-01-06</p> <p>The reference pipelines were updated to match the production template versions from <code>starrynight/src/starrynight/templates/cppipe/</code>. Key changes include:</p> <p>Metadata tag standardization:</p> <ul> <li><code>ref_5_BC_Illum.cppipe</code>: Changed from <code>SBSCycle</code> to <code>Cycle</code> metadata tag</li> <li><code>ref_6_BC_Apply_Illum.cppipe</code> and <code>ref_7_BC_Preprocess.cppipe</code>: Changed from <code>Well_Value</code> to <code>Well</code> metadata tag</li> </ul> <p>Channel naming consistency:</p> <ul> <li><code>ref_6_BC_Apply_Illum.cppipe</code> and <code>ref_7_BC_Preprocess.cppipe</code>: Changed from <code>DAPI</code> to <code>DNA</code> for DNA channel naming</li> </ul> <p>These changes ensure consistency between test fixtures and production templates, facilitating better validation and testing workflows.</p> <p>Pipeline Selection Details</p> <p>Pipeline variants in <code>_refsource</code> are from <code>s3://BUCKET/projects/PROJECT/workspace/pipelines/BATCH</code>. These variants were compared to select the most appropriate reference implementations.</p> <pre><code>cd _refsource\ndiff 1_CP_Illum/1_CP_Illum.cppipe 1_CP_Illum/1_Illum_Plate1_Plate2.cppipe &gt; 1_CP_Illum/1_CP_Illum__1_Illum_Plate1_Plate2.diff\ndiff 2_CP_Apply_Illum/2_CP_Apply_Illum.cppipe 2_CP_Apply_Illum/2_CP_Apply_Illum_Plate3_Plate4.cppipe &gt; 2_CP_Apply_Illum/2_CP_Apply_Illum__2_CP_Apply_Illum_Plate3_Plate4.diff\ndiff 3_CP_SegmentationCheck/3_CP_SegmentationCheck_Plate3_Plate4.cppipe 3_CP_SegmentationCheck/3_CP_SegmentationCheck_Plate1_Plate2.cppipe &gt; 3_CP_SegmentationCheck/3_CP_SegmentationCheck_Plate3_Plate4__3_CP_SegmentationCheck_Plate1_Plate2.diff\ndiff 5_BC_Illum/5_BC_Illum.cppipe 5_BC_Illum/5_BC_Illum_byWell.cppipe &gt; 5_BC_Illum/5_BC_Illum__5_BC_Illum_byWell.diff\ndiff 7_BC_Preprocess/7_BC_Preprocess.cppipe 7_BC_Preprocess/7_BC_Preprocess_4.cppipe &gt; 7_BC_Preprocess/7_BC_Preprocess__7_BC_Preprocess_4.diff\ndiff 9_Analysis/9_Analysis.cppipe 9_Analysis/9_Analysis_Plate1_Plate2.cppipe &gt; 9_Analysis/9_Analysis__9_Analysis_Plate1_Plate2.diff\ndiff 9_Analysis/9_Analysis_foci.cppipe 9_Analysis/9_Analysis_Plate1_Plate2.cppipe &gt; 9_Analysis/9_Analysis_foci__9_Analysis_Plate1_Plate2.diff\ndiff 9_Analysis/9_Analysis_rerun.cppipe 9_Analysis/9_Analysis_Plate1_Plate2.cppipe &gt; 9_Analysis/9_Analysis_rerun__9_Analysis_Plate1_Plate2.diff\ncd -\n</code></pre> <p>After reviewing the diffs, these pipeline variants were selected:</p> <ul> <li><code>1_CP_Illum</code>: <code>1_Illum_Plate1_Plate2.cppipe</code></li> <li><code>2_CP_Apply_Illum</code>: <code>2_CP_Apply_Illum.cppipe</code></li> <li><code>3_CP_SegmentationCheck</code>: <code>3_CP_SegmentationCheck_Plate1_Plate2.cppipe</code></li> <li><code>5_BC_Illum</code>: <code>5_BC_Illum.cppipe</code></li> <li><code>6_BC_Apply_Illum</code>: <code>6_BC_Apply_Illum.cppipe</code></li> <li><code>7_BC_Preprocess</code>: <code>7_BC_Preprocess.cppipe</code></li> <li><code>9_Analysis</code>: <code>9_Analysis_Plate1_Plate2.cppipe</code></li> </ul> <p>Reference Pipelines Creation</p> <p>The selected pipeline variants were initially copied to create the reference pipelines:</p> <pre><code>cp _refsource/1_CP_Illum/1_Illum_Plate1_Plate2.cppipe ref_1_CP_Illum.cppipe\ncp _refsource/2_CP_Apply_Illum/2_CP_Apply_Illum.cppipe ref_2_CP_Apply_Illum.cppipe\ncp _refsource/3_CP_SegmentationCheck/3_CP_SegmentationCheck_Plate1_Plate2.cppipe ref_3_CP_SegmentationCheck.cppipe\ncp _refsource/5_BC_Illum/5_BC_Illum.cppipe ref_5_BC_Illum.cppipe\ncp _refsource/6_BC_Apply_Illum/6_BC_Apply_Illum.cppipe ref_6_BC_Apply_Illum.cppipe\ncp _refsource/7_BC_Preprocess/7_BC_Preprocess.cppipe ref_7_BC_Preprocess.cppipe\ncp _refsource/9_Analysis/9_Analysis_Plate1_Plate2.cppipe ref_9_Analysis.cppipe\n</code></pre> <p>The reference pipelines were modified over time to:</p> <ol> <li>Drop cycles 4-10</li> <li>Replace <code>RunCellPose</code> with <code>IdentifyPrimaryObjects</code></li> <li>Synchronize with production template versions (2025-01-06)</li> </ol> <p>You can view the commit history of specific pipeline files using GitHub, for example: <pre><code>git log --follow -- tests/pcpip-pipelines/ref_1_CP_Illum.cppipe\n</code></pre></p> <p>Or view the history through GitHub's interface by clicking on a file and then selecting \"History\" or \"Blame\".</p> <p>Pipeline Comparison Details</p> <p>Comparisons between reference pipeline sources and PCPIP 12-cycle pipelines were conducted using the following commands:</p> <pre><code>mkdir -p _pcpip_12cycles/diff\n\nrefsource_1=\"_refsource/1_CP_Illum/1_Illum_Plate1_Plate2.cppipe\"\npcpip_1=\"_pcpip_12cycles/1_CP_Illum.cppipe\"\ndiff_1=\"_pcpip_12cycles/diff/1_CP_Illum\"\ndiff -w ${refsource_1} ${pcpip_1} &gt; ${diff_1}.diff\n\nrefsource_2=\"_refsource/2_CP_Apply_Illum/2_CP_Apply_Illum.cppipe\"\npcpip_2=\"_pcpip_12cycles/2_CP_Apply_Illum.cppipe\"\ndiff_2=\"_pcpip_12cycles/diff/2_CP_Apply_Illum\"\ndiff -w ${refsource_2} ${pcpip_2} &gt; ${diff_2}.diff\n\nrefsource_3=\"_refsource/3_CP_SegmentationCheck/3_CP_SegmentationCheck_Plate1_Plate2.cppipe\"\npcpip_3=\"_pcpip_12cycles/3_CP_SegmentationCheck.cppipe\"\ndiff_3=\"_pcpip_12cycles/diff/3_CP_SegmentationCheck\"\ndiff -w ${refsource_3} ${pcpip_3} &gt; ${diff_3}.diff\n\nrefsource_5=\"_refsource/5_BC_Illum/5_BC_Illum.cppipe\"\npcpip_5=\"_pcpip_12cycles/5_BC_Illum.cppipe\"\ndiff_5=\"_pcpip_12cycles/diff/5_BC_Illum\"\ndiff -w ${refsource_5} ${pcpip_5} &gt; ${diff_5}.diff\n\nrefsource_6=\"_refsource/6_BC_Apply_Illum/6_BC_Apply_Illum.cppipe\"\npcpip_6=\"_pcpip_12cycles/6_BC_Apply_Illum.cppipe\"\ndiff_6=\"_pcpip_12cycles/diff/6_BC_Apply_Illum\"\ndiff -w ${refsource_6} ${pcpip_6} &gt; ${diff_6}.diff\n\nrefsource_7=\"_refsource/7_BC_Preprocess/7_BC_Preprocess.cppipe\"\npcpip_7=\"_pcpip_12cycles/7_BC_Preprocess.cppipe\"\ndiff_7=\"_pcpip_12cycles/diff/7_BC_Preprocess\"\ndiff -w ${refsource_7} ${pcpip_7} &gt; ${diff_7}.diff\n\nrefsource_9=\"_refsource/9_Analysis/9_Analysis_Plate1_Plate2.cppipe\"\npcpip_9=\"_pcpip_12cycles/9_Analysis.cppipe\"\ndiff_9=\"_pcpip_12cycles/diff/9_Analysis\"\ndiff -w ${refsource_9} ${pcpip_9} &gt; ${diff_9}.diff\n</code></pre> <p>This was discussed further in issue #68.</p> <p>Pipeline Visualization Generation</p> <p>Files in <code>_ref_graph_format</code> were created using the following:</p> <ul> <li>JSON files: Exported from CellProfiler 4.2.8</li> <li>DOT files: Generated using cp_graph tool</li> <li>SVG/PNG files: Generated from DOT files using Graphviz</li> </ul> <pre><code>cd _ref_graph_format/\nrm -rf dot dotmin png svg\nmkdir -p dot dotmin png svg\n\nCP_GRAPH=\"${HOME}/Documents/GitHub/cp_graph/cp_graph.py\"\nROOT_NODES_FILE=root_nodes.txt\nROOT_NODES=$(cat ${ROOT_NODES_FILE}| tr ',' '\\n' | paste -sd \",\" -)\nfind json/ -name \"*.json\" | \\\nparallel uv run --script ${CP_GRAPH} \\\n  {} \\\n  dot/{/.}.dot \\\n  --rank-nodes \\\n  --remove-unused-data \\\n  --exclude-module-types=ExportToSpreadsheet \\\n  --rank-ignore-filtered \\\n  --root-nodes=${ROOT_NODES}  # --highlight-filtered\n\nfind dot -name \"*.dot\" | parallel dot -Gdpi=50 -Tpng {} -o png/{/.}.png\n\nfind dot -name \"*.dot\" | parallel dot -Tsvg {} -o svg/{/.}.svg\n\nfind json/ -name \"*.json\" | \\\nparallel uv run --script ${CP_GRAPH} \\\n  {} \\\n  dotmin/{/.}.dot \\\n  --remove-unused-data \\\n  --exclude-module-types=ExportToSpreadsheet \\\n  --root-nodes=${ROOT_NODES} \\\n  --ultra-minimal # --highlight-filtered\n</code></pre>"},{"location":"developer/roadmap/aws-deployment-roadmap/","title":"AWS Deployment Roadmap","text":"<p>This roadmap outlines our 8-week plan to deploy StarryNight on AWS, transitioning from local servers to cloud infrastructure.</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#aws-deployment-architecture","title":"AWS Deployment Architecture","text":""},{"location":"developer/roadmap/aws-deployment-roadmap/#core-services","title":"Core Services","text":"<ul> <li>AWS Batch: Orchestrates containerized compute jobs</li> <li>EC2: Hosts login node for job coordination</li> <li>S3: Stores data and results</li> <li>Container Registry: Public registry (GitHub or similar) for images</li> <li>Supporting: CloudWatch (logs), IAM (access), VPC (networking)</li> </ul>"},{"location":"developer/roadmap/aws-deployment-roadmap/#system-topology","title":"System Topology","text":"<p>The AWS deployment follows a hub-and-spoke architecture:</p> <pre><code>flowchart TB\n    subgraph \"Control Plane\"\n        EC2[EC2 Login Node&lt;br/&gt;StarryNight Coordinator]\n        GRAFANA[Grafana Stack&lt;br/&gt;OpenTelemetry Collector]\n    end\n\n    subgraph \"Compute Plane\"\n        BATCH[AWS Batch&lt;br/&gt;Job Queue]\n        COMPUTE[Compute Environment&lt;br/&gt;EC2/Fargate]\n    end\n\n    subgraph \"Storage\"\n        S3[S3 Buckets&lt;br/&gt;Data Lake]\n        REG[Container Registry&lt;br/&gt;GitHub/Public]\n    end\n\n    subgraph \"Infrastructure Management\"\n        PULUMI[Pulumi&lt;br/&gt;IaC]\n    end\n\n    EC2 --&gt;|Submit Jobs| BATCH\n    BATCH --&gt;|Schedule| COMPUTE\n    COMPUTE --&gt;|Read/Write| S3\n    COMPUTE --&gt;|Pull Images| REG\n    COMPUTE --&gt;|Send Telemetry| GRAFANA\n    EC2 &lt;--&gt;|Monitor| GRAFANA\n    PULUMI --&gt;|Provision| EC2 &amp; BATCH &amp; S3</code></pre>"},{"location":"developer/roadmap/aws-deployment-roadmap/#monthly-costs-excluding-computestorage","title":"Monthly Costs (excluding compute/storage)","text":"Service Cost Notes EC2 Login Node $30-40 t3.medium, always-on CloudWatch $10-50 Varies with job volume Pulumi $0-75 Free tier likely sufficient Total $40-165 Plus data transfer costs"},{"location":"developer/roadmap/aws-deployment-roadmap/#8-week-deployment-plan","title":"8-Week Deployment Plan","text":""},{"location":"developer/roadmap/aws-deployment-roadmap/#phase-1-infrastructure-foundation-weeks-1-2","title":"Phase 1: Infrastructure Foundation (Weeks 1-2)","text":"<ol> <li>Configure Pulumi project structure (team has experience from CytoSkel)</li> <li>Define AWS Batch compute environments</li> <li>Create S3 bucket hierarchy</li> <li>Deploy EC2 login node (24/7 coordinator)</li> </ol> <p>Key Unknown: Team has no AWS Batch experience; configuration requirements unclear</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#phase-2-container-pipeline-weeks-3-4","title":"Phase 2: Container Pipeline (Weeks 3-4)","text":"<ol> <li>Select public container registry</li> <li>Set up automated builds triggered by CellProfiler releases</li> <li>Implement independent StarryNight versioning</li> <li>Test build pipeline</li> </ol> <p>Note: Automated builds minimize maintenance burden</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#phase-3-integration-testing-weeks-5-6","title":"Phase 3: Integration Testing (Weeks 5-6)","text":"<ol> <li>Test job submission pipeline</li> <li>Validate Snakemake \u2192 AWS Batch translation</li> <li>Verify telemetry (using StarryNight's built-in system)</li> <li>Run end-to-end workflows</li> </ol> <p>Key Risk: CellProfiler error handling in containerized environment</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#phase-4-production-ready-weeks-7-8","title":"Phase 4: Production Ready (Weeks 7-8)","text":"<ol> <li>Finalize documentation and runbooks</li> <li>Configure monitoring and alerts</li> <li>Complete VPC/security setup</li> <li>Optimize costs</li> </ol> <p>Note: Security hardening deferred (internal users only)</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#configuration-requirements","title":"Configuration Requirements","text":""},{"location":"developer/roadmap/aws-deployment-roadmap/#user-infrastructure-configuration","title":"User Infrastructure Configuration","text":"<p>Based on planning discussions, users configure infrastructure through the UI:</p> <ol> <li>Module Parameter Exposure<ul> <li>Module authors expose parameters like memory and compute requirements</li> <li>Example from discussion: inventory module exposes <code>dataset_path</code> parameter</li> <li>Proposed: Modules could expose <code>memory</code> or similar resource parameters</li> <li>Backend implementation decides how to handle these parameters</li> <li>Note: Specific parameter names and UI interface details TBD</li> </ul> </li> <li>Configuration Flow<ul> <li>UI \u2192 Module \u2192 Pipeline \u2192 AWS Batch job definitions</li> <li>Users cannot directly configure AWS Batch settings</li> <li>Backend determines infrastructure choices (e.g., AWS Batch vs alternatives)</li> </ul> </li> </ol>"},{"location":"developer/roadmap/aws-deployment-roadmap/#job-failure-and-restart-procedures","title":"Job Failure and Restart Procedures","text":"<p>From the planning discussions:</p> <ol> <li>Snakemake Intelligence<ul> <li>Snakemake automatically tracks successful jobs and won't re-run them</li> <li>Only failed or not-yet-run jobs execute on retry</li> <li>Target-based execution model checks for output files</li> </ul> </li> <li>QC Review Points<ul> <li>QC steps implemented as modules that fail by default</li> <li>Human review required before marking as passed</li> <li>After review, job can be manually marked to proceed</li> <li>Note: Specific UI for QC approval TBD</li> </ul> </li> <li>Individual Module Re-execution<ul> <li>Each module can be run independently with different parameters</li> <li>Users can modify parameters and re-run specific modules</li> <li>Logs available for each run attempt</li> </ul> </li> </ol>"},{"location":"developer/roadmap/aws-deployment-roadmap/#partial-failure-recovery","title":"Partial Failure Recovery","text":"<p>Based on the discussion about 90% success / 10% failure scenarios:</p> <ol> <li>Automatic Detection<ul> <li>Snakemake identifies which jobs succeeded vs failed</li> <li>Re-running a pipeline only executes failed jobs</li> <li>Note: Specific mechanism for failure detection not fully detailed</li> </ul> </li> <li>Telemetry and Monitoring<ul> <li>OpenTelemetry integration sends logs to central Grafana stack</li> <li>All stdout/stderr piped through telemetry system</li> <li>Challenge: CellProfiler containers need custom wrappers for proper error reporting</li> </ul> </li> <li>Resource Adjustment<ul> <li>Failed jobs can be retried with adjusted resources</li> <li>Note: UI mechanism for resource adjustment per retry TBD</li> </ul> </li> </ol>"},{"location":"developer/roadmap/aws-deployment-roadmap/#infrastructure-configuration-notes","title":"Infrastructure Configuration Notes","text":"<p>Note: Many specifics remain TBD during implementation.</p> <p>Potential areas requiring configuration:</p> <ul> <li>Network setup (VPC, security groups)</li> <li>S3 access policies</li> <li>IAM permissions</li> <li>Compute preferences (spot vs on-demand)</li> </ul> <p>StarryNight manages job execution; IT retains security/cost control.</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#failure-handling","title":"Failure Handling","text":"<p>Snakemake provides intelligent recovery:</p> <ul> <li>Successful jobs are never re-run</li> <li>Failed jobs can be retried with adjusted resources</li> <li>QC steps pause for manual review via dummy modules</li> </ul>"},{"location":"developer/roadmap/aws-deployment-roadmap/#validation-checklist","title":"Validation Checklist","text":"<ul> <li> 100-job pipeline test</li> <li> Partial failure recovery</li> <li> Container version switching</li> <li> Telemetry completeness</li> <li> Internal user testing</li> </ul>"},{"location":"developer/roadmap/aws-deployment-roadmap/#key-risks","title":"Key Risks","text":"Risk Mitigation AWS Batch complexity Early proof-of-concept Container maintenance Automated builds Cost overruns Monitoring and alerts CellProfiler integration Extensive testing"},{"location":"developer/roadmap/aws-deployment-roadmap/#stakeholder-approval-process","title":"Stakeholder Approval Process","text":"<p>Note: This section requires stakeholder input to define the approval process.</p>"},{"location":"developer/roadmap/aws-deployment-roadmap/#proposed-review-structure-tbd","title":"Proposed Review Structure (TBD)","text":"<ul> <li> Technical review by engineering team</li> <li> Security review by IT/compliance</li> <li> Final approval by project sponsors</li> </ul>"},{"location":"developer/roadmap/aws-deployment-roadmap/#open-questions-for-stakeholders","title":"Open Questions for Stakeholders","text":"<ol> <li>Who are the key stakeholders for approval?</li> <li>What are the approval criteria?</li> <li>What documentation is required for each review?</li> <li>What is the timeline for reviews?</li> </ol> <p>Context from Planning Discussions</p> <p>IT Team Constraints: IT team will likely lack bandwidth to implement custom infrastructure solutions. StarryNight must provide a predefined AWS configuration that IT teams can approve with a simple \"yes/no\" decision, rather than requiring custom backend development.</p> <p>AWS Batch Experience Gap: The team has no hands-on experience with AWS Batch for scientific workloads. The 8-week timeline is based on theoretical assumptions rather than practical knowledge, creating significant unknown risks.</p> <p>Pulumi Cost Scaling Uncertainty: Unclear how Pulumi pricing scales with infrastructure complexity (10 vs 1000 compute instances). Monthly costs could exceed projections if Pulumi charges per AWS resource rather than per managed service.</p> <p>Always-On Coordinator Requirements: StarryNight needs a persistent coordinator node for job submission and state management. Requires dedicated EC2 instance running 24/7; this may introduce infrastructure complexity.</p> <p>Custom Container Maintenance: Cannot use official CellProfiler containers due to need for custom telemetry and error handling wrappers. StarryNight must maintain its own CellProfiler builds, requiring automated CI/CD pipeline and coordination with CellProfiler releases.</p> <p>CellProfiler Integration Complexity: CellProfiler doesn't behave like standard command-line tools - unreliable exit codes and inconsistent error reporting in containers. Standard containerization approaches may fail; requires custom error detection and handling mechanisms.</p> <p>Bottom Line: While the roadmap provides a structured 8-week plan, several critical unknowns could significantly impact timeline and complexity. Early proof-of-concept testing is essential before committing to full deployment.</p>"},{"location":"developer/roadmap/pipeline-automation/","title":"Value and Limitations of Pipeline Automation","text":"<p>This document explores the value and limitations of automating CellProfiler pipeline generation for the next generation of Pooled Cell Painting Image Processing (PCPIP). It explains different module customization categories, their customization requirements, and the overall importance of programmatic pipeline creation.</p>"},{"location":"developer/roadmap/pipeline-automation/#pipeline-structure-overview","title":"Pipeline Structure Overview","text":"<p>The PCPIP workflow consists of two parallel tracks followed by integrated analysis:</p> <ol> <li>Cell Painting Track (Pipelines 1-4): Processes morphological channels (e.g., DNA, Phalloidin, ZO1)</li> <li>Barcoding Track (Pipelines 5-8): Processes genetic barcode channels (DAPI, A, C, G, T)</li> <li>Combined Analysis (Pipeline 9): Integrates phenotype and genotype data</li> </ol>"},{"location":"developer/roadmap/pipeline-automation/#module-customization-categories","title":"Module Customization Categories","text":"<p>Pipeline modules can be categorized into five distinct types based on their customization requirements (see diagrams below for reference):</p>"},{"location":"developer/roadmap/pipeline-automation/#1-base-times-cycle-barcoding-modules","title":"1. Base-Times-Cycle Barcoding Modules","text":"<p>Description:</p> <ul> <li>Modules that repeat once per base per cycle</li> <li>Highly repetitive with minor variations in channel references</li> <li>Most tedious to manually configure</li> </ul> <p>Specific Examples:</p> <ul> <li>Pipeline 6 (BC_Apply_Illum): <code>SaveImages</code> modules. Module count = A,T,G,C,DAPI x # cycles</li> </ul> <p>Automation Value: High for initial creation, but moderate overall since there are finite cycle counts (3-12)</p> <p></p>"},{"location":"developer/roadmap/pipeline-automation/#2-all-cycles-in-one-barcoding-modules","title":"2. All-Cycles-In-One Barcoding Modules","text":"<p>Description:</p> <ul> <li>Modules that appear once but list all cycles in their settings</li> <li>Need updates to all cycle references when cycle count changes</li> </ul> <p>Specific Examples:</p> <ul> <li>Pipeline 6 (BC_Apply_Illum): <code>CorrectIlluminationApply</code> modules. Module count = A,T,G,C,DAPI. Setting count/module = # cycles</li> <li>Pipeline 7 (BC_Preprocess): <code>CompensateColors</code> module. Module count = 1. Setting count/module = A,T,G,C,DAPI x # cycles.</li> </ul> <p>Automation Value: High for initial creation, but moderate overall since there are finite cycle counts (3-12)</p> <p></p> <p></p>"},{"location":"developer/roadmap/pipeline-automation/#3-cycle-count-parameter-barcoding-modules","title":"3. Cycle-Count-Parameter Barcoding Modules","text":"<p>Description:</p> <ul> <li>Modules with a single cycle count parameter</li> <li>Simple to update manually (just changing a number)</li> </ul> <p>Specific Examples:</p> <ul> <li>Pipeline 9 (Analysis): <code>CallBarcodes</code> module. Module count = 1. Setting count/module = 1.</li> </ul> <p>Automation Value: Low - these are trivial to update manually</p> <p></p>"},{"location":"developer/roadmap/pipeline-automation/#4-phenotype-measurement-modules","title":"4. Phenotype Measurement Modules","text":"<p>Description:</p> <ul> <li>Need adjustment for channel names but follow standard patterns</li> <li>Consistent structure across experiments</li> <li>CellProfiler often catches configuration errors</li> </ul> <p>Specific Examples:</p> <ul> <li>Pipeline 2 (CP_Apply_Illum): <code>CorrectIlluminationApply</code> modules. Module count = 1. Setting count/module = # channels.</li> <li>Pipeline 9 (Analysis): <code>MeasureObjectIntensity</code> modules (and all <code>Measure*</code> modules in general). Module count = 1. Setting count/module = # channels.</li> </ul> <p>Automation Value: Moderate - useful templates but easy to manually adjust</p> <p></p> <p></p>"},{"location":"developer/roadmap/pipeline-automation/#5-phenotype-segmentation-modules","title":"5. Phenotype Segmentation Modules","text":"<p>Description:</p> <ul> <li>Require expert tuning for each experiment</li> <li>Highly variable based on cell types and imaging conditions</li> </ul> <p>Specific Examples:</p> <ul> <li>Pipeline 2 (CP_Apply_Illum): <code>IdentifyPrimaryObjects</code> module with manually tuned diameter ranges and thresholding methods</li> <li>Pipeline 9 (Analysis): <code>IdentifySecondaryObjects</code> module for cell segmentation that uses nuclei as seeds</li> </ul> <p>Automation Value: Low - human expertise required regardless of automation</p> <p></p> <p></p>"},{"location":"developer/roadmap/pipeline-automation/#automation-priority","title":"Automation Priority","text":"<p>While automating pipeline generation would be beneficial, particularly for repetitive cycle-specific configurations, it's not a critical priority given:</p> <ol> <li>Bounded Problem Space: The finite range of cycle counts (3-12) means templates for common configurations can cover most use cases.</li> <li>Existing Resources: Pipelines for most common cycle counts already exist and can be adapted.</li> <li>Human Expertise Requirement: The most complex pipeline elements require expert tuning regardless of automation.</li> <li>Reasonable Manual Effort: Modifying pipelines for a new experiment typically takes only a couple of hours of work.</li> <li>Higher-Value Automation Targets: Other areas offer greater automation benefits:<ul> <li>File-to-LoadData parsing</li> <li>Workflow step chaining</li> <li>QC report generation</li> <li>Computing resource orchestration</li> </ul> </li> </ol>"},{"location":"developer/roadmap/pipeline-automation/#addendum-pipeline-diagrams","title":"Addendum: Pipeline Diagrams","text":"<p>Pipeline 2 (CP_Apply_Illum)</p> <p></p> <p>Pipeline 6 (BC_Apply_Illum)</p> <p></p> <p>Pipeline 7 (BC_Preprocess)</p> <p></p> <p>Pipeline 9 (Analysis)</p> <p></p>"},{"location":"developer/roadmap/requirements/","title":"Requirements","text":"<p>Note</p> <ul> <li>The status indicators may not reflect the current state of development, as the document is only reviewed and updated periodically.</li> <li>No requirements-to-implementation mapping exists yet.</li> </ul>"},{"location":"developer/roadmap/requirements/#1-system-overview","title":"1. System Overview","text":"<p>This document outlines requirements for a next-generation system for processing, analyzing, and managing high-throughput microscopy data from image-based profiling experiments, in particular, pooled optical screens. The system will orchestrate complex image processing workflows, manage computational resources efficiently, and provide mechanisms for both automated and manual intervention during the processing pipeline.</p> <p>Key:</p> <ul> <li>starrynight roadmap: \ud83d\udfe6 planned \ud83d\udfe8 in progress \ud83d\udd32 not planned</li> <li>priority: \u2611\ufe0f low \u2611\ufe0f\u2611\ufe0f medium \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f high</li> </ul>"},{"location":"developer/roadmap/requirements/#2-core-requirements","title":"2. Core Requirements","text":""},{"location":"developer/roadmap/requirements/#21-image-processing-capabilities","title":"2.1 Image Processing Capabilities","text":"<ul> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support automated execution of CellProfiler pipelines for all stages of image processing:<ul> <li>Illumination correction calculation and application</li> <li>Cell segmentation and validation</li> <li>Image stitching and cropping</li> <li>Channel alignment across imaging cycles</li> <li>Barcode identification and calling</li> <li>Feature extraction and analysis</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must process both Cell Painting (CP) and Barcoding (BC) image tracks in parallel, with integration points for combined analysis.</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must allow integration of non-CellProfiler image processing tools such as FIJI/ImageJ and custom analysis scripts.</li> </ul>"},{"location":"developer/roadmap/requirements/#22-experimental-configuration","title":"2.2 Experimental Configuration","text":"<ul> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support all image grid configuration parameters:<ul> <li><code>painting_rows</code>, <code>painting_columns</code>: For square acquisition patterns</li> <li><code>painting_imperwell</code>: For circular acquisition patterns (overrides rows/columns)</li> <li><code>barcoding_rows</code>, <code>barcoding_columns</code>: For square acquisition patterns</li> <li><code>barcoding_imperwell</code>: For circular acquisition patterns (overrides rows/columns)</li> </ul> </li> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support complex channel dictionary configuration:<ul> <li>Mapping microscope channel names to biological stains and frame indices</li> <li>Multi-round experiment support (e.g., SABER) with round identifiers</li> <li>Single-round experiment support with simpler configuration</li> </ul> </li> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support processing configuration settings:<ul> <li><code>barcoding_cycles</code>: Number of barcoding cycles to process</li> <li><code>range_skip</code>: Sampling frequency for quality control</li> </ul> </li> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support detailed stitching configuration:<ul> <li><code>overlap_pct</code>: Image overlap percentage between fields</li> <li><code>stitchorder</code>: Tile arrangement strategy based on acquisition pattern</li> <li><code>tileperside</code>: Number of tiles along each side of the stitched grid</li> <li><code>final_tile_size</code>: Pixel dimensions of output tiles</li> <li><code>round_or_square</code>: Well shape for cropping calculations</li> <li><code>quarter_if_round</code>: Division strategy for round wells</li> <li>Offset parameters for alignment troubleshooting</li> <li><code>compress</code>: Output file compression settings</li> </ul> </li> </ul>"},{"location":"developer/roadmap/requirements/#23-workflow-control","title":"2.3 Workflow Control","text":"<ul> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f: Must support fully automated end-to-end processing with configurable pipeline sequences:<ul> <li>While current optical pooled screening experiments require significant human judgment at multiple stages, the system should be designed to enable full automation as a long-term goal</li> <li>Must support both fully automated workflows for mature processing paths and semi-automated workflows requiring human intervention</li> <li>Must allow gradual transition from manual to automated processing as confidence in automated methods increases</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must enable manual intervention at any stage with the ability to:<ul> <li>Restart pipeline from checkpoint</li> <li>Inspect intermediate results before proceeding</li> <li>Modify parameters between stages</li> <li>Re-run specific stages with adjusted settings</li> <li>Launch inspection notebooks</li> </ul> </li> </ul>"},{"location":"developer/roadmap/requirements/#24-compute-resource-management","title":"2.4 Compute Resource Management","text":"<ul> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f: Must efficiently manage computational resources appropriate to each processing stage:<ul> <li>Scale resources based on workload</li> <li>Optimize resource allocation for memory-intensive vs. CPU-intensive tasks</li> <li>Support parallel processing of independent tasks</li> </ul> </li> <li>Must work across diverse compute environments:<ul> <li>Cloud platforms</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: AWS</li> <li>\ud83d\udd32 \u2611\ufe0f: Azure</li> <li>\ud83d\udd32 \u2611\ufe0f: GCP</li> <li>\ud83d\udd32 \u2611\ufe0f: On-premises high-performance computing clusters</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Local workstations (with appropriate scaling)</li> </ul> </li> </ul>"},{"location":"developer/roadmap/requirements/#25-data-management","title":"2.5 Data Management","text":"<ul> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must organize input and output data in a consistent, browsable structure:<ul> <li>Must maintain compatibility with existing input and output data structures</li> <li>Must produce outputs that match current output structures, which will remain rigid unless changed at the level of code</li> <li>Must provide clear documentation for any structural changes</li> </ul> </li> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f: Must track data provenance including:<ul> <li>Processing history for each image</li> <li>Parameters used at each stage</li> <li>Software versions and dependencies</li> <li>Note: Implementation should prioritize critical tracking elements needed for reproducibility while balancing system performance.</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must handle large data volumes (terabytes) with AWS backend.</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must implement flexible path parsing and data organization:<ul> <li>Standardized but configurable system for extracting metadata from file paths</li> <li>Support for mapping from various microscope vendor file organizations to internal structure</li> <li>Ability to adapt to different naming conventions without code changes</li> </ul> </li> </ul>"},{"location":"developer/roadmap/requirements/#26-user-interaction","title":"2.6 User Interaction","text":"<ul> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must provide multiple interaction mechanisms:<ul> <li>Command-line interface for scripting and automation</li> <li>Web-based or desktop GUI for visualization and control</li> <li>Programmatic API for integration with other systems</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must support both expert and non-expert users with appropriate levels of abstraction and guidance.<ul> <li>Web-based UI must provide two abstraction levels:<ul> <li>Simplified interface for non-computational scientists with guided workflows and sensible defaults</li> <li>Advanced interface with full parameter control for experienced users</li> </ul> </li> <li>Command-line interface and programmatic API will target computational experts only, with comprehensive documentation</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must integrate result visualization and quality control:<ul> <li>Built-in visualization tools for reviewing processing results, including cell segmentation, barcode calling, and feature data</li> <li>Integrated quality control metrics with contextual interpretations</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must provide interactive inspection tools:<ul> <li>Support for Jupyter notebooks (or similar) as a first-class inspection interface</li> </ul> </li> </ul>"},{"location":"developer/roadmap/requirements/#3-technical-requirements","title":"3. Technical Requirements","text":""},{"location":"developer/roadmap/requirements/#31-cross-platform-support","title":"3.1 Cross-Platform Support","text":"<ul> <li>Must run on multiple operation systems</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f Linux</li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f MacOS</li> <li>\ud83d\udd32 \u2611\ufe0f Windows (WSL)</li> </ul>"},{"location":"developer/roadmap/requirements/#32-extensibility","title":"3.2 Extensibility","text":"<ul> <li>\ud83d\udfe6 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must allow addition of new processing tools beyond CellProfiler.</li> <li>\ud83d\udfe6 \u2611\ufe0f: Must support custom analysis modules for specialized experiments.</li> </ul>"},{"location":"developer/roadmap/requirements/#33-documentation-and-support","title":"3.3 Documentation and Support","text":"<ul> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must provide comprehensive user documentation including installation and setup guides, configuration reference, workflow tutorials, and troubleshooting information.<ul> <li>Workflow tutorials and certain aspects of troubleshooting/setup guides will be developed collaboratively with end users, with the expectation that users will eventually maintain these materials</li> </ul> </li> <li>\ud83d\udfe8 \u2611\ufe0f\u2611\ufe0f\u2611\ufe0f: Must include developer documentation covering architecture overview, API references, extension guides, development setup, and technical decision rationale that explains key design choices and alternatives considered :sta</li> </ul>"},{"location":"developer/roadmap/walkthrough-2025-01-topics/","title":"StarryNight Walkthrough - Topics Discussed","text":""},{"location":"developer/roadmap/walkthrough-2025-01-topics/#layer-enforcement","title":"Layer Enforcement","text":"<p>Issue: #120</p> <p>Discussion about enforcement between layers - no current enforcement that algorithms have corresponding CLI commands or modules. Considered decorator approach to automatically create CLI from algorithms. Decided enforcement at test level might be useful, not at code level.</p>"},{"location":"developer/roadmap/walkthrough-2025-01-topics/#terminology-clarification","title":"Terminology Clarification","text":"<p>Needs Issue Creation</p> <p>Terminology confusion emerged about \"pipeline\" vs \"workflow\". CellProfiler uses \"pipeline\", Snakemake uses \"pipeline\", StarryNight also uses \"pipeline\" - all mean different things. Decided to keep \"pipeline\" term but disambiguate in documentation. \"Workflow\" defined loosely as scientific image processing sequence implemented in modules.</p>"},{"location":"developer/roadmap/walkthrough-2025-01-topics/#channel-mapping-and-multi-cycle-experiments","title":"Channel Mapping and Multi-Cycle Experiments","text":"<p>Issue: #125</p> <p>Channel mapping problem surfaced as major issue. Current system hardcoded for 3 channels: nucleus, cell, mitochondria. These are \"special\" channels needed for segmentation. Other channels processed in aggregate but can't be named. Real experiments have many channels with different meanings across cycles. Example: AF488 in cycle 0 means DNA, but AF488 in cycle 1 means RNA.</p> <p>Biologists need semantic names (DNA, RNA) not microscope names (AF488, AF750). Different microscopes call same thing different names - Texas Red vs 594. Currently adding new channels requires going to algorithm layer and changing code. This is unacceptable for biologists.</p> <p>Configuration vs parser debate. Channel mapping could be handled at parser level or configuration level. Parser uses Lark grammar, originally written in Rust then moved to Python. Biologists find parser modification too technical. Agreement that channel mapping belongs in configuration not parser.</p> <p>Example channel dictionary structure discussed - uses folder names as keys to handle multiple cycles. Load data CSV generation needs to support this complexity. Need 2 levels of nesting: file level and channel within file. Biologists won't follow strict file naming conventions even with clear documentation.</p> <p>SABER experiments particularly challenging - multiple rounds of acquisition on same well. Published Periscope data was single phenotyping cycle so didn't hit this issue. SBS arm handles multiple files per site but channels have consistent meaning. Phenotyping with multiple rounds has channels that change meaning.</p> <p>File naming chaos - biologists add random strings, periods, hashes. Microscopes have configurable output names. Can't rely on consistent naming even with strict guidelines. Erin enforced conventions for gallery but still had issues.</p> <p>Parser configuration allows custom grammar definitions. Users can implement own parsers but this is too complex for biologists. Need to hide this complexity while maintaining flexibility.</p> <p>LoadData CSV creator needs to handle arbitrary nesting and mappings. Current implementation supports standard single cycle or SABER throughout. Runs as Python script on AWS.</p>"},{"location":"developer/roadmap/walkthrough-2025-01-topics/#installation-options","title":"Installation Options","text":"<p>Issue: #121</p> <p>Installation barriers discussed. Nix provides reproducible builds but scares biologists - \"might nuke my computer\". Need multiple installation options: Docker for simplicity, pip/uv for familiarity, Nix for developers. Current getting started guide too technical.</p>"},{"location":"developer/roadmap/walkthrough-2025-01-topics/#documentation-structure","title":"Documentation Structure","text":"<p>Issue: #114</p> <p>Documentation structure needs different entry points. Current docs start with architecture which loses biologists. Need \"I just want to run my images\" path separate from developer path. Architecture for biologists document exists but might not be discoverable enough.</p>"},{"location":"developer/validation/testing-framework/","title":"StarryNight Testing Framework","text":"<p>IMPORTANT: Code Migration Notice All implementation code has been moved from <code>/docs/tester/assets/</code> to the <code>/tests/</code> directory as part of the documentation refactoring plan. This documentation has been updated to reference the new locations, but refers to the same underlying functionality.</p> <p>Welcome to the StarryNight testing framework! This is your comprehensive guide for validating, testing, and creating test fixtures for PCPIP (Pooled Cell Painting Image Processing) workflows.</p>"},{"location":"developer/validation/testing-framework/#introduction-why-validation-matters","title":"Introduction: Why Validation Matters","text":"<p>StarryNight reimagines the Pooled Cell Painting Image Processing (PCPIP) pipeline with a more maintainable architecture, but we must ensure scientific equivalence with the original implementation. The testing framework outlined here provides a systematic approach to verify that StarryNight produces functionally equivalent results to the reference PCPIP pipeline, giving users confidence in the transition.</p> <p>What is validation? In this context, validation means comparing StarryNight outputs against the original PCPIP implementation at multiple levels:</p> <ul> <li>Pipeline structure comparison</li> <li>Output file verification</li> <li>Content similarity analysis</li> <li>End-to-end workflow execution</li> </ul> <p>Our validation employs progressive testing of individual components before integration, multiple levels of comparison from pipeline structure to final outputs, tolerance for non-critical numerical differences while ensuring functional equivalence, and documentation-driven development with clear, reproducible testing procedures.</p> <p>Core Principle: The goal is not byte-for-byte identical outputs, but functionally equivalent results that ensure StarryNight delivers the same scientific value as the original PCPIP implementation.</p>"},{"location":"developer/validation/testing-framework/#getting-started","title":"Getting Started","text":"<p>This guide serves different types of testers. Find your path based on your role:</p>"},{"location":"developer/validation/testing-framework/#im-new-to-starrynight-testing","title":"I'm new to StarryNight testing","text":"<p>If you're new to the testing framework and want to understand the process:</p> <ol> <li>Start here: Read the introduction and Validation Process Overview sections to understand the big picture</li> <li>Review the Validation Documents section below to see all the pipelines that need validation</li> <li>Look at the Pipeline 1 (illum_calc) Validation as a concrete example with detailed commands</li> <li>See the Testing Tools Summary for an overview of the available tools</li> </ol>"},{"location":"developer/validation/testing-framework/#i-need-to-validate-a-specific-pipeline-module","title":"I need to validate a specific pipeline module","text":"<p>If you need to validate a specific StarryNight module:</p> <ol> <li>Check the Validation Documents section below to find the corresponding pipeline</li> <li>Follow the 5-stage validation process outlined in the Validation Stages section</li> <li>Use Pipeline 1 (illum_calc) Validation as a template</li> </ol>"},{"location":"developer/validation/testing-framework/#reference-materials","title":"Reference Materials","text":"<p>The testing framework includes these key resources:</p> <ul> <li>pcpip-pipelines: Reference CellProfiler pipeline files</li> <li>pcpip-create-fixture: Tools for creating test fixtures</li> <li>pcpip-test: Scripts for pipeline execution and comparison</li> </ul>"},{"location":"developer/validation/testing-framework/#testing-environment-setup","title":"Testing Environment Setup","text":"<p>Beyond the standard StarryNight installation, validation requires additional setup:</p>"},{"location":"developer/validation/testing-framework/#prerequisites","title":"Prerequisites","text":"<ul> <li>Nix: For setting up the complete development environment<ul> <li>Follow the standard StarryNight installation</li> </ul> </li> <li>Additional Tools:<ul> <li>Graphviz: For pipeline visualization (<code>apt install graphviz</code> or <code>brew install graphviz</code>)</li> <li>cp_graph.py: For CellProfiler pipeline graph analysis (clone from <code>https://github.com/shntnu/cp_graph</code>)</li> </ul> </li> <li>AWS Access (optional): For downloading reference datasets<ul> <li>AWS CLI configured with access to <code>s3://imaging-platform/projects/2024_03_12_starrynight/</code></li> </ul> </li> <li>Test Data Alternatives (if no AWS access):<ul> <li>Use the minimal test fixtures in <code>/tests/fixtures/minimal/</code></li> </ul> </li> </ul>"},{"location":"developer/validation/testing-framework/#test-dataset","title":"Test Dataset","text":"<p>Set up the validation dataset:</p> <pre><code># Set up environment\nexport STARRYNIGHT_REPO=\"$(git rev-parse --show-toplevel)\"\nmkdir -p ${STARRYNIGHT_REPO}/scratch\n\n# Download test fixture (if you have AWS access)\naws s3 sync s3://imaging-platform/projects/2024_03_12_starrynight/starrynight_example_input ${STARRYNIGHT_REPO}/scratch/starrynight_example_input\n\n# Create validation workspace\nmkdir -p ${STARRYNIGHT_REPO}/scratch/starrynight_example_output/workspace/validation\n</code></pre>"},{"location":"developer/validation/testing-framework/#common-environment-variables","title":"Common Environment Variables","text":"<p>Use these in all validation scripts:</p> <pre><code># Base directories\nexport STARRYNIGHT_REPO=\"$(git rev-parse --show-toplevel)\"\nexport WKDIR=\"${STARRYNIGHT_REPO}/scratch/starrynight_example_output/workspace\"\nexport VALIDATION_DIR=\"${WKDIR}/validation\"\n\n# Reference locations (common across validations)\nexport REF_PIPELINES=\"${STARRYNIGHT_REPO}/tests/pcpip-pipelines\"\n</code></pre> <p>See individual validation documents for pipeline-specific variables.</p>"},{"location":"developer/validation/testing-framework/#validation-process-overview","title":"Validation Process Overview","text":"<p>The following diagram illustrates the 5-stage validation process and the key tools used at each stage:</p> <pre><code>flowchart TD\n    subgraph Stage1[\"Stage 1: Topology\"]\n        RefPipe[Reference Pipeline] --&gt;|cp_graph.py| RefGraph[Reference Graph]\n        SNPipe[StarryNight Pipeline] --&gt;|cp_graph.py| SNGraph[StarryNight Graph]\n        RefGraph --&gt;|diff| GraphComp\n        SNGraph --&gt;|diff| GraphComp\n        GraphComp[Graph Comparison]\n    end\n\n    subgraph Stage2[\"Stage 2: LoadData\"]\n        RefLD[Reference LoadData] --&gt;|diff| LDComp\n        SNLD[StarryNight LoadData] --&gt;|diff| LDComp\n        LDComp[LoadData Comparison]\n    end\n\n    subgraph Stage3[\"Stage 3: Reference Run\"]\n        RefPipeExec[Reference Pipeline] --&gt;|run_pcpip.sh| RefOut\n        RefOut --&gt;|verify_file_structure.py| RefStruct\n        RefOut[Reference Output]\n        RefStruct[Reference Structure]\n    end\n\n    subgraph Stage4[\"Stage 4: StarryNight by hand\"]\n        SNPipeExec[StarryNight Pipeline] --&gt;|run_pcpip.sh| SN1Out\n        SN1Out --&gt;|verify_file_structure.py| SN1Struct\n        SN1Out[StarryNight Output]\n        SN1Struct[StarryNight Structure]\n\n        SN1Out --&gt;|diff| RefSN1Comp\n        Ref1Struct --&gt;|diff| RefSN1Comp\n\n        RefSN1Comp[Output Comparison]\n        Ref1Struct[Reference Structure]\n    end\n\n    subgraph Stage5[\"Stage 5: StarryNight End-to-End\"]\n        SNCLI[StarryNight CLI] --&gt;|starrynight commands| SN2Out\n        SN2Out --&gt;|verify_file_structure.py| SN2Struct\n        SN2Out[End-to-End Output]\n        SN2Struct[E2E Structure]\n\n        SN2Out --&gt;|diff| RefSN2Comp\n        Ref2Struct --&gt;|diff| RefSN2Comp\n\n        RefSN2Comp[Output Comparison]\n        Ref2Struct[Reference Structure]\n\n    end\n\n    Stage1 --&gt; Stage2 --&gt; Stage3\n    Stage3 --&gt; Stage4\n    Stage3 --&gt; Stage5</code></pre>"},{"location":"developer/validation/testing-framework/#validation-stages","title":"Validation Stages","text":""},{"location":"developer/validation/testing-framework/#stage-1-pipeline-graph-topology-validation","title":"Stage 1: Pipeline Graph Topology Validation","text":"<ul> <li>Objective: Verify that StarryNight pipelines have identical module dependency graphs compared to PCPIP</li> <li>Approach:<ul> <li>Use <code>cp_graph.py</code> to convert CellProfiler pipeline JSONs to DOT graph files</li> <li>Compare generated DOT files against reference graphs in <code>_ref_graph_format/dot</code></li> <li>Validate module connections, data flow, and overall structure</li> </ul> </li> <li>Success Criteria: Graph structural equivalence without requiring identical module settings</li> </ul>"},{"location":"developer/validation/testing-framework/#stage-2-loaddata-csv-generation-validation","title":"Stage 2: LoadData CSV Generation Validation","text":"<ul> <li>Objective: Ensure StarryNight generates compatible LoadData CSV files</li> <li>Approach:<ul> <li>Generate LoadData CSVs using StarryNight</li> <li>Compare against reference LoadData CSVs using <code>compare_structures.py</code></li> <li>Validate CSV structure, headers, and key metadata fields</li> </ul> </li> <li>Success Criteria: Functionally equivalent CSV files (allowing for formatting differences)</li> </ul>"},{"location":"developer/validation/testing-framework/#csv-validation-philosophy","title":"CSV Validation Philosophy","text":"<p>The validation of LoadData CSVs is intentionally nuanced rather than a simple binary comparison. While a direct dataframe diff would be simplest, it would flag many acceptable differences that don't impact scientific validity. Our validation approach:</p> <ol> <li>Beyond Binary Comparison: Rather than just determining whether files are identical, the validation identifies and classifies the types of differences</li> <li>Forensic Analysis: The validation acts as a forensic tool to explain differences, answering \"what kind of difference is this?\" rather than just \"is there a difference?\"</li> <li>Acceptable Variations: Certain differences (ordering, floating point precision, etc.) are expected and acceptable</li> <li>Critical vs. Non-Critical: The validation distinguishes between differences that impact scientific results and those that don't</li> </ol> <p>This approach is essential for scientific pipelines where small variations in output might be acceptable due to factors such as: - Different ordering of otherwise identical data - Minor floating point precision differences - Alternative but equivalent path representations - Metadata differences that don't affect processing</p>"},{"location":"developer/validation/testing-framework/#stage-3-reference-pipeline-execution","title":"Stage 3: Reference Pipeline Execution","text":"<ul> <li>Objective: Run reference pipelines with reference LoadData and capture outputs</li> <li>Approach:<ul> <li>Execute reference CellProfiler pipelines using command-line invocation</li> <li>Use <code>run_pcpip.sh</code> script to orchestrate multi-stage pipeline execution</li> <li>Capture all outputs and file structures</li> </ul> </li> <li>Success Criteria: Successful execution of all pipeline stages with expected outputs</li> </ul>"},{"location":"developer/validation/testing-framework/#stage-4-starrynight-pipeline-execution","title":"Stage 4: StarryNight Pipeline Execution","text":"<ul> <li>Objective: Run StarryNight-generated pipelines with reference LoadData</li> <li>Approach:<ul> <li>Execute StarryNight-generated CellProfiler pipelines with identical inputs</li> <li>Compare outputs against reference using <code>verify_file_structure.py</code> and <code>compare_structures.py</code></li> <li>Iterate on pipelines until outputs match</li> </ul> </li> <li>Success Criteria: Outputs that match reference results (allowing for numerical differences)</li> </ul>"},{"location":"developer/validation/testing-framework/#stage-5-starrynight-end-to-end-testing","title":"Stage 5: StarryNight End-to-End Testing","text":"<ul> <li>Objective: Validate complete StarryNight workflow including orchestration</li> <li>Approach:<ul> <li>Execute StarryNight's CellProfiler invocation with StarryNight-generated LoadData</li> <li>Compare against reference outputs</li> <li>Iterate on orchestration system until outputs match</li> </ul> </li> <li>Success Criteria: End-to-end process produces equivalent results to reference</li> </ul>"},{"location":"developer/validation/testing-framework/#validation-documents","title":"Validation Documents","text":"<p>Currently, a validation document has been created only for Pipeline 1: illum_calc. To create validation documents for other pipelines, use this as a template and adjust the pipeline-specific details accordingly.</p> <p>Here are the reference CellProfiler pipelines and their StarryNight module counterparts:</p> <ul> <li><code>ref_1_CP_Illum.cppipe</code>: <code>illum_calc</code></li> <li><code>ref_2_CP_Apply_Illum.cppipe</code>: <code>illum_apply</code></li> <li><code>ref_3_CP_SegmentationCheck.cppipe</code>: <code>segcheck</code></li> <li><code>ref_5_BC_Illum.cppipe</code>:  <code>illum_calc</code></li> <li><code>ref_6_BC_Apply_Illum.cppipe</code>: <code>illum_apply_sbs</code></li> <li><code>ref_7_BC_Preprocess.cppipe</code>: <code>preprocess</code></li> <li><code>ref_9_Analysis.cppipe</code>: <code>analysis</code></li> </ul>"},{"location":"developer/validation/testing-framework/#validation-strategy-and-success-criteria","title":"Validation Strategy and Success Criteria","text":"<p>To ensure StarryNight can confidently replace the original PCPIP implementation, our validation approach:</p> <ul> <li>Tracks progress through GitHub issues (one per pipeline) linked to detailed documentation, where issues serve as discussion forums while validation documents contain the actual technical details and results</li> <li>Uses a balanced test fixture from <code>/tests/pcpip-fixtures</code> that's small yet representative</li> <li>Defines success through:<ul> <li>Structural equivalence: Identical data flow between modules</li> <li>Functional equivalence: Comparable outputs (with acceptable numerical differences)</li> <li>Reproducibility: Consistent results across executions</li> <li>Traceability: Clear documentation of validation results</li> </ul> </li> </ul> <p>The goal isn't byte-for-byte identical outputs, but functionally equivalent results that deliver the same scientific value with improved maintainability and extensibility.</p> <p>Testing Framework Roadmap: <code>compare_structures.py</code> should be modified to allow direct comparison of LoadData CSVs and specify exact files to compare, <code>run_pcpip.sh</code> should be updated to accept the output base path as a parameter and pipeline paths as parameters or standardize locations of StarryNight pipelines for symmetry.</p>"},{"location":"developer/validation/testing-framework/#testing-tools-summary","title":"Testing Tools Summary","text":"<p>The validation process uses these key tools:</p> Tool Purpose Source Used In cp_graph.py Creates graph visualizations of pipeline structure External repo Stage 1 verify_file_structure.py Validates output file existence and structure tests/tools Stages 3-5 compare_structures.py Compares output structures for differences tests/tools Stages 4-5 run_pcpip.sh Executes CellProfiler pipeline workflows tests/tools Stage 3-4"},{"location":"developer/validation/validation-pipeline-1-illum-calc/","title":"Pipeline Validation: 1_CP_Illum (illum_calc)","text":"<p>IMPORTANT: Code Migration Notice All implementation code has been moved from <code>/docs/tester/assets/</code> to the <code>/tests/</code> directory as part of the documentation refactoring plan. This document has been updated to reference the new locations.</p>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#pipeline-overview","title":"Pipeline Overview","text":"<ul> <li>Reference Pipeline: <code>ref_1_CP_Illum.cppipe</code></li> <li>StarryNight Module: <code>illum_calc</code></li> <li>Description: Calculates illumination correction functions to normalize uneven lighting patterns across Cell Painting channels specifically. Barcoding/SPS channels are handled separately by Pipeline 5. See PCPIP documentation for details on all pipelines.</li> </ul>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#validation-status","title":"Validation Status","text":"<ul> <li> Stage 1 - Graph Topology</li> <li> Stage 2 - LoadData Generation</li> <li> Stage 3 - Reference Execution</li> <li> Stage 4 - StarryNight Pipeline</li> <li> Stage 5 - End-to-End</li> </ul>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#setup-and-environment","title":"Setup and Environment","text":"<p>Set up required environment variables before running any commands:</p> <pre><code># Base directories\nexport STARRYNIGHT_REPO=\"$(git rev-parse --show-toplevel)\"\nexport WKDIR=\"./scratch/starrynight_example_output/workspace\"\n\n# Reference data locations\nexport REF_PIPELINE=\"${STARRYNIGHT_REPO}/tests/pcpip-pipelines/ref_1_CP_Illum.cppipe\"\nexport REF_GRAPH=\"${STARRYNIGHT_REPO}/tests/pcpip-pipelines/_ref_graph_format/dot/ref_1_CP_Illum.dot\"\nexport REF_LOADDATA=\"${STARRYNIGHT_REPO}/scratch/pcpip_example_output/Source1/workspace/load_data_csv/Batch1/Plate1_trimmed/load_data_pipeline1.csv\"\n\n# StarryNight output locations\nexport SN_LOADDATA=\"${WKDIR}/cellprofiler/loaddata/cp/illum_calc/Batch1/illum_calc_Batch1_Plate1.csv\"\nexport SN_PIPELINE_DIR=\"${WKDIR}/cellprofiler/cppipe/cp/illum_calc\"  # Directory containing pipeline files\nexport SN_PIPELINE_CPPIPE=\"${SN_PIPELINE_DIR}/illum_calc_painting.cppipe\"  # CellProfiler pipeline file\nexport SN_PIPELINE_JSON=\"${SN_PIPELINE_DIR}/illum_calc_painting.json\"  # JSON representation\nexport SN_PIPELINE_DOT=\"${SN_PIPELINE_DIR}/illum_calc_painting.dot\"\nexport SN_PIPELINE_PNG=\"${SN_PIPELINE_DIR}/illum_calc_painting.png\"\nexport SN_PIPELINE_VISUAL_DOT=\"${SN_PIPELINE_DIR}/illum_calc_painting_visual.dot\"\nexport SN_OUTPUT=\"${WKDIR}/illum/cp/illum_calc\"\n\n# Validation outputs\nexport VALIDATION_DIR=\"${WKDIR}/validation/illum_calc\"\nexport REF_OUTPUT=\"${VALIDATION_DIR}/reference_output\"\nexport SN_TEST_OUTPUT=\"${VALIDATION_DIR}/starrynight_output\"\nexport EMBEDDING_DIR=\"${VALIDATION_DIR}/embeddings\"\n\n# Make validation directories\nmkdir -p ${VALIDATION_DIR}\nmkdir -p ${EMBEDDING_DIR}\n</code></pre>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#reference-materials","title":"Reference Materials","text":"<ul> <li>Reference Pipeline: https://github.com/broadinstitute/starrynight/blob/main/tests/pcpip-pipelines/ref_1_CP_Illum.cppipe</li> <li>Reference Graph: https://github.com/broadinstitute/starrynight/blob/main/tests/pcpip-pipelines/_ref_graph_format/dot/ref_1_CP_Illum.dot</li> <li>Reference JSON: https://github.com/broadinstitute/starrynight/blob/main/tests/pcpip-pipelines/_ref_graph_format/json/ref_1_CP_Illum.json</li> <li>Test Dataset: <code>${STARRYNIGHT_REPO}/starrynight_example_input/</code>, created using pcpip-create-fixture</li> <li>Reference LoadData CSV: <code>${STARRYNIGHT_REPO}/scratch/pcpip_example_output/Source1/workspace/load_data_csv/Batch1/Plate1_trimmed/load_data_pipeline1.csv</code>, created using pcpip-create-fixture</li> </ul>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#stage-1-graph-topology","title":"Stage 1: Graph Topology","text":"<p>Objective: Verify StarryNight pipeline structure matches reference</p> <p>StarryNight Command: <pre><code># Generate pipeline\nstarrynight illum calc cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum_calc/ \\\n    -o ${SN_PIPELINE_DIR} \\\n    -w ${WKDIR}\n\n# Note: StarryNight automatically generates JSON version alongside the .cppipe file\n</code></pre></p> <p>Comparison Command: <pre><code># First get the cp_graph.py tool (if not already available)\n# From: https://github.com/shntnu/cp_graph/blob/v0.8.0/cp_graph.py\n\n# 1. Generate ultra-minimal DOT graph for exact comparison\n# See README.md for more information on cp_graph.py usage\nuv run --script cp_graph.py ${SN_PIPELINE_JSON} ${SN_PIPELINE_DOT} --ultra-minimal\n\n# 2. Generate visual DOT graph for human inspection\nuv run --script cp_graph.py ${SN_PIPELINE_JSON} ${SN_PIPELINE_VISUAL_DOT}\n\n# 3. Create PNG visualization from the visual DOT file\n# Requires Graphviz to be installed\ndot -Tpng ${SN_PIPELINE_VISUAL_DOT} -o ${SN_PIPELINE_PNG}\n\n# 4. Compare generated DOT with reference for exact structural matching\ndiff ${SN_PIPELINE_DOT} ${REF_GRAPH}\n\n# 5. Optional: Also generate PNG from reference for visual comparison\ndot -Tpng ${REF_GRAPH} -o \"${VALIDATION_DIR}/ref_1_CP_Illum.png\"\n</code></pre></p> <p>Results:</p> <pre><code># Results will be added here\n</code></pre> <p>Discrepancies:</p> <ul> <li>None identified yet</li> </ul> <p>Resolution:</p> <ul> <li>None required yet</li> </ul>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#stage-2-loaddata-generation","title":"Stage 2: LoadData Generation","text":"<p>Objective: Verify StarryNight generates compatible LoadData CSVs</p> <p>StarryNight Command: <pre><code># Generate LoadData files\nstarrynight illum calc loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/cp/illum_calc\n</code></pre></p> <p>Comparison Command: <pre><code># Compare sample StarryNight LoadData with reference\n# Note: compare_structures.py expects two file structure YAML files\n# For comparing CSVs directly, we should extract headers and row counts first\n\n# Extract headers and row counts for both CSVs\npython -c \"\nimport csv\ndef compare_csv_structure(file1, file2):\n    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n        headers1 = next(csv.reader(f1))\n        headers2 = next(csv.reader(f2))\n        rows1 = sum(1 for _ in f1) + 1\n        rows2 = sum(1 for _ in f2) + 1\n    print(f'CSV Comparison:\\\\n- File 1: {file1}\\\\n  Headers: {len(headers1)}\\\\n  Rows: {rows1}\\\\n- File 2: {file2}\\\\n  Headers: {len(headers2)}\\\\n  Rows: {rows2}')\n    print('Headers match:' if sorted(headers1) == sorted(headers2) else 'Headers differ:')\n    if sorted(headers1) != sorted(headers2):\n        print(f'  Only in file1: {set(headers1) - set(headers2)}')\n        print(f'  Only in file2: {set(headers2) - set(headers1)}')\ncompare_csv_structure('${REF_LOADDATA}', '${SN_LOADDATA}')\n\" &gt; ${VALIDATION_DIR}/loaddata_comparison.txt\n</code></pre></p> <p>Results: <pre><code># Results will be added here\n</code></pre></p> <p>Discrepancies:</p> <ul> <li>None identified yet</li> </ul>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#stage-3-reference-execution","title":"Stage 3: Reference Execution","text":"<p>Objective: Establish baseline outputs from reference pipeline</p> <p>Command: <pre><code># Note: The run_pcpip.sh script must be used to run PCPIP steps\n# By default, outputs go to ${STARRYNIGHT_REPO}/scratch/reproduce_pcpip_example_output\n# To modify output location, update the REPRODUCE_DIR variable in the script\n\n# Run using run_pcpip.sh script\ncd ${STARRYNIGHT_REPO}/tests/tools/\n# First modify the script to output to the reference output location\nsed -i.bak \"s|REPRODUCE_DIR=.*|REPRODUCE_DIR=\\\"${REF_OUTPUT}\\\"|\" run_pcpip.sh\n# Run only pipeline 1\n./run_pcpip.sh 1\n\n# Validate the reference output structure\npython ${STARRYNIGHT_REPO}/tests/tools/verify_file_structure.py \\\n    --directory ${REF_OUTPUT} \\\n    --output ${VALIDATION_DIR}/reference_structure.yaml \\\n    --embedding-dir ${EMBEDDING_DIR}\n</code></pre></p> <p>Results: <pre><code># Results will be added here\n</code></pre></p>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#stage-4-starrynight-generated-pipeline-execution","title":"Stage 4: StarryNight-Generated Pipeline Execution","text":"<p>Objective: Verify StarryNight-generated CellProfiler pipeline with reference LoadData</p> <p>Command: <pre><code># Modify run_pcpip.sh to use StarryNight-generated pipeline\ncd ${STARRYNIGHT_REPO}/tests/tools/\n# Make a copy for StarryNight testing\ncp run_pcpip.sh run_starrynight.sh\n# Update output directory and pipeline path in the script\nsed -i.bak \"s|REPRODUCE_DIR=.*|REPRODUCE_DIR=\\\"${SN_TEST_OUTPUT}\\\"|\" run_starrynight.sh\n# Update pipeline path in PIPELINE_CONFIG array - line ~80\nsed -i.bak \"s|1,file=.*|1,file=\\\"${SN_PIPELINE_CPPIPE}\\\"|\" run_starrynight.sh\n# Run the modified script\n./run_starrynight.sh 1\n\n# Validate the StarryNight output structure\npython ${STARRYNIGHT_REPO}/tests/tools/verify_file_structure.py \\\n    --directory ${SN_TEST_OUTPUT} \\\n    --output ${VALIDATION_DIR}/starrynight_structure.yaml \\\n    --embedding-dir ${EMBEDDING_DIR}\n</code></pre></p> <p>Comparison Command: <pre><code># Compare reference and StarryNight output structures using multiple formats\n# YAML format (default, machine-readable)\npython ${STARRYNIGHT_REPO}/tests/tools/compare_structures.py \\\n    ${VALIDATION_DIR}/reference_structure.yaml \\\n    ${VALIDATION_DIR}/starrynight_structure.yaml \\\n    --output-file ${VALIDATION_DIR}/stage4_comparison.yaml \\\n    --compare-embeddings\n\n# Text format (human-readable summary)\npython ${STARRYNIGHT_REPO}/tests/tools/compare_structures.py \\\n    ${VALIDATION_DIR}/reference_structure.yaml \\\n    ${VALIDATION_DIR}/starrynight_structure.yaml \\\n    --output-file ${VALIDATION_DIR}/stage4_comparison.txt \\\n    --output-format text \\\n    --compare-embeddings\n</code></pre></p> <p>Results: <pre><code># Results will be added here\n</code></pre></p> <p>Discrepancies: - None identified yet</p>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#stage-5-end-to-end","title":"Stage 5: End-to-End","text":"<p>Objective: Verify complete StarryNight workflow</p> <p>Command: <pre><code># Generate LoadData files\nstarrynight illum calc loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/cp/illum_calc\n\n# Generate CellProfiler pipelines\nstarrynight illum calc cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum_calc/ \\\n    -o ${SN_PIPELINE_DIR} \\\n    -w ${WKDIR}\n\n# Execute pipelines\nstarrynight cp \\\n    -p ${SN_PIPELINE_DIR}/ \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum_calc \\\n    -o ${SN_OUTPUT}\n\n# Note: In the future, the SN_OUTPUT path may need to be adjusted to match\n# the expected output path structure from run_pcpip.sh for proper comparison\n\n# Validate the StarryNight end-to-end output structure\npython ${STARRYNIGHT_REPO}/tests/tools/verify_file_structure.py \\\n    --directory ${SN_OUTPUT} \\\n    --output ${VALIDATION_DIR}/e2e_structure.yaml \\\n    --embedding-dir ${EMBEDDING_DIR}\n</code></pre></p> <p>Comparison Command: <pre><code># Compare reference and StarryNight end-to-end output structures\n# YAML format (default, machine-readable)\npython ${STARRYNIGHT_REPO}/tests/tools/compare_structures.py \\\n    ${VALIDATION_DIR}/reference_structure.yaml \\\n    ${VALIDATION_DIR}/e2e_structure.yaml \\\n    --output-file ${VALIDATION_DIR}/e2e_comparison.yaml \\\n    --compare-embeddings\n\n# Text format (human-readable summary)\npython ${STARRYNIGHT_REPO}/tests/tools/compare_structures.py \\\n    ${VALIDATION_DIR}/reference_structure.yaml \\\n    ${VALIDATION_DIR}/e2e_structure.yaml \\\n    --output-file ${VALIDATION_DIR}/e2e_comparison.txt \\\n    --output-format text \\\n    --compare-embeddings\n\n# Set custom tolerance for numerical differences (if needed)\npython ${STARRYNIGHT_REPO}/tests/tools/compare_structures.py \\\n    ${VALIDATION_DIR}/reference_structure.yaml \\\n    ${VALIDATION_DIR}/e2e_structure.yaml \\\n    --output-file ${VALIDATION_DIR}/e2e_comparison_tolerance.yaml \\\n    --compare-embeddings \\\n    --tolerance 0.01\n</code></pre></p> <p>Results: <pre><code># Results will be added here\n</code></pre></p> <p>Discrepancies: - None identified yet</p>"},{"location":"developer/validation/validation-pipeline-1-illum-calc/#test-runs","title":"Test Runs","text":"<ul> <li>YYYY-MM-DD: [Stage] - [Command] - [Result]</li> </ul>"},{"location":"user/example-pipeline-cli/","title":"Complete Workflow Example (CLI Approach)","text":"<p>This guide walks through a complete example of processing microscopy images using StarryNight's key modules and CLI commands. It builds on the foundation established in the Getting Started guide, extending those concepts into a full image processing workflow.</p>"},{"location":"user/example-pipeline-cli/#prerequisites","title":"Prerequisites","text":"<p>Before starting this workflow, you need:</p> <ul> <li>To complete the workflow from Getting Started, including:<ul> <li>Setting up your environment</li> <li>Downloading sample data</li> <li>Creating experiment configuration</li> <li>Generating inventory and index</li> <li>Creating <code>experiment.json</code></li> <li>Running illumination correction calculation</li> </ul> </li> <li>Sufficient disk space for intermediate and output files (at least 50GB)</li> </ul> <p>Implementation Note</p> <p>This guide continues to use the CLI Approach introduced in Getting Started to demonstrate the complete workflow step by step. For details on executing this workflow via the Python/Module Approach used in production, see Practical Integration, which shows the same workflow implemented with the Python API in <code>starrynight/notebooks/pypct/exec_pcp_generic_pipe.py</code>.</p>"},{"location":"user/example-pipeline-cli/#workflow-overview","title":"Workflow Overview","text":"<p>This guide demonstrates a comprehensive workflow for processing optical pooled screening (OPS) data. The workflow consists of two parallel tracks - Cell Painting (CP) and Barcoding (Sequencing by Synthesis, SBS) - followed by a combined analysis step:</p> <pre><code>flowchart TD\n    subgraph \"Cell Painting (CP) Track\"\n        CP1[\"CP Illumination Calculation\"] --&gt;\n        CP2[\"CP Illumination Application\"] --&gt;\n        CP3[\"CP Segmentation Check\"] --&gt;\n        CP4[\"CP Stitch and Crop\"]\n    end\n\n    subgraph \"Barcoding (SBS) Track\"\n        SBS1[\"SBS Illumination Calculation\"] --&gt;\n        SBS2[\"SBS Illumination Application and Alignment\"] --&gt;\n        SBS3[\"SBS Preprocessing\"] --&gt;\n        SBS4[\"SBS Stitch and Crop\"]\n    end\n\n    CP4 &amp; SBS4 --&gt; Analysis[\"Combined Analysis\"]\n\n    %% Styling\n    classDef cpTrack fill:#e6f3ff,stroke:#0066cc\n    classDef sbsTrack fill:#ffe6e6,stroke:#cc0000\n    classDef analysisStep fill:#e6ffe6,stroke:#009900\n\n    class CP1,CP2,CP3,CP4 cpTrack\n    class SBS1,SBS2,SBS3,SBS4 sbsTrack\n    class Analysis analysisStep</code></pre> <p>The workflow includes these key steps:</p> <ul> <li>Setup and Preparation: Generate inventory/index and create experiment configuration (completed in Getting Started)</li> <li>CP Illumination Calculation: Generate correction functions for CP images (completed in Getting Started)</li> <li>CP Illumination Application: Apply illumination corrections to CP images</li> <li>CP Segmentation Check: Verify cell segmentation quality in CP images</li> <li>CP Stitch and Crop: Stitch multi-site images and crop for analysis</li> <li>SBS Illumination Calculation: Generate correction functions for SBS images</li> <li>SBS Illumination Application and Alignment: Apply illumination corrections and align DAPI images across cycles</li> <li>SBS Preprocessing: Process SBS images, compensate channels, and perform preliminary barcode calling</li> <li>SBS Stitch and Crop: Stitch multi-site SBS images and crop for analysis</li> <li>Analysis: Integrate CP and SBS data, perform final barcode calling, and extract measurements</li> </ul> <p>This workflow follows the same processing pipeline described in the PCPIP specifications, adapted for the StarryNight CLI interface.</p> <p>All CellProfiler-based modules in this workflow follow a consistent three-step pattern:</p> <ol> <li>Generate LoadData files: Create CSV files that tell CellProfiler which images to process</li> <li>Generate CellProfiler pipelines: Create customized or point to existing CellProfiler pipeline files (cppipe files)</li> <li>Execute CellProfiler: Run CellProfiler with the generated files</li> </ol> <p>Beyond CellProfiler</p> <p>While this workflow focuses on CellProfiler-based modules, StarryNight also includes other algorithm types that don't follow this three-step pattern. For example, the Stitch and Crop steps use Fiji/ImageJ for image processing. See Algorithm Layer for details.</p>"},{"location":"user/example-pipeline-cli/#starting-point","title":"Starting Point","text":"<p>This guide assumes you have completed the Getting Started guide through the step \"Running Illumination Correction Calculation\". Before continuing, make sure you have the following environment variables set:</p> <pre><code># Set environment variables for convenience (using absolute paths to avoid path resolution issues)\nexport DATADIR=\"$(pwd)/scratch/fix_s1_input\"\nexport WKDIR=\"$(pwd)/scratch/fix_s1_output/workspace\"\nexport CP_PLUGINS=\"$(pwd)/scratch/CellProfiler-plugins/active_plugins/\"\n# Add new environment variable needed for the complete workflow\nexport INPUT_WKDIR=\"$(pwd)/scratch/fix_s1_input/Source1/workspace\"\n\n# FIJI_PATH will be set up later when needed for stitching\n</code></pre> <p>You should already have:</p> <ul> <li>Inventory and index generated</li> <li>Experiment configuration <code>(experiment.json</code>) created</li> <li>Illumination correction calculation completed</li> </ul> <p>We'll now expand from there to the full pipeline.</p>"},{"location":"user/example-pipeline-cli/#cp-illumination-application","title":"CP Illumination Application","text":"<p>Since we've already completed the CP Illumination Calculation, we'll continue with applying those corrections:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_apply/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_apply/\nmkdir -p ${WKDIR}/illum/cp/illum_apply/\n\n# Generate LoadData files\nstarrynight illum apply loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_apply \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy\n\n# Generate CellProfiler pipelines\nstarrynight illum apply cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_apply \\\n    -o ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_apply \\\n    -w ${WKDIR} \\\n    --use_legacy\n\n# Execute pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_apply/illum_apply_painting.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_apply \\\n    -o ${WKDIR}/illum/cp/illum_apply\n</code></pre>"},{"location":"user/example-pipeline-cli/#cp-segmentation-check","title":"CP Segmentation Check","text":"<p>Evaluate cell segmentation quality in CP images. This step generates quality control images that overlay the segmentation outlines on the original images, allowing you to manually inspect the segmentation accuracy. The output images will need to be reviewed to determine if the segmentation parameters need adjustment:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/cp/segcheck/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/cp/segcheck/\nmkdir -p ${WKDIR}/segcheck/cp/\n\n# Generate LoadData files\nstarrynight segcheck loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/cp/segcheck \\\n    -c ${WKDIR}/illum/cp/illum_apply \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy\n\n# Generate CellProfiler pipelines\nstarrynight segcheck cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/segcheck/ \\\n    -o ${WKDIR}/cellprofiler/cppipe/cp/segcheck \\\n    -w ${WKDIR} \\\n    --use_legacy\n\n# Execute pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/cp/segcheck/segcheck_painting.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/segcheck \\\n    -o ${WKDIR}/segcheck/cp/ \\\n    --uow batch_id,plate_id,well_id\n</code></pre>"},{"location":"user/example-pipeline-cli/#download-and-setup-fiji-required-for-stitching","title":"Download and Setup Fiji (Required for Stitching)","text":"<p>Before running the stitch and crop steps, you need to download and set up Fiji/ImageJ:</p> <p>Fiji Installation</p> <p>Fiji (Fiji Is Just ImageJ) is required for the stitching and cropping steps in both the CP and SBS tracks. CellProfiler cannot handle the large image sizes from stitched 6-well plates, so Fiji is used for this memory-intensive processing.</p> <p>Download Fiji from the official download page and extract it. Then set the path to the executable:</p> <pre><code># Set the path to your Fiji executable\nexport FIJI_PATH=\"/Applications/Fiji.app/Contents/MacOS/ImageJ-macosx\"\n# Linux: export FIJI_PATH=\"/path/to/Fiji.app/ImageJ-linux64\"\n# Windows: export FIJI_PATH=\"/path/to/Fiji.app/ImageJ-win64.exe\"\n</code></pre>"},{"location":"user/example-pipeline-cli/#cp-stitch-and-crop","title":"CP Stitch and Crop","text":"<p>Stitch multi-site Cell Painting images together and crop them for analysis. This step uses Fiji/ImageJ to combine multiple fields of view from each well into a single stitched image, then crops it into tiles suitable for downstream analysis:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/fiji/pipeline/cp/stitchcrop\n\n# Generate fiji pipeline\nstarrynight stitchcrop pipeline \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/fiji/pipeline/cp/stitchcrop \\\n    -w ${WKDIR}/stitchcrop/cp \\\n    --images ${WKDIR}/illum/cp/illum_apply \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy \\\n    --uow batch_id,plate_id,well_id\n\n# Execute fiji\nstarrynight stitchcrop fiji \\\n    -p ${WKDIR}/fiji/pipeline/cp \\\n    -f ${FIJI_PATH}\n</code></pre>"},{"location":"user/example-pipeline-cli/#sbs-illumination-calculation","title":"SBS Illumination Calculation","text":"<p>Calculate illumination correction functions for SBS images:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_calc/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_calc/\nmkdir -p ${WKDIR}/illum/sbs/illum_calc/\n\n# Generate LoadData files\nstarrynight illum calc loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_calc \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --sbs \\\n    --use_legacy\n\n# Generate CellProfiler pipelines\nstarrynight illum calc cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_calc/ \\\n    -o ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_calc \\\n    -w ${WKDIR} \\\n    --sbs \\\n    --use_legacy\n\n# Execute pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_calc/illum_calc_sbs.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_calc \\\n    -o ${WKDIR}/illum/sbs/illum_calc \\\n    --sbs\n</code></pre>"},{"location":"user/example-pipeline-cli/#sbs-illumination-application-and-alignment","title":"SBS Illumination Application and Alignment","text":"<p>Apply illumination correction to SBS images:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_apply/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_apply/\nmkdir -p ${WKDIR}/illum/sbs/illum_apply/\n\n# Generate LoadData files\nstarrynight illum apply loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_apply \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --sbs \\\n    --use_legacy\n\n# Generate CellProfiler pipelines\nstarrynight illum apply cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_apply \\\n    -o ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_apply \\\n    -w ${WKDIR} \\\n    --sbs \\\n    --use_legacy\n\n# Execute pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/sbs/illum/illum_apply/illum_apply_sbs.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/illum/illum_apply \\\n    -o ${WKDIR}/illum/sbs/illum_apply \\\n    --sbs\n</code></pre>"},{"location":"user/example-pipeline-cli/#sbs-preprocessing","title":"SBS Preprocessing","text":"<p>Prepare SBS images for analysis, including barcode calling:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/sbs/preprocess/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/sbs/preprocess/\nmkdir -p ${WKDIR}/preprocess/sbs/\n\n# Generate LoadData files\nstarrynight preprocess loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/sbs/preprocess/ \\\n    -c ${WKDIR}/illum/sbs/illum_apply/ \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy\n\n# Generate CellProfiler pipelines\nstarrynight preprocess cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/preprocess/ \\\n    -o ${WKDIR}/cellprofiler/cppipe/sbs/preprocess/ \\\n    -w ${WKDIR}/preprocess/sbs/ \\\n    -b ${INPUT_WKDIR}/metadata/Barcodes.csv \\\n    --use_legacy\n\n# Execute preprocessing pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/sbs/preprocess/preprocess_sbs.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/sbs/preprocess \\\n    -o ${WKDIR}/preprocess/sbs/ \\\n    -d ${CP_PLUGINS} \\\n    --sbs\n</code></pre>"},{"location":"user/example-pipeline-cli/#sbs-stitch-and-crop","title":"SBS Stitch and Crop","text":"<p>Stitch multi-site SBS images together and crop them for analysis. Similar to the CP track, this step combines multiple fields of view from each well into stitched images suitable for barcode calling and analysis:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/fiji/pipeline/sbs/stitchcrop\n\n# Generate fiji pipeline\nstarrynight stitchcrop pipeline \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/fiji/pipeline/sbs/stitchcrop \\\n    -w ${WKDIR}/stitchcrop/sbs \\\n    --images ${WKDIR}/preprocess/sbs/ \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy \\\n    --uow batch_id,plate_id,well_id\n\n# Execute fiji\nstarrynight stitchcrop fiji \\\n    -p ${WKDIR}/fiji \\\n    -f ${FIJI_PATH}\n</code></pre>"},{"location":"user/example-pipeline-cli/#analysis","title":"Analysis","text":"<p>Extract cellular measurements and generate the final dataset by combining CP and SBS data:</p> <pre><code># Create necessary directories\nmkdir -p ${WKDIR}/cellprofiler/loaddata/analysis/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/analysis/\nmkdir -p ${WKDIR}/analysis/\n\n# Generate LoadData files\nstarrynight analysis loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/analysis/ \\\n    -c ${WKDIR}/illum/cp/illum_apply/ \\\n    -p ${WKDIR}/preprocess/sbs/ \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy\n\n# FIXME:\n# Once stitchcrop is fully integrated into this tutorial, the above will be changed to something like this:\n# -c ${WKDIR}/stitchcrop/cp/cropped/ \\\n# -p ${WKDIR}/stitchcrop/sbs/cropped/ \\\n\n# Generate CellProfiler pipelines\nstarrynight analysis cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/analysis/ \\\n    -o ${WKDIR}/cellprofiler/cppipe/analysis/ \\\n    -w ${WKDIR}/analysis/ \\\n    -b ${INPUT_WKDIR}/metadata/Barcodes.csv \\\n    --use_legacy\n\n# Execute analysis pipelines\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/analysis/analysis.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/analysis \\\n    -o ${WKDIR}/analysis/ \\\n    -d ${CP_PLUGINS}\n</code></pre>"},{"location":"user/example-pipeline-cli/#common-parameters","title":"Common Parameters","text":"<p>Throughout the pipeline, you'll use these common parameters:</p> <ul> <li><code>--exp_config</code>: Path to the experiment configuration file</li> <li><code>--use_legacy</code>: Use pre-fabricated CellProfiler pipelines (recommended for stability)</li> <li><code>-i, --index</code>: Path to the index.parquet file</li> <li><code>-o, --output</code>: Output directory for generated files</li> <li><code>-w, --workspace</code>: Path to the workspace directory</li> <li><code>-l, --loaddata</code>: Path to LoadData CSV files</li> <li><code>-p, --pipeline</code>: Path to CellProfiler pipeline file or directory (for <code>stitchcrop fiji</code>, this is the directory containing pipeline files)</li> <li><code>-c, --corr_images</code>: Path to illumination-corrected images</li> <li><code>-b, --barcode</code>: Path to barcode CSV file for sequencing data</li> <li><code>--sbs</code>: Flag to process SBS images only</li> <li><code>--images</code>: Path to input images directory (used in stitchcrop)</li> <li><code>-j, --jobs</code>: Number of parallel jobs (used in stitchcrop fiji)</li> <li>FIXME: Decide whether needed <code>--uow</code>: Unit of work hierarchy for pipeline generation (e.g., <code>batch_id,plate_id,well_id</code> for well-level processing)</li> </ul>"},{"location":"user/example-pipeline-cli/#next-steps","title":"Next Steps","text":"<p>After completing this workflow, you can:</p> <ol> <li>Export the extracted data from the analysis output directories for use in downstream tools</li> <li>Perform statistical analysis on the extracted features using your preferred analysis framework</li> <li>Visualize results using tools like Python/Matplotlib or other visualization platforms</li> </ol>"},{"location":"user/example-pipeline-cli/#conclusion","title":"Conclusion","text":"<p>You've now learned how to set up and execute a complete StarryNight workflow for optical pooled screening data analysis. The modular structure of StarryNight allows for efficient processing of both CP and SBS images in parallel tracks, producing quality-controlled, normalized outputs suitable for downstream analysis. With these techniques, you can build robust image processing workflows for your own high-content screening experiments.</p> <p>For more detailed information on the concepts used throughout this workflow:</p> <ul> <li>See Architecture Overview for details on StarryNight's modular design</li> <li>See Parser Configuration for customizing path parsing for different data structures</li> </ul> <p>For Document Contributors</p> <p>This section contains editorial guidelines for maintaining this document. These guidelines are intended for contributors and maintainers, not end users.</p> <p>Document Purpose and Audience</p> <p>This document serves as a bridge between the introductory Getting Started guide and complete workflow implementation. It targets users who:</p> <ul> <li>Are familiar with basic StarryNight concepts</li> <li>Need to implement a complete processing pipeline</li> <li>Prefer using the CLI interface rather than Python API</li> <li>Need to understand both CP and SBS processing tracks</li> </ul> <p>Guiding Structure Principles</p> <ol> <li>Progressive disclosure approach - Start with a clear overview before diving into implementation details</li> <li>Parallel tracks visualization - Keep the CP and SBS tracks visually distinct in diagrams and structure</li> <li>Complete CLI commands - Provide fully copyable command blocks with all necessary parameters</li> <li>Directory creation consistency - Always include <code>mkdir</code> commands before operations that require directories</li> <li>Three-step pattern emphasis - Maintain the loaddata \u2192 cppipe \u2192 execute pattern for each CellProfiler-based module</li> </ol> <p>Content Style Principles</p> <ol> <li>Command formatting consistency - Format all CLI commands with consistent indentation and backslash line continuations</li> <li>Abbreviation usage - Define abbreviations (CP, SBS) once, then use consistently throughout</li> <li>Parameter explanation - Keep parameter explanations concise and grouped in the Common Parameters section</li> <li>Lightweight diagrams - Keep mermaid diagrams focused on structure without duplicating descriptions</li> <li>CLI vs Python differentiation - Clearly distinguish CLI-based approaches from Python API approaches</li> </ol> <p>Future Documentation Enhancements</p> <p>Consider expanding this document with:</p> <ul> <li>Troubleshooting guide for common errors and failure recovery</li> <li>Output verification checkpoints for each processing step</li> <li>Resource requirements and sizing guidelines</li> <li>Configuration customization examples for different experimental setups</li> <li>Intermediate results visualization techniques</li> <li>Pipeline variations for specialized use cases</li> </ul> <p>Document Relationships</p> <ul> <li>Getting Started dependency - This document builds directly on <code>getting-started.md</code> and should stay aligned with it</li> <li>Practical Integration complement - Serves as the CLI alternative to <code>architecture/08_practical_integration.md</code>'s Python API approach</li> <li>Module reference complementarity - Complements module-specific documentation without duplicating it</li> </ul> <p>Terminology Consistency</p> <ul> <li>Cell Painting (CP) - Refers to the morphological imaging workflow track</li> <li>Barcoding / Sequencing by Synthesis (SBS) - Refers to the barcoding workflow track</li> <li>LoadData files - CSV files that configure CellProfiler input images</li> <li>Experiment configuration - The <code>experiment.json</code> file that defines processing parameters</li> </ul>"},{"location":"user/getting-started/","title":"Getting Started with StarryNight","text":"<p>This guide will help you install StarryNight and run your first workflow using the CLI (Command Line Interface) approach. You'll set up the environment and calculate illumination correction functions for Cell Painting images.</p> <p>Implementation Approaches</p> <p>GUI Approach (coming soon): Point-and-click interface through Canvas for biologists who prefer visual workflows.</p> <p>CLI Approach (shown in this guide): Uses direct command-line commands for learning and exploration. It's a simpler way to understand workflow operations step-by-step.</p> <p>Python/Module Approach (used in production): Most users will execute these operations through Python code (as shown in <code>starrynight/notebooks/pypct/exec_pcp_generic_pipe.py</code>). This approach provides standardized components, containerized execution, and integration with the Canvas UI.</p> <p>This guide focuses on the CLI approach as a foundation. See Practical Integration for the Python implementation and Architecture Overview for system design details.</p>"},{"location":"user/getting-started/#installation","title":"Installation","text":"<p>StarryNight uses the Nix package manager to provide a consistent and reproducible environment:</p> <p>Install Nix:</p> <pre><code># Install Nix\nsh &lt;(curl -L https://nixos.org/nix/install) --daemon\n</code></pre> <p>Clone the Repository:</p> <pre><code># Clone the repository and navigate to it\ngit clone https://github.com/broadinstitute/starrynight.git\ncd starrynight\n</code></pre> <p>Set Up the Environment:</p> <pre><code># Set up the Nix development environment\nnix develop --extra-experimental-features nix-command --extra-experimental-features flakes .\n</code></pre> <p>Install Dependencies and Project:</p> <pre><code># Install basic dependencies\nuv sync\n\n# Install the project in editable mode with development tools\nuv pip install -e \".[dev]\"\n</code></pre> <p>Verify Installation:</p> <pre><code># Verify the installation\nstarrynight --help\n</code></pre> <p>For Developers:</p> <p>If you're developing for StarryNight, the setup process is the same as above. For detailed information on the project architecture and how to extend components, see the Architecture Overview.</p>"},{"location":"user/getting-started/#workflow-steps","title":"Workflow Steps","text":"<p>The following sections guide you through running a basic illumination correction calculation workflow for Cell Painting (CP) images. This process involves downloading sample data, setting up an experiment configuration, generating inventory and index files, and calculating illumination correction functions.</p> <p>Focus of This Guide</p> <p>This guide focuses only on the Cell Painting (CP) track and specifically on the illumination correction calculation step. The Complete Workflow Example will add the Sequencing by Synthesis (SBS) (commonly referred to as barcoding) track and show the full analysis workflow.</p> <pre><code>flowchart LR\n    Setup[\"Setup &amp; Preparation\"] --&gt; CPIllumCalc[\"CP Illumination Calculation\"]\n    style CPIllumCalc stroke:#0066cc</code></pre>"},{"location":"user/getting-started/#download-sample-data","title":"Download Sample Data","text":"<p>StarryNight includes curated test data (FIX-S1) that provides a complete, small-scale dataset for learning the workflow:</p> <pre><code># Create a directory for the sample data\nmkdir -p scratch\n\n# Download the FIX-S1 test dataset (36MB)\ncd scratch\nwget https://github.com/shntnu/starrynight/releases/download/v0.0.1/fix_s1_input.tar.gz\n\n# Verify the download integrity\necho \"ddba28e1593986013d10880678d2d7715af8d2ee1cfa11ae7bcea4d50c30f9e0  fix_s1_input.tar.gz\" | sha256sum -c\n\n# Extract the data\n# Ignore warnings like \"tar: Ignoring unknown extended header keyword\"\ntar -xzf fix_s1_input.tar.gz\n\n# Clean up macOS metadata files\nfind fix_s1_input -name '._*' -delete\nfind fix_s1_input -name '.DS_Store' -delete\n\n# Return to project root\ncd ..\n</code></pre> <p>This creates a <code>fix_s1_input/</code> directory containing Cell Painting and SBS imaging data from 2 wells with multiple sites and channels.</p> <p>Before running any commands, set up your data and workspace directories as environment variables:</p>"},{"location":"user/getting-started/#clone-cellprofiler-plugins-github-repository","title":"Clone CellProfiler plugins github repository","text":"<pre><code>cd scratch\ngit clone https://github.com/CellProfiler/CellProfiler-plugins.git\n\n# Return to project root\ncd ..\n</code></pre>"},{"location":"user/getting-started/#setup-environment-variables","title":"Setup environment variables","text":"<pre><code># Set environment variables for convenience (using absolute paths to avoid path resolution issues)\nexport DATADIR=\"$(pwd)/scratch/fix_s1_input\"\nexport WKDIR=\"$(pwd)/scratch/fix_s1_output/workspace\"\nexport CP_PLUGINS=\"$(pwd)/scratch/CellProfiler-plugins/active_plugins/\"\n\n# Additional environment variable needed for the complete workflow\nexport INPUT_WKDIR=\"$(pwd)/scratch/fix_s1_input/Source1/workspace\"\n</code></pre> <p>Environment Variable Organization</p> <p>The directory structure separates input data from output workspace:</p> <ul> <li><code>DATADIR</code>: Points to the downloaded sample data (input)</li> <li><code>WKDIR</code>: Points to your working directory where outputs will be generated</li> <li><code>INPUT_WKDIR</code>: Points to the input workspace folder, within which barcode files are located</li> <li><code>CP_PLUGINS</code>: Points to CellProfiler plugins needed for advanced processing</li> </ul>"},{"location":"user/getting-started/#create-experiment-configuration","title":"Create Experiment Configuration","text":"<p>The experiment configuration file defines parameters for your processing workflow:</p> <pre><code># Create necessary directories for the workflow\nmkdir -p scratch/fix_s1_output/workspace/\n\n# Generate a default experiment configuration template\nstarrynight exp init -e \"Pooled CellPainting [Generic]\" -o ${WKDIR}\n</code></pre> <p>Available Experiment Configurations</p> <p>StarryNight currently supports these experiment types:</p> <ul> <li><code>\"Pooled CellPainting [Generic]\"</code>: Standard Cell Painting workflow with SBS barcoding</li> <li><code>\"Pooled CellPainting [Stitchcrop]\"</code>: Cell Painting workflow with image stitching and cropping</li> </ul> <p>Both configurations use the same underlying experiment model and support the full range of Cell Painting and SBS processing steps. The choice determines which pipeline templates and processing modules are available for your workflow.</p> <p>This creates an <code>experiment_init.json</code> file in your workspace that you can edit to match your dataset's characteristics:</p> <p>Required Channel Mapping Configuration</p> <p>As of recent updates, StarryNight now requires custom channel mapping configurations in the <code>experiment_init.json</code> file. You must include both <code>cp_custom_channel_map</code> and <code>sbs_custom_channel_map</code> fields that map microscope channel names to semantic biological names.</p> <p>Additionally, the <code>sbs_cell_channel</code> and <code>sbs_mito_channel</code> parameters must be specified even when only working with Cell Painting (CP) data. Include these parameters with the same values as your CP channels.</p> <p>For the example experiment, the following values can be used.</p> <pre><code>{\n    \"barcode_csv_path\": \".\",\n    \"use_legacy\": false,\n    \"cp_img_overlap_pct\": 10,\n    \"cp_img_frame_type\": \"round\",\n    \"cp_acquisition_order\": \"snake\",\n    \"sbs_img_overlap_pct\": 10,\n    \"sbs_img_frame_type\": \"round\",\n    \"sbs_acquisition_order\": \"snake\",\n    \"cp_nuclei_channel\": \"DAPI\",\n    \"cp_cell_channel\": \"PhalloAF750\",\n    \"cp_mito_channel\": \"ZO1AF488\",\n    \"cp_custom_channel_map\": {\n        \"DAPI\": \"DNA\",\n        \"ZO1AF488\": \"ZO1\",\n        \"PhalloAF750\": \"Phalloidin\"\n    },\n    \"sbs_nuclei_channel\": \"DAPI\",\n    \"sbs_cell_channel\": \"PhalloAF750\",\n    \"sbs_mito_channel\": \"ZO1AF488\",\n    \"sbs_custom_channel_map\": {\n        \"DAPI\": \"DNA\",\n        \"A\": \"A\",\n        \"T\": \"T\",\n        \"G\": \"G\",\n        \"C\": \"C\"\n    }\n}\n</code></pre> <p>Key parameters explained:</p> <ul> <li><code>cp_img_overlap_pct</code>: Percentage overlap between adjacent images for stitching (typically 10%)</li> <li><code>cp_img_frame_type</code>: Image shape - \"round\" for circular fields, \"square\" for rectangular</li> <li><code>cp_acquisition_order</code>: Imaging pattern - \"snake\" for serpentine, \"rows\" for row-by-row</li> <li><code>cp_nuclei_channel</code>, <code>cp_cell_channel</code>, <code>cp_mito_channel</code>: Channel names for key cellular components</li> <li><code>cp_custom_channel_map</code>: Maps microscope channel names (like \"DAPI\", \"PhalloAF750\") to semantic biological names (like \"DNA\", \"Phalloidin\") used in analysis pipelines</li> <li><code>sbs_custom_channel_map</code>: Maps SBS channel names to their biological meanings for barcode analysis</li> <li><code>use_legacy</code>: Whether to use pre-tested pipeline templates (recommended: false for new experiments)</li> </ul> <p>Adjust the values to match your experiment setup.</p>"},{"location":"user/getting-started/#generate-inventory","title":"Generate Inventory","text":"<p>Create a catalog of all image files in your dataset:</p> <pre><code># Generate the inventory\nstarrynight inventory gen \\\n    -d ${DATADIR} \\\n    -o ${WKDIR}/inventory\n</code></pre> <p>The inventory is a comprehensive catalog of all files in your dataset that:</p> <ul> <li>Contains basic file information: path, name, extension</li> <li>Created by scanning the data directory recursively</li> <li>Stored as a Parquet file for efficient querying</li> </ul> <p>This command will create an inventory file:</p> <pre><code>${WKDIR}/inventory/\n\u251c\u2500\u2500 inv/                # Shard directory with temporary files\n\u2514\u2500\u2500 inventory.parquet   # Main inventory file\n</code></pre>"},{"location":"user/getting-started/#generate-index","title":"Generate Index","text":"<p>Parse the inventory to create a structured index with metadata:</p> <pre><code>starrynight index gen \\\n    -i ${WKDIR}/inventory/inventory.parquet \\\n    -o ${WKDIR}/index/\n</code></pre> <p>The index is a structured database of metadata extracted from file paths that:</p> <ul> <li>Contains rich, queryable information: dataset, batch, plate, well, site, channel info</li> <li>Is created by parsing file paths using a grammar-based parser</li> <li>Enables sophisticated filtering and selection of images</li> <li>Is stored as a structured Parquet file</li> </ul> <p>The result will be an <code>index.parquet</code> file containing structured metadata for each image. This index will be used in all subsequent processing steps through the <code>-i</code> parameter.</p> <p>Path Parsing System</p> <p>StarryNight automatically extracts metadata from file paths using a grammar-based parsing system. The default parser handles the file structure shown in this example, but you can write custom parsers for different organizations. If your data follows a different organization than the pattern used as an example here, you can customize the parser as described in the Parser Configuration guide.</p> <p>Expected Parse Errors</p> <p>During index generation, you may see multiple warning messages for files that don't match the expected image path patterns. This is normal and expected behavior:</p> <pre><code>Unable to parse: {'key': 'fix_s1_input/Source1/.DS_Store', ...} because of Unexpected token Token('DOT', '.')\nUnable to parse: {'key': 'fix_s1_input/Source1/workspace/._.DS_Store', ...} because of Unexpected token Token('WORKSPACE', 'workspace')\nUnable to parse: {'key': 'fix_s1_input/Source1/workspace/metadata/Barcodes.csv', ...} because of Unexpected token Token('WORKSPACE', 'workspace')\n</code></pre> <p>These warnings occur because:</p> <ul> <li>Hidden files like <code>.DS_Store</code> and <code>._*</code> can reappear even after deletion (macOS metadata files)</li> <li>The parser is designed specifically for image files with structured paths</li> <li>Non-image files (CSV, metadata) don't follow the expected naming pattern</li> </ul> <p>What this means: The parser is intentionally strict and only accepts properly formatted Cell Painting (CP) and SBS (Sequencing by Synthesis) image paths. While these warnings may seem numerous, they don't indicate failure - all valid image files are processed correctly and invalid files are safely skipped.</p>"},{"location":"user/getting-started/#create-experiment-file","title":"Create Experiment File","text":"<p>Initialize an experiment using your index and configuration:</p> <pre><code>starrynight exp new \\\n    -i ${WKDIR}/index/index.parquet \\\n    -e \"Pooled CellPainting [Generic]\" \\\n    -c ${WKDIR}/experiment_init.json \\\n    -o ${WKDIR}\n</code></pre> <p>This creates an <code>experiment.json</code> file with dataset-specific parameters derived from your index.</p>"},{"location":"user/getting-started/#run-illumination-correction-calculation","title":"Run Illumination Correction Calculation","text":"<p>Let's run the illumination correction calculation, which follows the standard CellProfiler module pattern of generating LoadData files, creating pipeline files, and executing CellProfiler:</p> <p>First, ensure the directories exist:</p> <pre><code>mkdir -p ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_calc/\nmkdir -p ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_calc/\nmkdir -p ${WKDIR}/illum/cp/illum_calc/\n</code></pre> <p>Pipeline Generation Approaches</p> <p>StarryNight offers two ways to generate CellProfiler pipelines:</p> <ul> <li>Pre-fabricated Pipelines: Uses established, tested pipeline templates (add <code>--use_legacy</code> flag)</li> <li>Dynamic Generation: Automatically generates pipelines based on configuration (omit the <code>--use_legacy</code> flag)</li> </ul> <p>This guide uses the pre-fabricated approach for stability.</p> <p>Directory Structure</p> <p>Throughout this guide, we're creating a workspace with this directory structure:</p> <pre><code>${WKDIR}/\n\u251c\u2500\u2500 cellprofiler/                # CellProfiler-related files\n\u2502   \u251c\u2500\u2500 loaddata/                # Generated LoadData CSV files\n\u2502   \u2514\u2500\u2500 cppipe/                  # Pipeline files\n\u251c\u2500\u2500 index/                       # Structured metadata\n\u2502   \u2514\u2500\u2500 index.parquet            # Index file with extracted metadata\n\u251c\u2500\u2500 illum/                       # Illumination correction files\n\u2502   \u251c\u2500\u2500 cp/                      # Cell Painting illumination\n\u2502   \u2514\u2500\u2500 sbs/                     # SBS illumination\n\u2514\u2500\u2500 experiment.json              # Experiment configuration\n</code></pre> <p>This structure separates inputs, intermediate results, and final outputs, maintaining clear data provenance throughout the workflow.</p> <p>Generate LoadData Files:</p> <pre><code># Generate loaddata files using established pipeline templates\nstarrynight illum calc loaddata \\\n    -i ${WKDIR}/index/index.parquet \\\n    -o ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_calc \\\n    --exp_config ${WKDIR}/experiment.json \\\n    --use_legacy\n</code></pre> <p>Generate CellProfiler Pipelines:</p> <pre><code># Generate CellProfiler pipeline files using established templates\nstarrynight illum calc cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_calc/ \\\n    -o ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_calc \\\n    -w ${WKDIR} \\\n    --use_legacy\n</code></pre> <p>Execute CellProfiler Pipelines:</p> <pre><code># The path must point to a specific .cppipe file, not a directory\nstarrynight cp \\\n    -p ${WKDIR}/cellprofiler/cppipe/cp/illum/illum_calc/illum_calc_painting.cppipe \\\n    -l ${WKDIR}/cellprofiler/loaddata/cp/illum/illum_calc \\\n    -o ${WKDIR}/illum/cp/illum_calc\n</code></pre>"},{"location":"user/getting-started/#verify-results","title":"Verify Results","text":"<p>The illumination correction files will be created in the output directory:</p> <pre><code>${WKDIR}/illum/cp/illum_calc/Batch1-Plate1\n\u251c\u2500\u2500 Plate1_IllumDNA.npy\n\u251c\u2500\u2500 Plate1_IllumPhalloidin.npy\n\u2514\u2500\u2500 Plate1_IllumZO1.npy\n</code></pre> <p>You can save the code below as a Python script (e.g., viz_example.py) and run it using uv:</p> <pre><code>uv run viz_example.py\n</code></pre> <pre><code># /// script\n# dependencies = [\n#   \"numpy\" ,\n#   \"matplotlib\",\n# ]\n# ///\n# Load one of the illumination correction files\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwkdir = os.environ.get(\"WKDIR\", \"./scratch/fix_s1_output/workspace\")\ndata = np.load(f\"{wkdir}/illum/cp/illum_calc/Batch1-Plate1/Plate1_IllumDNA.npy\")\n\n# Create a visualization\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap=\"viridis\")\nplt.colorbar()\nplt.title(\"DNA Illumination Correction\")\nplt.savefig(f\"{wkdir}/illum/cp/illum_calc/Batch1-Plate1/Plate1_IllumDNA.png\")\nplt.show()\n</code></pre>"},{"location":"user/getting-started/#advanced-cli-options","title":"Advanced CLI Options","text":"<p>StarryNight commands support additional options to customize processing:</p> <p>Path Masking: Specify a path prefix for resolving file locations using the <code>-m/--path_mask</code> option. This sets the base directory path that gets prepended to relative file paths in the generated LoadData CSV files:</p> <pre><code># Set custom path prefix for file resolution\nstarrynight illum calc loaddata -m \"/absolute/path/to/data\" ...\n</code></pre> <p>Parallel Processing: Control the number of parallel jobs with the <code>-j/--jobs</code> option:</p> <pre><code># Run with 8 parallel jobs (default is 50)\nstarrynight cp -j 8 -p /path/to/pipeline.cppipe ...\n</code></pre> <p>CellProfiler Plugins: Specify a directory containing CellProfiler plugins:</p> <pre><code>starrynight cp -d /path/to/plugins -p /path/to/pipeline.cppipe ...\n</code></pre>"},{"location":"user/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Continue to the Complete Workflow Example</li> <li>Check the Architecture Overview to understand the system structure</li> <li>For the Python/Module approach used in production, see Practical Integration</li> </ul> <p>For Document Contributors</p> <p>This section contains editorial guidelines for maintaining this document. These guidelines are intended for contributors and maintainers, not end users.</p> <p>Purpose and Audience</p> <ul> <li>Introductory Focus - This document is a user's first hands-on experience with StarryNight</li> <li>CLI Emphasis - Prioritize the CLI approach as an accessible entry point</li> <li>Single Path with Options - Present one primary workflow while noting alternatives</li> <li>Assumed Knowledge - Users understand basic command line but not StarryNight architecture</li> </ul> <p>Structure Principles</p> <ol> <li>Clear section headings - Use H2 headings for main workflow steps without numbers</li> <li>Notes for alternatives - Use MkDocs admonitions to present alternatives without disrupting flow</li> <li>Quick start spirit - Keep explanations brief and focused on practical execution</li> <li>Progressive detail - Start with setup, then basic workflow, then advanced options</li> <li>Clear prerequisites - Ensure directory creation and dependencies are explicitly mentioned</li> </ol> <p>Content Style Guidelines</p> <ol> <li>Command formatting - Include descriptive comments in code blocks</li> <li>Bold subheadings - Use bold text rather than deeper heading levels for substeps</li> <li>Copy-pastable commands - Ensure commands work as written without modification</li> <li>Environment variables - Use consistent variables (DATADIR, WKDIR)</li> <li>Expected outputs - Show example outputs and file structures where appropriate</li> </ol> <p>Terminology Consistency</p> <ul> <li>\"CLI approach\" vs \"Python/Module approach\" - Different ways to use StarryNight</li> <li>\"Pre-fabricated pipelines\" vs \"Dynamic pipeline generation\" - Two pipeline generation methods</li> <li>\"Workflow\" - The end-to-end image processing sequence</li> <li>\"Pipeline\" - The CellProfiler processing definition</li> <li>\"LoadData files\" - CSV files that tell CellProfiler which images to process</li> </ul>"},{"location":"user/operational-challenges/","title":"StarryNight Operational Challenges FAQ","text":""},{"location":"user/operational-challenges/#introduction","title":"Introduction","text":"<p>This FAQ addresses critical operational challenges faced by research teams transitioning from PCPIP to StarryNight for high-throughput microscopy image processing. These questions emerged from real-world discussions with users processing Optical Pooled Screening (OPS) data at scale.</p> <p>Status Legend:</p> <ul> <li>\u2705 Available: Ready to use with current system</li> <li>\u2699\ufe0f Workaround: Possible but requires manual coordination</li> <li>\ud83d\udea7 Planned: Architecturally supported, needs implementation</li> <li>\ud83d\udd2c Design: Requires significant architectural consideration</li> </ul>"},{"location":"user/operational-challenges/#1-starting-processing-before-all-images-are-available","title":"1. Starting Processing Before All Images Are Available","text":"<p>Real-World Scenario: Research teams often need to start processing before all images are available, particularly when SBS image generation takes weeks and they want to process the phenotyping arm while barcoding data is still being acquired.</p> <p>Status: \u2705 Currently Available</p> <p>Technical Solution: StarryNight's modular pipeline architecture supports this through composition and registry customization:</p> <p>Code Location: <code>/starrynight/src/starrynight/pipelines/pcp_generic.py</code></p> <p>Current implementation shows parallel CP and SBS processing:</p> <pre><code># Existing combined pipeline structure\nParallel([\n    Seq([cp_illum_calc_loaddata.pipe, cp_illum_calc_cpipe.pipe, ...]),  # CP pipeline\n    Seq([sbs_illum_calc_loaddata.pipe, sbs_illum_calc_cpipe.pipe, ...])  # SBS pipeline\n])\n</code></pre> <p>Implementation Steps:</p> <ol> <li>Create Pipeline Variants: Copy <code>pcp_generic.py</code> to create specialized versions (requires developer)</li> <li>Register New Pipelines: Add to <code>PIPELINE_REGISTRY</code> in <code>/starrynight/src/starrynight/pipelines/registry.py</code></li> <li>Execute Separately:    <pre><code># Current: Create full experiment\nstarrynight exp new -i index.parquet -e \"Pooled CellPainting [Generic]\" -c config.json -o output/\n\n# Proposed: Separate pipelines (would need implementation)\nstarrynight exp new -e \"Pooled CellPainting [Phenotypic]\" -c cp_config.json -o cp_output/\nstarrynight exp new -e \"Pooled CellPainting [SBS]\" -c sbs_config.json -o sbs_output/\n</code></pre></li> </ol> <p>Current Reality: Must modify codebase to add new pipeline types. The modular architecture supports this, but requires development effort rather than configuration.</p> <p>Current Workaround: Use existing pipeline but filter processing to skip unavailable data (discussed by developers as viable workaround).</p>"},{"location":"user/operational-challenges/#2-adding-cycles-mid-experiment","title":"2. Adding Cycles Mid-Experiment","text":"<p>Real-World Scenario: Experiments sometimes require additional cycles mid-process due to failed acquisitions, making the original cycle count configuration incorrect.</p> <p>Status: \u2705 Currently Available (with data loss)</p> <p>Technical Solution: StarryNight auto-detects cycles from data structure, not configuration.</p> <p>Code Evidence: In <code>/starrynight/src/starrynight/experiments/pcp_generic.py</code>: <pre><code># Auto-detects number of cycles from actual data\nsbs_n_cycles = (\n    sbs_images_df.select(pl.col(\"cycle_id\").unique().count())\n    .collect()\n    .rows()[0][0]\n)\n</code></pre></p> <p>Recovery Process:</p> <ol> <li>Add New Cycle Images: Place additional cycle data in the dataset</li> <li>Re-inventory: <code>starrynight inventory</code> (detects new files)</li> <li>Re-index: <code>starrynight index</code> (updates cycle count automatically)</li> <li>Reconfigure: <code>starrynight exp new</code> (creates new experiment with updated cycle count)</li> <li>Reprocess: All downstream processing adapts to detected cycles automatically</li> </ol> <p>Limitation: Previous processing results are lost and must be recomputed. No incremental update capability currently exists.</p> <p>Limitation: If you need to exclude specific cycles (e.g., skip corrupted cycle 5), this must be handled in the CellProfiler pipeline configuration, not in StarryNight's index layer.</p>"},{"location":"user/operational-challenges/#3-resource-management-and-retry-logic","title":"3. Resource Management and Retry Logic","text":""},{"location":"user/operational-challenges/#3a-failed-job-retry-on-larger-machines","title":"3A. Failed Job Retry on Larger Machines","text":"<p>Real-World Scenario: Tasks occasionally fail due to resource constraints and need retry with increased computational resources.</p> <p>Status: \ud83d\udea7 Planned Architecture (6-12 months)</p> <p>Current Reality: Basic retry exists through Snakemake backend (<code>/pipecraft/src/pipecraft/backend/snakemake.py</code>), but no resource escalation.</p> <p>Planned Solution - Resource Hints System: The architecture supports this through a planned resource hints API:</p> <pre><code># Conceptual module configuration\nmodule_config = {\n    \"cpu_cores\": 4,\n    \"memory_gb\": 16,\n    \"retry_with_larger\": True  # Escalate resources on failure\n}\n</code></pre> <p>Technical Flow: StarryNight modules \u2192 PipeCraft compilation \u2192 Backend execution (Snakemake/AWS Batch)</p> <p>Current Workaround: Manual job resubmission with modified resource configurations.</p>"},{"location":"user/operational-challenges/#3b-rerunning-successful-jobs-with-new-pipeline-versions","title":"3B. Rerunning Successful Jobs with New Pipeline Versions","text":"<p>Real-World Scenario: Users sometimes need to rerun specific subsets of previously successful jobs with updated pipeline versions.</p> <p>Status: \u2699\ufe0f Manual Workaround Available</p> <p>Current Capability: Can rerun individual modules or entire workflows through Conductor backend, but no bulk selection interface.</p> <p>Technical Reality: Developers acknowledge this wasn't part of the original design considerations, so implementing bulk job selection would require significant architectural planning.</p> <p>Current Workaround: Force rerun of specific modules through Canvas UI or rebuild entire pipeline with new parameters.</p>"},{"location":"user/operational-challenges/#4-handling-corrupted-images","title":"4. Handling Corrupted Images","text":"<p>Real-World Scenario: Corrupted images occasionally need to be excluded from experiments or replaced with dummy images to maintain data consistency.</p> <p>Status: \ud83d\udea7 Architectural Design Phase (6-12 months)</p> <p>Technical Challenge: Index files use Parquet format, which is (practically, given the size) immutable by design.</p> <p>Code Location: Index structure in <code>/starrynight/src/starrynight/algorithms/index.py</code></p> <p>Proposed Solution - Filter-Based Approach: Rather than modifying index files, create filtering layers:</p> <ol> <li>Blocklist Creation: Generate separate index files identifying corrupted images</li> <li>Anti-join Processing: Filter main index during data loading:    <pre><code># Conceptual filtering approach using polars\nfiltered_df = main_index_df.join(blacklist_df, on=\"file_path\", how=\"anti\")\n</code></pre></li> <li>Marimo Interface: Interactive notebook for filter creation and management</li> <li>Workflow Integration: All downstream modules respect filtered indices</li> </ol> <p>Implementation Gap: No current CLI commands for index manipulation. Would need new filtering utilities in <code>/starrynight/src/starrynight/utils/dfutils.py</code>.</p> <p>Current Workaround: Manual file deletion and complete re-indexing (requires careful coordination).</p>"},{"location":"user/operational-challenges/#5-different-pipeline-versions-per-plate","title":"5. Different Pipeline Versions Per Plate","text":"<p>Real-World Scenario: Different plates within the same experiment sometimes require different pipeline versions, with the need for variation only discovered during processing.</p> <p>Status: \u2699\ufe0f Currently Available (via multiple projects)</p> <p>Technical Solution: Index filtering + separate project management</p> <p>Code Location: <code>/starrynight/src/starrynight/utils/dfutils.py</code> provides filtering: <pre><code>def filter_df_by_column_val(df: pl.LazyFrame, column: str, val: str) -&gt; pl.LazyFrame:\n    filtered_df = df.filter(pl.col(column).eq(val))\n    return filtered_df\n</code></pre></p> <p>Implementation Workflow: Developers suggest a two-step approach: create separate Canvas projects for each plate group, using index filtering during project setup to include only the target plates for each pipeline version.</p> <ol> <li>Canvas UI: Create separate projects for each plate</li> <li>Index Filtering: During project setup, filter to include only target plate</li> <li>Pipeline Assignment: Configure different pipeline versions per project</li> <li>Coordinate Results: Manually manage multiple project outputs</li> </ol> <p>Developer Assessment: While functional, this approach requires manual coordination across multiple projects - a workable but inelegant solution similar to current multi-run workflows.</p> <p>User Requirements: Understanding of project management and index filtering concepts.</p> <p>Limitation: Not elegant - requires multiple project coordination. No single-project, multi-pipeline capability exists.</p>"},{"location":"user/operational-challenges/#6-different-pipeline-versions-per-well","title":"6. Different Pipeline Versions Per Well","text":"<p>Real-World Scenario: Individual wells occasionally require different pipeline processing than other wells within the same experiment.</p> <p>Status: \u2699\ufe0f Technically Possible (extreme edge case)</p> <p>Technical Solution: Same approach as plate-level filtering but at well granularity through Canvas project creation with index filtering.</p> <p>Technical Reality: Developers consider this an extreme edge case requiring substantial manual coordination. While technically possible through the filtering system, it's not streamlined for regular use.</p> <p>User Requirements: Advanced workflow management skills and deep understanding of granular data selection.</p> <p>Limitation: Functional but not elegant. Represents the boundary of practical workflow complexity.</p>"},{"location":"user/operational-challenges/#7-machine-failures-during-job-launch","title":"7. Machine Failures During Job Launch","text":"<p>Real-World Scenario: Job launch machines occasionally fail or become unresponsive during processing.</p> <p>Status: \u2705 Built-in Architecture Strength</p> <p>Technical Solution: Cloud-native design provides automatic resilience</p> <p>Technical Explanation: StarryNight's cloud-native design assumes S3-based storage for all operations, which means job state is automatically preserved in cloud storage rather than being dependent on local machine storage.</p> <p>Architecture Benefits:</p> <ul> <li>S3 Storage Backend: All state persisted to cloud storage</li> <li>Stateless Job Design: Jobs can be relaunched on any machine</li> <li>AWS Batch Recovery: Automatic instance failure detection and reallocation</li> <li>No EFS Required: S3-based approach more robust than EFS-backed alternatives</li> </ul> <p>User Experience: Transparent recovery - handled automatically at infrastructure level.</p>"},{"location":"user/operational-challenges/#8-incorrect-channel-mapping-information","title":"8. Incorrect Channel Mapping Information","text":"<p>Real-World Scenario: Channel mapping information provided during experiment setup is sometimes incorrect, requiring updates to the original configuration.</p> <p>Status: \u2705 Currently Available with built-in validation</p> <p>Technical Solution: StarryNight already provides syntactic validation</p> <p>Code Evidence: In <code>pcp_generic.py</code>: <pre><code># Existing channel validation\nfor cp_custom_channel in init_config_parsed.cp_custom_channel_map.keys():\n    assert cp_custom_channel in cp_channel_list\n</code></pre></p> <p>Validation Capabilities:</p> <ul> <li>Syntactic: Ensures channel names exist in indexed data</li> <li>Error Prevention: Clear error messages for non-existent channels</li> <li>Configuration Update: Modify mappings through Project Settings or experiment config</li> </ul> <p>Recovery Process:</p> <ol> <li>Update Channel Mapping: Modify experiment configuration with correct channel assignments</li> <li>Validate Configuration: Built-in checks verify channel names against index</li> <li>Reconfigure Project: Apply new mapping to all modules</li> <li>Full Rerun: Execute complete pipeline with corrected mapping</li> </ol> <p>Limitation: Only syntactic validation exists. Semantic validation (e.g., detecting swapped biological meanings) requires manual verification.</p> <p>User Interface: Available through Canvas Project Settings at <code>/canvas/app/dashboard/project/id/[id]/view/project-settings/</code></p>"},{"location":"user/operational-challenges/#9-incorrect-non-channel-parameters","title":"9. Incorrect Non-Channel Parameters","text":"<p>Real-World Scenario: Non-channel parameters like stitchcrop hyperparameters or barcode file paths sometimes need correction due to initially incorrect information or updated requirements.</p> <p>Status: \u2705 Fully Supported</p> <p>Technical Implementation: Parameter updates through UI and configuration management</p> <p>UI Access: Canvas project settings interface at <code>/canvas/app/dashboard/project/id/[id]/view/project-settings/</code></p> <p>Common Parameter Categories:</p> <ul> <li>Stitchcrop hyperparameters: Round vs. square acquisition settings</li> <li>Barcode CSV locations: Update path to correct barcode files</li> <li>Analysis thresholds: CellProfiler module parameters</li> <li>Output configurations: Result storage and naming</li> </ul> <p>Update Process:</p> <ol> <li>Navigate to Project Settings: Use web interface for parameter modification</li> <li>Update Relevant Parameters: Modify specific configuration values</li> <li>Save and Reconfigure: Apply changes to affected modules</li> <li>Selective Rerun: Execute only modules affected by parameter changes</li> </ol> <p>Alternative Interface: Parameters can also be modified in Marimo execution notebooks for advanced users.</p> <p>User Requirements: Understanding of parameter impacts on different pipeline stages and module dependencies.</p>"},{"location":"user/operational-challenges/#10-mixed-round-and-square-acquisition","title":"10. Mixed Round and Square Acquisition","text":"<p>Real-World Scenario: Experiments often combine different acquisition patterns, such as round acquisition for phenotyping and square acquisition for SBS imaging.</p> <p>Status: \u2705 Built-in Support</p> <p>Technical Implementation: Independent configuration for CP and SBS acquisition types</p> <p>Code Evidence: In <code>/starrynight/src/starrynight/experiments/pcp_generic.py</code>: <pre><code>class CPConfig(BaseModel):\n    img_frame_type: ImageFrameType = Field(ImageFrameType.ROUND)\n    acquisition_order: AcquisitionOrderType = Field(AcquisitionOrderType.SNAKE)\n\nclass SBSConfig(BaseModel):\n    img_frame_type: ImageFrameType = Field(ImageFrameType.ROUND)\n    acquisition_order: AcquisitionOrderType = Field(AcquisitionOrderType.SNAKE)\n</code></pre></p> <p>Configuration Options:</p> <ul> <li>Cell Painting Config: Independent image frame type (<code>ROUND</code>/<code>SQUARE</code>) and acquisition order</li> <li>SBS Config: Separate image frame type and acquisition pattern settings</li> <li>Module Support: Stitchcrop module automatically handles different acquisition types</li> </ul> <p>Usage Examples:</p> <ul> <li>Phenotypic arm: <code>ImageFrameType.ROUND</code> with <code>AcquisitionOrderType.SNAKE</code></li> <li>SBS arm: <code>ImageFrameType.SQUARE</code> with <code>AcquisitionOrderType.SNAKE</code></li> </ul> <p>User Interface: Settings configurable through standard experiment setup process.</p> <p>User Requirements: Understanding of acquisition type implications for downstream image processing.</p>"},{"location":"user/operational-challenges/#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started Guide - Basic StarryNight setup and usage patterns</li> <li>Parser Configuration - Data indexing and path parsing configuration</li> <li>Architecture Overview - System design and component interactions</li> <li>Pipeline Composition - Understanding modular pipeline design</li> </ul> <p>This FAQ reflects real-world usage scenarios and technical discussions with StarryNight users. The solutions balance practical implementation with architectural constraints, prioritizing functionality for competent users over simplified interfaces for every edge case.</p>"},{"location":"user/parser-configuration/","title":"Parser Configuration","text":"<p>This guide explains how to configure and customize path parsers in StarryNight to work with your own data organization.</p>"},{"location":"user/parser-configuration/#understanding-path-parsers","title":"Understanding Path Parsers","text":"<p>StarryNight uses a grammar-based path parsing system to extract structured metadata from file paths.</p> <pre><code>flowchart LR\n    Files[\"Raw File Paths\"] --&gt;|Grammar Rules| Parser[\"Path Parser\"]\n    Parser --&gt;|Transformer| Metadata[\"Structured Metadata\"]\n    Metadata --&gt; Index[\"Index Generation\"]\n\n    classDef default stroke:#333,stroke-width:1px;</code></pre>"},{"location":"user/parser-configuration/#how-path-parsing-works","title":"How Path Parsing Works","text":"<p>The StarryNight parser consists of three components:</p> <ol> <li>Lexer - Breaks the file path into tokens using regular expressions</li> <li>Grammar Rules - Defines token combinations and organization (in <code>.lark</code> file)</li> <li>Transformer - Converts the parsed structure into a Python dictionary</li> </ol> <p>This architecture enables flexible, robust parsing without relying on brittle string splitting.</p>"},{"location":"user/parser-configuration/#the-default-vincent-parser","title":"The Default \"Vincent\" Parser","text":"<p>StarryNight's default parser handles file paths with this structure, which is commonly produced by Phenix imaging systems:</p> <pre><code>[dataset]/[source_id]/[batch_id]/images/[plate_id]/[experiment_id]/Well[well_id]_Point[site_id]_[index]_Channel[channels]_Seq[sequence].ome.tiff\n</code></pre> <p>Example: <pre><code>MyDataset/Source1/Batch1/images/Plate1/20X_CP_Plate1/WellA01_PointA01_0_ChannelDAPI,AF488,AF647_Seq0.ome.tiff\n</code></pre></p> <p>The parser handles variations like:</p> <ul> <li>Sequencing-by-synthesis (SBS) folders vs Cell Painting (CP) folders</li> <li>Aligned images vs raw images</li> <li>Metadata files vs image files</li> <li>Illumination files</li> </ul>"},{"location":"user/parser-configuration/#understanding-the-grammar-file","title":"Understanding the Grammar File","text":"<p>The default grammar file (<code>path_parser_vincent.lark</code>) defines rules for parsing:</p> <pre><code>// Top-level rule - starting point for parsing\nstart: sep? dataset_id sep source_id sep _root_dir\n\n// Directory structure rules\n_root_dir: batch_id sep (_images_root_dir | _illum_root_dir | _images_aligned_root_dir | _workspace_root_dir)\n\n_images_root_dir: \"images\"i sep plate_id sep _plate_root_dir\n...\n</code></pre> <p>Rules prefixed with underscore (e.g., <code>_root_dir</code>) are internal structural rules that don't map to output metadata fields. Rules without underscores become fields in the output.</p>"},{"location":"user/parser-configuration/#customizing-the-parser","title":"Customizing the Parser","text":""},{"location":"user/parser-configuration/#when-to-create-a-custom-parser","title":"When to Create a Custom Parser","text":"<p>You'll need a custom parser when:</p> <ul> <li>Your file organization differs from the default pattern</li> <li>You need to extract different metadata fields</li> <li>You have a unique naming convention</li> </ul>"},{"location":"user/parser-configuration/#specifying-a-custom-parser","title":"Specifying a Custom Parser","text":"<p>Specify a custom parser with the CLI:</p> <pre><code>starrynight index gen \\\n    -i ./workspace/inventory/inventory.parquet \\\n    -o ./workspace/index/ \\\n    --parser /path/to/custom/parser.lark\n</code></pre>"},{"location":"user/parser-configuration/#creating-a-custom-grammar-file","title":"Creating a Custom Grammar File","text":"<p>To create a custom parser:</p> <ol> <li>Document your file patterns and identify metadata components to extract</li> <li>Write a <code>.lark</code> file that defines the path structure</li> <li>Test your grammar against sample paths</li> <li>Use it in your workflow with the <code>--parser</code> parameter</li> </ol>"},{"location":"user/parser-configuration/#example-custom-grammar-file","title":"Example: Custom Grammar File","text":"<p>Example grammar for a different file organization:</p> <pre><code>// Custom grammar for example_lab file organization\nstart: sep? project_name sep experiment_name sep plate_id sep _image_file\n\n_image_file: well_id \"_\" site_id \"_\" channel \"_\" cycle_id \".\" extension\n\nproject_name: stringwithdashcommaspace\nexperiment_name: stringwithdashcommaspace\nplate_id: string\nwell_id: (LETTER | DIGIT)~2\nsite_id: DIGIT~1..4\nchannel: stringwithdash\ncycle_id: DIGIT~1..2\nextension: stringwithdots\n\nsep: \"/\"\nstring: (LETTER | DIGIT)+\nstringwithdash: (string | \"-\")+\nstringwithdashcommaspace: ( string | \"-\" | \"_\" | \",\" | \" \" )+\nstringwithdots: ( string | \".\" )+\nDIGIT: \"0\"..\"9\"\n\n%import common.LETTER\n</code></pre> <p>Parses paths like: <pre><code>MyProject/Experiment-2023-05/Plate1/A1_01_DAPI_01.tiff\n</code></pre></p>"},{"location":"user/parser-configuration/#testing-custom-parsers","title":"Testing Custom Parsers","text":"<ol> <li> <p>Use Lark Parser IDE: Test at Lark Parser IDE to visualize parse trees.</p> </li> <li> <p>Test with sample paths:</p> </li> </ol> <pre><code>from lark import Lark\n\n# Load grammar and test paths\nparser = Lark.open('/path/to/grammar.lark', parser='lalr')\npaths = ['MyProject/Experiment-2023-05/Plate1/A1_01_DAPI_01.tiff']\n\nfor path in paths:\n    try:\n        tree = parser.parse(path)\n        print(f\"\u2713 Parsed: {path}\")\n    except Exception as e:\n        print(f\"\u2717 Failed: {path} - {e}\")\n</code></pre>"},{"location":"user/parser-configuration/#parser-architecture","title":"Parser Architecture","text":"<p>The parser works through three layers:</p> <ol> <li>Lexer: Tokenizes paths using regexes (uppercase rules like <code>DIGIT</code>)</li> <li>Parser: Builds a tree using grammar rules (lowercase rules like <code>well_id</code>)</li> <li>Transformer: Maps parse tree to metadata dictionary (handles special cases)</li> </ol>"},{"location":"user/parser-configuration/#best-practices","title":"Best Practices","text":"<p>When creating parsers:</p> <ol> <li>Start simple - Begin with basic grammar and add complexity as needed</li> <li>Test thoroughly - Validate with diverse file paths</li> <li>Consider performance - Complex parsers can slow index generation</li> <li>Document your schema - Document your file organization pattern</li> <li>Separate concerns:<ul> <li>Lexer for basic pattern matching</li> <li>Grammar for structural relationships</li> <li>Transformer for conversion logic</li> </ul> </li> </ol>"},{"location":"user/parser-configuration/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues:</p> <ul> <li>Parsing errors: Check grammar rules, test in Lark IDE, add permissive rules</li> <li>Missing metadata: Ensure grammar extracts all needed fields with matching names</li> <li>Performance issues: Simplify complex rules, reduce nesting, move pattern matching to lexer</li> </ul>"},{"location":"user/parser-configuration/#using-your-custom-parser","title":"Using Your Custom Parser","text":"<p>Use your parser in the index generation step:</p> <pre><code>starrynight index gen \\\n    -i ./workspace/inventory/inventory.parquet \\\n    -o ./workspace/index/ \\\n    --parser /path/to/custom/parser.lark\n</code></pre> <p>Validate results by examining the <code>index.parquet</code> file.</p> <p>Custom Transformers</p> <p>Creating custom transformers requires modifying source code. For most users, a custom grammar file provides sufficient flexibility.</p> <p>For Document Contributors</p> <p>Guidelines for maintaining this document:</p> <p>Audience: Users adapting StarryNight to non-standard file organization with sufficient technical knowledge of grammar files, and developers extending functionality.</p> <p>Organization Principles:</p> <ol> <li>Progressive disclosure (basics \u2192 advanced)</li> <li>Practical, functional examples</li> <li>Implementation details for extensibility</li> </ol> <p>Style Guidelines:</p> <ol> <li>Consistent command formatting</li> <li>Define technical terms at first use</li> <li>Prioritize practical guidance over theory</li> <li>Use real-world examples</li> </ol> <p>Related Docs: Builds on Getting Started, complements Complete Workflow Example, references Architecture Overview for advanced details.</p>"}]}