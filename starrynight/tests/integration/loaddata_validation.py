"""LoadData CSV validation framework.

This module provides a configurable framework for validating LoadData CSV files
generated by different pipeline steps in the StarryNight workflow.

The framework supports:
1. Core validations for common fields across all LoadData files
2. Pipeline-specific validations defined by configuration
3. Dynamic column group validation based on naming patterns
4. Extensible SQL-based custom checks
"""

import re
from pathlib import Path
from typing import Any, Optional

import duckdb
import pandas as pd


class LoadDataValidator:
    """Validates LoadData CSV files with configurable pipeline-specific rules."""

    def __init__(self, pipeline_config: dict[str, Any]) -> None:
        """Initialize validator with pipeline-specific configuration.

        Args:
            pipeline_config: Configuration for specific pipeline type validation.
                            Should contain keys for validation rules, required columns, etc.

        """
        self.config = pipeline_config
        self.errors = []

    def validate(
        self,
        generated_csv_path: Path,
        ref_csv_path: Path,
    ) -> list[str]:
        """Validate generated LoadData CSV against reference file.

        Args:
            generated_csv_path: Path to generated CSV
            ref_csv_path: Path to reference CSV

        Returns:
            Error messages (empty list if validation passed)

        """
        self.errors = []

        try:
            # Set up the database connection with both CSVs as views
            with duckdb.connect(":memory:") as conn:
                self._create_views(conn, generated_csv_path, ref_csv_path)

                # Run core validations (common across all pipeline types)
                self._validate_core_metadata(conn)

                # Run column existence validation
                self._validate_required_columns(conn)

                # Run column group pattern validations
                for group_config in self.config.get("column_groups", []):
                    self._validate_column_group(conn, group_config)

                # Run pipeline-specific checks
                self._run_pipeline_specific_checks(conn)

        except duckdb.Error as e:
            self.errors.append(
                f"Database error: {str(e)} (check CSV format/types)"
            )

        return self.errors

    def _create_views(
        self,
        conn: duckdb.DuckDBPyConnection,
        generated_csv_path: Path,
        ref_csv_path: Path,
    ) -> None:
        """Create database views for both CSVs.

        Args:
            conn: Database connection
            generated_csv_path: Path to generated CSV
            ref_csv_path: Path to reference CSV

        """
        conn.execute(
            f"CREATE VIEW generated AS SELECT * FROM read_csv_auto('{str(generated_csv_path)}')"
        )
        conn.execute(
            f"CREATE VIEW reference AS SELECT * FROM read_csv_auto('{str(ref_csv_path)}')"
        )

    def _validate_core_metadata(self, conn: duckdb.DuckDBPyConnection) -> None:
        """Validate core metadata columns common to all pipelines.

        Args:
            conn: Database connection with views

        """
        # Get the core metadata columns from the config or use defaults
        core_metadata = self.config.get(
            "core_metadata", ["Metadata_Plate", "Metadata_Site"]
        )

        # Validate each core metadata column exists in both reference and generated
        for column in core_metadata:
            self._check_column_exists(conn, column)

        # Compare metadata values between reference and generated
        for col, label in [
            ("Metadata_Plate", "plates"),
            ("Metadata_Site", "sites"),
        ]:
            if col in core_metadata:
                self._check_column_values(conn, col, label)

        # Validate Well if it's in core_metadata
        if "Metadata_Well" in core_metadata:
            self._check_column_values(conn, "Metadata_Well", "wells")

    def _validate_required_columns(
        self, conn: duckdb.DuckDBPyConnection
    ) -> None:
        """Validate required columns exist and aren't null.

        Args:
            conn: Database connection with views

        """
        required_columns = self.config.get("required_columns", [])

        # Check if required columns exist
        for column in required_columns:
            self._check_column_exists(conn, column)

        # Check for nulls in required columns
        if required_columns:
            where_clause = " OR ".join(
                [f"{col} IS NULL" for col in required_columns]
            )
            query = f"SELECT COUNT(*) FROM generated WHERE {where_clause}"
            count = conn.execute(query).fetchone()[0]

            if count > 0:
                self.errors.append(
                    f"Found {count} rows with missing required fields"
                )

    def _validate_column_group(
        self, conn: duckdb.DuckDBPyConnection, group_config: dict[str, Any]
    ) -> None:
        """Validate a group of related columns (e.g., all Channel1 columns).

        Args:
            conn: Database connection with views
            group_config: Configuration for the column group validation

        """
        # Get column group parameters
        group_name = group_config["name"]
        prefix_pattern = group_config["prefix_pattern"]
        column_types = group_config.get(
            "column_types", ["FileName", "PathName"]
        )

        # Find all matching columns in the generated CSV
        all_columns_query = "SELECT * FROM generated LIMIT 0"
        column_info = conn.execute(all_columns_query).description
        all_columns = [col[0] for col in column_info]

        matching_columns = {}
        for col_type in column_types:
            # Find columns matching the pattern for this type
            pattern = f"{col_type}_{prefix_pattern}"
            matching = [col for col in all_columns if re.match(pattern, col)]
            matching_columns[col_type] = matching

        # Check if matching columns exist
        if not any(matching_columns.values()):
            self.errors.append(
                f"No columns found matching pattern '{prefix_pattern}' for {group_name}"
            )
            return

        # Check for nulls in grouped columns
        for col_type, columns in matching_columns.items():
            if columns:
                where_clause = " OR ".join(
                    [f"{col} IS NULL" for col in columns]
                )
                query = f"SELECT COUNT(*) FROM generated WHERE {where_clause}"
                count = conn.execute(query).fetchone()[0]

                if count > 0:
                    self.errors.append(
                        f"Found {count} rows with missing {col_type} values for {group_name}"
                    )

        # Check file naming patterns if specified in the config
        pattern_check = group_config.get("filename_pattern")
        if pattern_check and "FileName" in column_types:
            filename_cols = matching_columns["FileName"]
            if filename_cols:
                # Build a query that checks if any filename columns don't match the pattern
                where_parts = []
                for col in filename_cols:
                    where_parts.append(f"{col} NOT LIKE '{pattern_check}'")

                where_clause = " OR ".join(where_parts)
                query = f"SELECT COUNT(*) FROM generated WHERE {where_clause}"
                count = conn.execute(query).fetchone()[0]

                if count > 0:
                    self.errors.append(
                        f"Found {count} {group_name} filenames that don't match pattern '{pattern_check}'"
                    )

    def _run_pipeline_specific_checks(
        self, conn: duckdb.DuckDBPyConnection
    ) -> None:
        """Run checks specific to this pipeline type.

        Args:
            conn: Database connection with views

        """
        specific_checks = self.config.get("pipeline_specific_checks", [])
        self._run_sql_checks(conn, specific_checks)

    def _run_sql_checks(
        self, conn: duckdb.DuckDBPyConnection, checks: list[dict[str, Any]]
    ) -> None:
        """Run SQL-based checks.

        Args:
            conn: Database connection with views
            checks: List of SQL check configurations

        """
        for check in checks:
            query = check["query"]
            error_msg = check["error_msg"]

            try:
                count = conn.execute(query).fetchone()[0]
                if count > 0:
                    self.errors.append(error_msg.format(count=count))
            except duckdb.Error as e:
                self.errors.append(f"Error in SQL check '{query}': {str(e)}")

    def _check_column_exists(
        self, conn: duckdb.DuckDBPyConnection, column: str
    ) -> bool:
        """Check if a column exists in both reference and generated CSVs.

        Args:
            conn: Database connection with views
            column: Column name to check

        Returns:
            True if the column exists in both CSVs

        """
        try:
            # Check if column exists in reference
            conn.execute(f"SELECT {column} FROM reference LIMIT 1")

            # Check if column exists in generated
            conn.execute(f"SELECT {column} FROM generated LIMIT 1")
            return True
        except duckdb.Error:
            self.errors.append(f"Required column '{column}' missing in CSV")
            return False

    def _check_column_values(
        self,
        conn: duckdb.DuckDBPyConnection,
        columns: list[str] | str,
        label: str,
        mode: str = "subset",  # "subset", "exact", "superset"
    ) -> None:
        """Compare column values between reference and generated tables.

        Args:
            conn: Database connection with views
            columns: Column(s) to check
            label: Name for error messages
            mode: "subset" (default), "exact", or "superset"

        """
        # Handle single column or multiple columns
        if isinstance(columns, str):
            columns = [columns]

        # Get distinct values from both tables
        if len(columns) == 1:
            col = columns[0]
            try:
                ref_values = {
                    row[0]
                    for row in conn.execute(
                        f"SELECT DISTINCT {col} FROM reference"
                    ).fetchall()
                }
                gen_values = {
                    row[0]
                    for row in conn.execute(
                        f"SELECT DISTINCT {col} FROM generated"
                    ).fetchall()
                }
            except duckdb.Error:
                self.errors.append(f"Error comparing values for column '{col}'")
                return

        else:
            # For multiple columns, concatenate as a composite key
            concat_cols = " || ',' || ".join(columns)
            try:
                ref_values = {
                    row[0]
                    for row in conn.execute(
                        f"SELECT DISTINCT {concat_cols} FROM reference"
                    ).fetchall()
                }
                gen_values = {
                    row[0]
                    for row in conn.execute(
                        f"SELECT DISTINCT {concat_cols} FROM generated"
                    ).fetchall()
                }
            except duckdb.Error:
                self.errors.append(
                    f"Error comparing values for columns {columns}"
                )
                return

        # Check if reference values exist in generated (subset mode)
        if mode in ["subset", "exact"]:
            missing = ref_values - gen_values
            if missing:
                self.errors.append(
                    f"Missing {label} in generated CSV: {missing}"
                )

        # Check if generated has extra values (superset mode)
        if mode in ["superset", "exact"]:
            extra = gen_values - ref_values
            if extra:
                self.errors.append(f"Extra {label} in generated CSV: {extra}")


# Dictionary of pipeline configurations for different LoadData types
PIPELINE_CONFIGS = {
    # CP illum calc LoadData configuration (pipeline1)
    "cp_illum_calc": {
        "name": "CP Illumination Calculation",
        "core_metadata": ["Metadata_Plate", "Metadata_Site", "Metadata_Well"],
        "required_columns": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "FileName_OrigDNA",
            "FileName_OrigZO1",
            "FileName_OrigPhalloidin",
            "PathName_OrigDNA",
            "PathName_OrigZO1",
            "PathName_OrigPhalloidin",
            "Frame_OrigDNA",
            "Frame_OrigZO1",
            "Frame_OrigPhalloidin",
        ],
        "column_groups": [
            {
                "name": "original images",
                "prefix_pattern": "Orig(DNA|ZO1|Phalloidin)",
                "column_types": ["FileName", "PathName", "Frame"],
                "filename_pattern": "%Channel%",
            }
        ],
        "pipeline_specific_checks": [
            {
                "query": """
                    SELECT COUNT(*) FROM generated
                    WHERE Frame_OrigDNA IS NULL
                    OR Frame_OrigZO1 IS NULL
                    OR Frame_OrigPhalloidin IS NULL
                """,
                "error_msg": "Found {count} rows with missing frame information",
            }
        ],
    },
    # CP illum apply LoadData configuration (pipeline2)
    "cp_illum_apply": {
        "name": "CP Illumination Application",
        "core_metadata": ["Metadata_Plate", "Metadata_Site", "Metadata_Well"],
        "required_columns": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "FileName_OrigDNA",
            "FileName_OrigZO1",
            "FileName_OrigPhalloidin",
            "PathName_OrigDNA",
            "PathName_OrigZO1",
            "PathName_OrigPhalloidin",
            "FileName_IllumDNA",
            "FileName_IllumZO1",
            "FileName_IllumPhalloidin",
            "PathName_IllumDNA",
            "PathName_IllumZO1",
            "PathName_IllumPhalloidin",
            "Frame_OrigDNA",
            "Frame_OrigZO1",
            "Frame_OrigPhalloidin",
        ],
        "column_groups": [
            {
                "name": "original images",
                "prefix_pattern": "Orig(DNA|ZO1|Phalloidin)",
                "column_types": ["FileName", "PathName", "Frame"],
                "filename_pattern": "%Channel%",
            },
            {
                "name": "illumination correction files",
                "prefix_pattern": "Illum(DNA|ZO1|Phalloidin)",
                "column_types": ["FileName", "PathName"],
            },
        ],
        "pipeline_specific_checks": [
            {
                "query": """
                    SELECT COUNT(*) FROM generated
                    WHERE FileName_IllumDNA IS NULL
                    OR FileName_IllumZO1 IS NULL
                    OR FileName_IllumPhalloidin IS NULL
                    OR PathName_IllumDNA IS NULL
                    OR PathName_IllumZO1 IS NULL
                    OR PathName_IllumPhalloidin IS NULL
                """,
                "error_msg": "Found {count} rows missing illumination filename information",
            }
        ],
    },
    # CP segmentation check LoadData configuration (pipeline3)
    "cp_segmentation_check": {
        "name": "CP Segmentation Check",
        "core_metadata": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "Metadata_Well_Value",
        ],
        "required_columns": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "Metadata_Well_Value",
            "PathName_DNA",
            "PathName_Phalloidin",
            "PathName_ZO1",
            "FileName_DNA",
            "FileName_Phalloidin",
            "FileName_ZO1",
        ],
        "column_groups": [
            {
                "name": "segmentation check images",
                "prefix_pattern": "(DNA|ZO1|Phalloidin)",
                "column_types": ["FileName", "PathName"],
            }
        ],
    },
    # SBS illum calc LoadData configuration (pipeline5)
    "sbs_illum_calc": {
        "name": "SBS Illumination Calculation",
        "core_metadata": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "Metadata_SBSCycle",
        ],
        "required_columns": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_SBSCycle",
            "Metadata_Well",
            "PathName_OrigT",
            "PathName_OrigG",
            "PathName_OrigA",
            "PathName_OrigC",
            "PathName_OrigDNA",
            "FileName_OrigT",
            "FileName_OrigG",
            "FileName_OrigA",
            "FileName_OrigC",
            "FileName_OrigDNA",
            "Frame_OrigT",
            "Frame_OrigG",
            "Frame_OrigA",
            "Frame_OrigC",
            "Frame_OrigDNA",
        ],
        "column_groups": [
            {
                "name": "SBS channel images",
                "prefix_pattern": "Orig(T|G|A|C|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
                "filename_pattern": "%Channel%",
            }
        ],
        "pipeline_specific_checks": [
            {
                "query": """
                    SELECT COUNT(*) FROM generated
                    WHERE Metadata_SBSCycle IS NULL
                """,
                "error_msg": "Found {count} rows with missing SBS cycle information",
            }
        ],
    },
    # SBS illum apply LoadData configuration (pipeline6)
    "sbs_illum_apply": {
        "name": "SBS Illumination Application",
        "core_metadata": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            # "Metadata_Well_Value",
        ],
        "required_columns": [],  # Too many to list individually, handled dynamically
        "column_groups": [
            {
                "name": "SBS Cycle1 original images",
                "prefix_pattern": "Cycle01_Orig(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
            {
                "name": "SBS Cycle1 illumination files",
                "prefix_pattern": "Cycle01_Illum(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
            {
                "name": "SBS Cycle2 original images",
                "prefix_pattern": "Cycle02_Orig(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
            {
                "name": "SBS Cycle2 illumination files",
                "prefix_pattern": "Cycle02_Illum(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
            {
                "name": "SBS Cycle3 original images",
                "prefix_pattern": "Cycle03_Orig(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
            {
                "name": "SBS Cycle3 illumination files",
                "prefix_pattern": "Cycle03_Illum(A|C|G|T|DNA)",
                "column_types": ["FileName", "PathName", "Frame"],
            },
        ],
    },
    # SBS preprocessing LoadData configuration (pipeline7)
    "sbs_preprocessing": {
        "name": "SBS Preprocessing",
        "core_metadata": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "Metadata_Well_Value",
        ],
        "required_columns": [],  # Many columns, handled dynamically
        "column_groups": [
            {
                "name": "SBS Cycle1 channels",
                "prefix_pattern": "Cycle01_(A|C|G|T)",
                "column_types": ["FileName", "PathName"],
            },
            {
                "name": "SBS Cycle2 channels",
                "prefix_pattern": "Cycle02_(A|C|G|T)",
                "column_types": ["FileName", "PathName"],
            },
            {
                "name": "SBS Cycle3 channels",
                "prefix_pattern": "Cycle03_(A|C|G|T)",
                "column_types": ["FileName", "PathName"],
            },
            {
                "name": "SBS DAPI channel",
                "prefix_pattern": "Cycle01_DAPI",
                "column_types": ["FileName", "PathName"],
            },
        ],
    },
    # Analysis LoadData configuration (pipeline9)
    "analysis": {
        "name": "Analysis",
        "core_metadata": [
            "Metadata_Plate",
            "Metadata_Site",
            "Metadata_Well",
            "Metadata_Well_Value",
        ],
        "required_columns": [],  # Many columns, handled dynamically
        "column_groups": [
            {
                "name": "SBS cycle channels",
                "prefix_pattern": "Cycle0[123]_(A|C|G|T)",
                "column_types": ["FileName", "PathName"],
            },
            {
                "name": "Corrected channels",
                "prefix_pattern": "Corr(DNA|ZO1|Phalloidin)",
                "column_types": ["FileName", "PathName"],
            },
            {
                "name": "DAPI channel",
                "prefix_pattern": "Cycle01_DAPI",
                "column_types": ["FileName", "PathName"],
            },
        ],
    },
}


def validate_loaddata_csv(
    generated_csv_path: Path,
    ref_csv_path: Path,
    pipeline_type: str,
) -> list[str]:
    """Validate generated LoadData CSV against reference file.

    Args:
        generated_csv_path: Path to generated CSV
        ref_csv_path: Path to reference CSV
        pipeline_type: Type of pipeline to validate (cp_illum_calc, etc.)

    Returns:
        Error messages (empty list if validation passed)

    """
    # Get the appropriate pipeline configuration
    if pipeline_type not in PIPELINE_CONFIGS:
        return [f"Unknown pipeline type: {pipeline_type}"]

    pipeline_config = PIPELINE_CONFIGS[pipeline_type]

    # Create validator and run validation
    validator = LoadDataValidator(pipeline_config)
    return validator.validate(generated_csv_path, ref_csv_path)
