"""LoadData CSV validation framework.

This module provides a simplified framework for validating LoadData CSV files
generated by different pipeline steps in the StarryNight workflow.

The framework supports:
1. Direct column validation and null checking
2. Simple pattern validation for specific columns
3. Custom SQL checks for complex validations
"""

from pathlib import Path
from typing import Any, Optional

import duckdb


def create_db_views(
    conn: duckdb.DuckDBPyConnection,
    generated_csv_path: Path,
    ref_csv_path: Path,
) -> None:
    """Create database views for both CSVs.

    Args:
        conn: Database connection
        generated_csv_path: Path to generated CSV
        ref_csv_path: Path to reference CSV

    """
    conn.execute(
        f"CREATE VIEW generated AS SELECT * FROM read_csv_auto('{str(generated_csv_path)}')"
    )
    conn.execute(
        f"CREATE VIEW reference AS SELECT * FROM read_csv_auto('{str(ref_csv_path)}')"
    )


def column_exists(conn: duckdb.DuckDBPyConnection, column: str) -> bool:
    """Check if a column exists in the generated CSV.

    Args:
        conn: Database connection with views
        column: Column name to check

    Returns:
        True if the column exists

    """
    try:
        conn.execute(f"SELECT {column} FROM generated LIMIT 1")
        return True
    except duckdb.Error:
        return False


def check_required_columns_not_null(
    conn: duckdb.DuckDBPyConnection, config: dict[str, Any]
) -> list[str]:
    """Check that required columns exist and don't contain nulls.

    Args:
        conn: Database connection with views
        config: Validation configuration

    Returns:
        List of error messages

    """
    errors = []
    required_columns = config.get("columns", {}).get("required", [])

    # Skip validation if no required columns are specified
    if not required_columns:
        return errors

    # Check column existence first
    for column in required_columns:
        if not column_exists(conn, column):
            errors.append(f"Required column '{column}' missing in CSV")

    # If columns are missing, we can't check for nulls
    if errors:
        return errors

    # Check for nulls in required columns that exist
    where_clause = " OR ".join([f"{col} IS NULL" for col in required_columns])
    try:
        query = f"SELECT COUNT(*) FROM generated WHERE {where_clause}"
        count = conn.execute(query).fetchone()[0]

        if count > 0:
            errors.append(f"Found {count} rows with missing required values")
    except duckdb.Error as e:
        errors.append(f"Error checking null values: {str(e)}")

    return errors


def check_column_pattern(
    conn: duckdb.DuckDBPyConnection, column: str, pattern: str
) -> list[str]:
    """Check if values in a column match a pattern.

    Args:
        conn: Database connection with views
        column: Column to check
        pattern: SQL LIKE pattern to match

    Returns:
        List of error messages

    """
    errors = []

    # Skip if column doesn't exist
    if not column_exists(conn, column):
        return [f"Cannot check pattern for missing column '{column}'"]

    try:
        query = f"SELECT COUNT(*) FROM generated WHERE {column} NOT LIKE '{pattern}'"
        count = conn.execute(query).fetchone()[0]

        if count > 0:
            errors.append(
                f"Found {count} values in '{column}' that don't match pattern '{pattern}'"
            )
    except duckdb.Error as e:
        errors.append(f"Error checking pattern for column '{column}': {str(e)}")

    return errors


def run_sql_check(
    conn: duckdb.DuckDBPyConnection, query: str, error_message: str
) -> list[str]:
    """Run a custom SQL check.

    Args:
        conn: Database connection with views
        query: SQL query that should return a count of invalid rows
        error_message: Error message to return if count > 0

    Returns:
        List of error messages

    """
    errors = []

    try:
        count = conn.execute(query).fetchone()[0]
        if count > 0:
            errors.append(f"{error_message} (found {count} rows)")
    except duckdb.Error as e:
        errors.append(f"Error in SQL check '{query}': {str(e)}")

    return errors


def validate_loaddata_csv(
    generated_csv_path: Path,
    ref_csv_path: Path,
    pipeline_type: str,
) -> list[str]:
    """Validate generated LoadData CSV against reference file.

    Args:
        generated_csv_path: Path to generated CSV
        ref_csv_path: Path to reference CSV
        pipeline_type: Type of pipeline to validate (cp_illum_calc, etc.)

    Returns:
        Error messages (empty list if validation passed)

    """
    # Get the appropriate validation configuration
    if pipeline_type not in VALIDATION_CONFIGS:
        return [f"Unknown pipeline type: {pipeline_type}"]

    config = VALIDATION_CONFIGS[pipeline_type]
    errors = []

    try:
        # Set up database connection with views
        with duckdb.connect(":memory:") as conn:
            create_db_views(conn, generated_csv_path, ref_csv_path)

            # Validate required columns exist and aren't null
            errors.extend(check_required_columns_not_null(conn, config))

            # Check pattern validations for specific columns
            for column, pattern in (
                config.get("columns", {}).get("pattern_check", {}).items()
            ):
                errors.extend(check_column_pattern(conn, column, pattern))

            # Run custom SQL checks
            for check in config.get("custom_checks", []):
                errors.extend(
                    run_sql_check(conn, check["sql"], check["message"])
                )

    except duckdb.Error as e:
        errors.append(f"Database error: {str(e)} (check CSV format/types)")

    return errors


# Dictionary of validation configurations for different LoadData CSV formats
VALIDATION_CONFIGS = {
    # CP illum calc LoadData configuration (pipeline1)
    "cp_illum_calc": {
        "name": "CP Illumination Calculation",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                "FileName_OrigDNA",
                "FileName_OrigZO1",
                "FileName_OrigPhalloidin",
                "PathName_OrigDNA",
                "PathName_OrigZO1",
                "PathName_OrigPhalloidin",
                "Frame_OrigDNA",
                "Frame_OrigZO1",
                "Frame_OrigPhalloidin",
            ],
            "pattern_check": {
                "FileName_OrigDNA": "%Channel%",
                "FileName_OrigZO1": "%Channel%",
                "FileName_OrigPhalloidin": "%Channel%",
            },
        },
        "custom_checks": [
            {
                "sql": """
                    SELECT COUNT(*) FROM generated
                    WHERE Frame_OrigDNA IS NULL
                    OR Frame_OrigZO1 IS NULL
                    OR Frame_OrigPhalloidin IS NULL
                """,
                "message": "Found rows with missing frame information",
            }
        ],
    },
    # CP illum apply LoadData configuration (pipeline2)
    "cp_illum_apply": {
        "name": "CP Illumination Application",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                "FileName_OrigDNA",
                "FileName_OrigZO1",
                "FileName_OrigPhalloidin",
                "PathName_OrigDNA",
                "PathName_OrigZO1",
                "PathName_OrigPhalloidin",
                "FileName_IllumDNA",
                "FileName_IllumZO1",
                "FileName_IllumPhalloidin",
                "PathName_IllumDNA",
                "PathName_IllumZO1",
                "PathName_IllumPhalloidin",
                "Frame_OrigDNA",
                "Frame_OrigZO1",
                "Frame_OrigPhalloidin",
            ],
            "pattern_check": {
                "FileName_OrigDNA": "%Channel%",
                "FileName_OrigZO1": "%Channel%",
                "FileName_OrigPhalloidin": "%Channel%",
            },
        },
        "custom_checks": [
            {
                "sql": """
                    SELECT COUNT(*) FROM generated
                    WHERE FileName_IllumDNA IS NULL
                    OR FileName_IllumZO1 IS NULL
                    OR FileName_IllumPhalloidin IS NULL
                    OR PathName_IllumDNA IS NULL
                    OR PathName_IllumZO1 IS NULL
                    OR PathName_IllumPhalloidin IS NULL
                """,
                "message": "Found rows missing illumination filename information",
            }
        ],
    },
    # CP segmentation check LoadData configuration (pipeline3)
    "cp_segmentation_check": {
        "name": "CP Segmentation Check",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                "Metadata_Well_Value",
                "PathName_DNA",
                "PathName_Phalloidin",
                "PathName_ZO1",
                "FileName_DNA",
                "FileName_Phalloidin",
                "FileName_ZO1",
            ],
            "pattern_check": {},
        },
        "custom_checks": [],
    },
    # SBS illum calc LoadData configuration (pipeline5)
    "sbs_illum_calc": {
        "name": "SBS Illumination Calculation",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                # Note: Using Metadata_Cycle instead of Metadata_SBSCycle
                # The reference files use Metadata_Cycle while the code expects Metadata_SBSCycle
                "Metadata_Cycle",
                "PathName_OrigT",
                "PathName_OrigG",
                "PathName_OrigA",
                "PathName_OrigC",
                "PathName_OrigDNA",
                "FileName_OrigT",
                "FileName_OrigG",
                "FileName_OrigA",
                "FileName_OrigC",
                "FileName_OrigDNA",
                "Frame_OrigT",
                "Frame_OrigG",
                "Frame_OrigA",
                "Frame_OrigC",
                "Frame_OrigDNA",
            ],
            "pattern_check": {
                "FileName_OrigT": "%Channel%",
                "FileName_OrigG": "%Channel%",
                "FileName_OrigA": "%Channel%",
                "FileName_OrigC": "%Channel%",
                "FileName_OrigDNA": "%Channel%",
            },
        },
        "custom_checks": [
            {
                "sql": "SELECT COUNT(*) FROM generated WHERE Metadata_Cycle IS NULL",
                "message": "Found rows with missing cycle information",
            }
        ],
    },
    # SBS illum apply LoadData configuration (pipeline6)
    "sbs_illum_apply": {
        "name": "SBS Illumination Application",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                # Note: Unlike SBS illum calc, this doesn't have Metadata_Cycle
                # Cycle info is embedded in column names like Cycle01_*, Cycle02_*, etc.
            ],
            "pattern_check": {},
        },
        "custom_checks": [],
    },
    # SBS preprocessing LoadData configuration (pipeline7)
    "sbs_preprocessing": {
        "name": "SBS Preprocessing",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                "Metadata_Well_Value",
                # Note: More columns would be needed for a complete validation
            ],
            "pattern_check": {},
        },
        "custom_checks": [],
    },
    # Analysis LoadData configuration (pipeline9)
    "analysis": {
        "name": "Analysis",
        "columns": {
            "required": [
                "Metadata_Plate",
                "Metadata_Site",
                "Metadata_Well",
                "Metadata_Well_Value",
                # Note: More columns would be needed for a complete validation
            ],
            "pattern_check": {},
        },
        "custom_checks": [],
    },
}
